{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %load /home/gaetan/Desktop/geovpylib/templates/heading.py\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Common imports\n","import os\n","import pandas as pd, numpy as np\n","import datetime\n","#import json\n","#import request\n","#import duckdb\n","#import plotly.express as px\n","\n","# Geovpylib library\n","import geovpylib.analysis as a\n","import geovpylib.database as db\n","import geovpylib.decorators as d\n","import geovpylib.magics\n","import geovpylib.pks as pks\n","import geovpylib.queries as q\n","import geovpylib.sparql as sparql\n","import geovpylib.utils as u\n","eta = u.Eta()\n","\n","\n","env = 'prod'\n","pk_project = pks.projects.symogih\n","execute = False\n","metadata_str = 'collective-actor-correction'\n","import_manner = 'one-shot'"]},{"cell_type":"markdown","metadata":{},"source":["# Collective Actors Correction"]},{"cell_type":"markdown","metadata":{},"source":["## 1./ Delete everything"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.0/ Fetch general data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["existing_groups = db.query(f\"\"\"\n","    select \n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","existing_groups['pk_bhp'] = [string[string.rfind('/') + 1:] for string in existing_groups['uri']]\n","existing_groups.drop(columns='uri', inplace=True)\n","existing_groups = existing_groups[existing_groups['pk_bhp'].str.contains('CoAc')]\n","\n","pk_groups_str = u.get_sql_ready_str(existing_groups['pk_group'])\n","\n","# a.infos(existing_groups)\n","\n","#########\n","\n","existing_formations = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_formation,\n","        ipr2.pk_entity as pk_ipr_has_formed_group\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.formation_hasFormed_group} and s2.fk_object_info in {pk_groups_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    where r1.fk_class = {pks.classes.formation}\n","\"\"\")\n","pk_formations_str = u.get_sql_ready_str(existing_formations['pk_formation'])\n","\n","# a.infos(existing_formations)\n","\n","#########\n","\n","existing_dissolutions = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_dissolution,\n","        ipr2.pk_entity as pk_ipr_has_dissolved_group\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.dissolution_dissolved_group} and s2.fk_object_info in {pk_groups_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    where r1.fk_class = {pks.classes.dissolution}\n","\"\"\")\n","pk_dissolution_str = u.get_sql_ready_str(existing_dissolutions['pk_dissolution'])\n","\n","# a.infos(existing_dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.1.a/ Delete `has_formed_group` statement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pk_ipr_has_formed_group = u.get_sql_ready_str(existing_formations['pk_ipr_has_formed_group'])\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_has_formed_group}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.1.b/ Delete `has_dissolved_group` statement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pk_ipr_has_dissolved_group = u.get_sql_ready_str(existing_dissolutions['pk_ipr_has_dissolved_group'])\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_has_dissolved_group}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2/ Delete time information"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["time_properties = f\"({pks.properties.timeSpan_atSomeTimeWithin_timePrimitive}, {pks.properties.timespan_beginOfTheBegin_timePrim}, {pks.properties.timespan_endOfTheEnd_timePrim}, {pks.properties.timeSpan_ongoingThroughout_timePrimitive}, {pks.properties.timespan_endOfTheBegin_timePrim}, {pks.properties.timespan_beginOfTheEnd_timePrim})\""]},{"cell_type":"markdown","metadata":{},"source":["#### 1.2.a/ Delete time information for formation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formation_time_stmts = db.query(f\"\"\"\n","    select\n","        ipr.pk_entity as pk_ipr_time_info_form\n","    from information.statement s\n","    inner join projects.info_proj_rel ipr on ipr.fk_entity = s.pk_entity and ipr.fk_project = {pk_project} and ipr.is_in_project = true\n","    where s.fk_subject_info in {pk_formations_str} and s.fk_property in {time_properties}\n","\"\"\")\n","pk_ipr_time_info_form = u.get_sql_ready_str(formation_time_stmts['pk_ipr_time_info_form'])\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_time_info_form}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.2.b/ Delete time information for dissolution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolution_time_stmts = db.query(f\"\"\"\n","    select\n","        ipr.pk_entity as pk_ipr_time_info_diss\n","    from information.statement s\n","    inner join projects.info_proj_rel ipr on ipr.fk_entity = s.pk_entity and ipr.fk_project = {pk_project} and ipr.is_in_project = true\n","    where s.fk_subject_info in {pk_dissolution_str} and s.fk_property in {time_properties}\n","\"\"\")\n","pk_ipr_time_info_diss = u.get_sql_ready_str(dissolution_time_stmts['pk_ipr_time_info_diss'])\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_time_info_diss}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3/ Delete certainty comment"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.a/ Delete certainty comment for formations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formation_comments = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 7953586\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_comment_form = formation_comments['pk_ipr_comment'].tolist() + formation_comments['pk_ipr_stmt_has_comment'].tolist() + formation_comments['pk_ipr_stmt_comment_has_type'].tolist() + formation_comments['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_comment_form = u.get_sql_ready_str(pk_iprs_cert_comment_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_comment_form}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.b/ Delete certainty comment for dissolutions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolution_comments = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 7953586\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_comment_diss = dissolution_comments['pk_ipr_comment'].tolist() + dissolution_comments['pk_ipr_stmt_has_comment'].tolist() + dissolution_comments['pk_ipr_stmt_comment_has_type'].tolist() + dissolution_comments['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_comment_diss = u.get_sql_ready_str(pk_iprs_cert_comment_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_comment_diss}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.4/ Delete URI informations"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.4.a/ Delete URI informations for formations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formation_uri = db.query(f\"\"\"\n","    select\n","        ipr1.pk_entity as pk_ipr_uri,\n","        ipr2.pk_entity as pk_ipr_stmt_same_as_uri,\n","        ipr3.pk_entity as pk_ipr_stmt_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    where r1.fk_class = {pks.classes.uri}\n","\"\"\")\n","\n","pk_iprs_uri_form = formation_uri['pk_ipr_uri'].tolist() + formation_uri['pk_ipr_stmt_same_as_uri'].tolist() + formation_uri['pk_ipr_stmt_has_value'].tolist()\n","pk_iprs_uri_form = u.get_sql_ready_str(pk_iprs_uri_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_uri_form}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.4.b/ Delete URI informations for dissolution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolution_uri = db.query(f\"\"\"\n","    select\n","        ipr1.pk_entity as pk_ipr_uri,\n","        ipr2.pk_entity as pk_ipr_stmt_same_as_uri,\n","        ipr3.pk_entity as pk_ipr_stmt_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    where r1.fk_class = {pks.classes.uri}\n","\"\"\")\n","\n","pk_iprs_uri_diss = dissolution_uri['pk_ipr_uri'].tolist() + dissolution_uri['pk_ipr_stmt_same_as_uri'].tolist() + dissolution_uri['pk_ipr_stmt_has_value'].tolist()\n","pk_iprs_uri_diss = u.get_sql_ready_str(pk_iprs_uri_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_uri_diss}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5/ Delete date complements"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.5.a/ Delete date complements on formations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formation_date_cmplt = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065621\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_date_cmplt_form = formation_date_cmplt['pk_ipr_comment'].tolist() + formation_date_cmplt['pk_ipr_stmt_has_comment'].tolist() + formation_date_cmplt['pk_ipr_stmt_comment_has_type'].tolist() + formation_date_cmplt['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_date_cmplt_form = u.get_sql_ready_str(pk_iprs_cert_date_cmplt_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_date_cmplt_form}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.5.b/ Delete date complements on dissolution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolution_date_cmplt = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065621\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_date_cmplt_diss = dissolution_date_cmplt['pk_ipr_comment'].tolist() + dissolution_date_cmplt['pk_ipr_stmt_has_comment'].tolist() + dissolution_date_cmplt['pk_ipr_stmt_comment_has_type'].tolist() + dissolution_date_cmplt['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_date_cmplt_diss = u.get_sql_ready_str(pk_iprs_cert_date_cmplt_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_date_cmplt_diss}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.6/ Delete note on dates"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.6.a/ Delete note on dates on formations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formation_note_dates = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065632\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_note_dates_form = formation_note_dates['pk_ipr_comment'].tolist() + formation_note_dates['pk_ipr_stmt_has_comment'].tolist() + formation_note_dates['pk_ipr_stmt_comment_has_type'].tolist() + formation_note_dates['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_note_dates_form = u.get_sql_ready_str(pk_iprs_cert_note_dates_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_note_dates_form}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.6.b/ Delete note on dates on dissolution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolution_note_dates = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065632\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_note_dates_diss = dissolution_note_dates['pk_ipr_comment'].tolist() + dissolution_note_dates['pk_ipr_stmt_has_comment'].tolist() + dissolution_note_dates['pk_ipr_stmt_comment_has_type'].tolist() + dissolution_note_dates['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_note_dates_diss = u.get_sql_ready_str(pk_iprs_cert_note_dates_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_note_dates_diss}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.7/ Delete Formation, Dissolutions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_formations_str}\n","\"\"\")\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_dissolution_str}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["## 2./ Find and create missing Groups"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1/ Find all existing groups in symogih project in Geovistory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find existing group on GV\n","groups = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1 \n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2a on ipr2a.fk_entity = s2.pk_entity and ipr2a.fk_project = {pk_project} and ipr2a.is_in_project = true\n","    inner join projects.info_proj_rel ipr2b on ipr2b.fk_entity = s2.fk_object_info and ipr2b.fk_project = {pk_project} and ipr2b.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","groups = groups[groups['uri'].str.contains('CoAc')]\n","groups['pk_coac'] = [string[string.rfind('/') + 1:] for string in groups['uri']]\n","groups.drop(columns='uri', inplace=True)\n","\n","# a.infos(groups)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2/ Find all Collective actors from BHP (CSV file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n","coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n","coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n","\n","coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n","coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n","coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n","coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n","\n","coacs['pk_collective_actor'] = 'CoAc' + coacs['pk_collective_actor'].astype(str)\n","coacs.rename(columns={'pk_collective_actor':'pk_coac'}, inplace=True)\n","\n","# a.infos(coacs)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3/ Get record linkage result (CSV file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["record_linkage = pd.read_csv('../../data/record-linkage-collective-actors-filled.csv')\n","record_linkage = record_linkage[record_linkage['Doublon'] == 'oui']\n","record_linkage = record_linkage[['pk_bhp', 'pk_gv']]\n","record_linkage['pk_bhp'] = 'CoAc' + record_linkage['pk_bhp'].astype(str)\n","record_linkage.rename(columns={'pk_bhp': 'pk_coac', 'pk_gv': 'pk_group'}, inplace=True)\n","\n","# a.infos(record_linkage)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.4/ Find missing ones"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["existing_groups = groups['pk_coac'].tolist()\n","record_linkage_groups = record_linkage['pk_coac'].tolist()\n","missing_groups = coacs[[pk_coac not in existing_groups and pk_coac not in record_linkage_groups for pk_coac in coacs['pk_coac']]].reset_index(drop=True)\n","\n","# All missing ones should be either existing or in the record linkage\n","assert len(missing_groups) == 0"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5/ Add record linkage result to project"]},{"cell_type":"markdown","metadata":{},"source":["The problem was actually that all information about the entities where added, but not the entity itself.. Dumb me.."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db.info_proj_rels.create(record_linkage['pk_group'])"]},{"cell_type":"markdown","metadata":{},"source":["## 3./ Add Formations"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1/ Fetch local file about coacs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n","coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n","coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n","\n","coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n","coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n","coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n","coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n","\n","coacs['pk_collective_actor'] = 'CoAc' + coacs['pk_collective_actor'].astype(str)\n","coacs.rename(columns={'pk_collective_actor':'pk_coac'}, inplace=True)\n","\n","# a.infos(coacs)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2/ Fetch Geovistory existing groups"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find existing group on GV\n","groups = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1 \n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2a on ipr2a.fk_entity = s2.pk_entity and ipr2a.fk_project = {pk_project} and ipr2a.is_in_project = true\n","    inner join projects.info_proj_rel ipr2b on ipr2b.fk_entity = s2.fk_object_info and ipr2b.fk_project = {pk_project} and ipr2b.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","groups = groups[groups['uri'].str.contains('CoAc')]\n","groups['pk_coac'] = [string[string.rfind('/') + 1:] for string in groups['uri']]\n","groups.drop(columns='uri', inplace=True)\n","\n","coacs_str = u.get_sql_ready_str(groups['pk_coac'])\n","\n","# a.infos(groups)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3/ Fetch BHP related formation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to BHP database\n","db_url_env_var_name = 'YELLOW_BHP' # Name of an environment variable holding the Postgres database URL\n","db.connect_external(os.getenv(db_url_env_var_name), execute=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations_bhp_info = db.query(f\"\"\"\n","    select\n","        ir.fk_associated_object as pk_coac, \n","        i.pk_information, \n","        id.year, id.month, id.day,\n","        id.fk_abob_type_information_date,\n","        id.complement as complement, \n","        id.notes as notes,\n","        id.certainty_date\n","    from bhp.information_role ir\n","    inner join bhp.information i on i.pk_information = ir.fk_information and i.fk_type_information = 30\n","    inner join bhp.information_date id on id.fk_information = i.pk_information\n","    where ir.fk_associated_object in {coacs_str} and ir.fk_type_role = 49\n","\"\"\")\n","\n","formations_bhp_info['year'] = formations_bhp_info['year'].astype(pd.Int64Dtype())\n","formations_bhp_info['month'] = formations_bhp_info['month'].astype(pd.Int64Dtype())\n","formations_bhp_info['day'] = formations_bhp_info['day'].astype(pd.Int64Dtype())\n","formations_bhp_info['fk_abob_type_information_date'] = formations_bhp_info['fk_abob_type_information_date'].astype(pd.Int64Dtype())\n","formations_bhp_info['date_bhp'] = [(row.year, row.month, row.day) for i, row in formations_bhp_info.iterrows()]\n","formations_bhp_info['uri'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in formations_bhp_info['pk_information']]\n","formations_bhp_info.drop(columns=['year', 'month', 'day', 'pk_information'], inplace=True)\n","formations_bhp_info['complement'] = [pd.NA if pd.isna(row['complement']) or row['complement'].strip() == '' else row['complement'] for _,row in formations_bhp_info.iterrows()]\n","formations_bhp_info['notes'] = [pd.NA if pd.isna(row['notes']) or row['notes'].strip() == '' else row['notes'] for _,row in formations_bhp_info.iterrows()]\n","formations_bhp_info['notes'] = [s.replace('<p>', '').replace('</p>', '') if pd.notna(s) else pd.NA for s in formations_bhp_info['notes']]\n","formations_bhp_info['notes'] = [s.replace('<em>', '').replace('</em>', '') if pd.notna(s) else pd.NA for s in formations_bhp_info['notes']]\n","formations_bhp_info['complement'] = [e.replace('<p>', '').replace('</p>', '') if pd.notna(e) else pd.NA for e in formations_bhp_info['complement']]\n","\n","# a.infos(formations_bhp_info)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4/ Build import table"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.1/ Create table, and link with GV id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations = pd.DataFrame()\n","formations['pk_coac'] = np.unique(coacs['pk_coac'].tolist() + formations_bhp_info['pk_coac'].tolist())\n","formations = formations.merge(groups)\n","\n","formations.sort_values('pk_coac', inplace=True)\n","formations = formations[['pk_group', 'pk_coac']]\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.2/ Add date informations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Begin of the begin ###\n","note_begin = 4\n","property_name = 'begin_of_the_begin'\n","abob_types = [1125,1321,1322]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Begin of the end ###\n","property_name = 'begin_of_the_end'\n","abob_types = [1290]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### End of the begin ###\n","property_name = 'end_of_the_begin'\n","abob_types = [1323]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### End of the end ###\n","note_begin = 1\n","property_name = 'end_of_the_end'\n","abob_types = [256,1126,1128]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Ongoing throughout ###\n","note_begin = 3\n","property_name = 'ongoing_throughout'\n","abob_types = [258]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### At some time within ###\n","note_begin = 2\n","property_name = 'at_some_time_within'\n","abob_types = [246]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Default cases: At some time within\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for i, row in coacs.iterrows():\n","    if pd.isna(row['notes_begin']) or row['notes_begin'] not in [1,2,3,4]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if pd.notna(row['begin_year_coac']):\n","                formations.at[row['pk_coac'], 'at_some_time_within'] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infos\n","for i, row in formations_bhp_info.iterrows():\n","    if row['fk_abob_type_information_date'] not in [246,1125,1126,258,1289,1290,1321,1322,1323,256,1128,1128]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if row['date_bhp'] != (pd.NA, pd.NA, pd.NA):\n","                formations.at[row['pk_coac'], 'at_some_time_within'] = row['date_bhp']        \n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.3/ Add URI"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Only concerns the BHP informations\n","\n","formations['uri'] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","for _, row in formations_bhp_info.iterrows():\n","    formations.at[row['pk_coac'], 'uri'] = row['uri']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.4/ Add certainty comment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations['certainty_comment'] = pd.NA\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for _, row in coacs.iterrows():\n","    if pd.notna(row['certainty_begin']) and row['certainty_begin'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_begin']) and row['certainty_begin'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","# From BHP info\n","for _, row in formations_bhp_info.iterrows():\n","    if pd.notna(row['certainty_date']) and row['certainty_date'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_date']) and row['certainty_date'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.5/ Add Date complement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations['date_complement'] = pd.NA\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in formations_bhp_info.iterrows():\n","    formations.at[row['pk_coac'], 'date_complement'] = row['complement']\n","    \n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.6/ Add date note"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations['date_note'] = pd.NA\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in formations_bhp_info.iterrows():\n","    formations.at[row['pk_coac'], 'date_note'] = '[Note] ' + row['notes']\n","    \n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.5/ Remove formation that should not be created"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations = formations[\n","    pd.notna(formations['begin_of_the_begin']) |\n","    pd.notna(formations['begin_of_the_end']) |\n","    pd.notna(formations['end_of_the_begin']) |\n","    pd.notna(formations['end_of_the_end']) |\n","    pd.notna(formations['ongoing_throughout']) |\n","    pd.notna(formations['at_some_time_within']) |\n","    pd.notna(formations['uri']) |\n","    pd.notna(formations['certainty_comment']) |\n","    pd.notna(formations['date_complement']) |\n","    pd.notna(formations['date_note'])\n","].reset_index(drop=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.6/ Generate CSV for validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations.to_csv('./formation.csv', index=False)\n","\n","a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["## 4./ Add Dissolutions"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1/ Fetch local file about coacs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n","coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n","coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n","\n","coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n","coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n","coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n","coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n","\n","coacs['pk_collective_actor'] = 'CoAc' + coacs['pk_collective_actor'].astype(str)\n","coacs.rename(columns={'pk_collective_actor':'pk_coac'}, inplace=True)\n","\n","# a.infos(coacs)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2/ Fetch Geovistory existing groups"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find existing group on GV\n","groups = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1 \n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2a on ipr2a.fk_entity = s2.pk_entity and ipr2a.fk_project = {pk_project} and ipr2a.is_in_project = true\n","    inner join projects.info_proj_rel ipr2b on ipr2b.fk_entity = s2.fk_object_info and ipr2b.fk_project = {pk_project} and ipr2b.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","groups = groups[groups['uri'].str.contains('CoAc')]\n","groups['pk_coac'] = [string[string.rfind('/') + 1:] for string in groups['uri']]\n","groups.drop(columns='uri', inplace=True)\n","\n","coacs_str = u.get_sql_ready_str(groups['pk_coac'])\n","\n","# a.infos(groups)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4.3/ Fetch BHP related formation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to BHP database\n","db_url_env_var_name = 'YELLOW_BHP' # Name of an environment variable holding the Postgres database URL\n","db.connect_external(os.getenv(db_url_env_var_name), execute=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions_bhp_info = db.query(f\"\"\"\n","    select\n","        ir.fk_associated_object as pk_coac, \n","        i.pk_information, \n","        id.year, id.month, id.day,\n","        id.fk_abob_type_information_date,\n","        id.complement as complement, \n","        id.notes as notes,\n","        id.certainty_date\n","    from bhp.information_role ir\n","    inner join bhp.information i on i.pk_information = ir.fk_information and i.fk_type_information = 33\n","    inner join bhp.information_date id on id.fk_information = i.pk_information\n","    where ir.fk_associated_object in {coacs_str} and ir.fk_type_role = 54\n","\"\"\")\n","\n","dissolutions_bhp_info['year'] = dissolutions_bhp_info['year'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['month'] = dissolutions_bhp_info['month'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['day'] = dissolutions_bhp_info['day'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['fk_abob_type_information_date'] = dissolutions_bhp_info['fk_abob_type_information_date'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['date_bhp'] = [(row.year, row.month, row.day) for i, row in dissolutions_bhp_info.iterrows()]\n","dissolutions_bhp_info['uri'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in dissolutions_bhp_info['pk_information']]\n","dissolutions_bhp_info.drop(columns=['year', 'month', 'day', 'pk_information'], inplace=True)\n","dissolutions_bhp_info['complement'] = [pd.NA if pd.isna(row['complement']) or row['complement'].strip() == '' else row['complement'] for _,row in dissolutions_bhp_info.iterrows()]\n","dissolutions_bhp_info['notes'] = [pd.NA if pd.isna(row['notes']) or row['notes'].strip() == '' else row['notes'] for _,row in dissolutions_bhp_info.iterrows()]\n","dissolutions_bhp_info['complement'] = [e.replace('<p>', '').replace('</p>', '') if pd.notna(e) else pd.NA for e in dissolutions_bhp_info['complement']]\n","\n","a.infos(dissolutions_bhp_info)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.4/ Build import table"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.1/ Create table, and link with GV id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions = pd.DataFrame()\n","dissolutions['pk_coac'] = np.unique(coacs['pk_coac'].tolist() + dissolutions_bhp_info['pk_coac'].tolist())\n","dissolutions = dissolutions.merge(groups)\n","\n","dissolutions.sort_values('pk_coac', inplace=True)\n","dissolutions = dissolutions[['pk_group', 'pk_coac']]\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.2/ Add date informations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Begin of the begin ###\n","note_begin = 4\n","property_name = 'begin_of_the_begin'\n","abob_types = [1125,1321,1322]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Begin of the end ###\n","property_name = 'begin_of_the_end'\n","abob_types = [1290]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### End of the begin ###\n","property_name = 'end_of_the_begin'\n","abob_types = [1323]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### End of the end ###\n","note_begin = 1\n","property_name = 'end_of_the_end'\n","abob_types = [256,1126,1128]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Ongoing throughout ###\n","note_begin = 3\n","property_name = 'ongoing_throughout'\n","abob_types = [258]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### At some time within ###\n","note_begin = 2\n","property_name = 'at_some_time_within'\n","abob_types = [246]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Default cases: At some time within\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for i, row in coacs.iterrows():\n","    if pd.isna(row['notes_begin']) or row['notes_begin'] not in [1,2,3,4]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if pd.notna(row['end_year_coac']):\n","                dissolutions.at[row['pk_coac'], 'at_some_time_within'] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infos\n","for i, row in dissolutions_bhp_info.iterrows():\n","    if row['fk_abob_type_information_date'] not in [246,1125,1126,258,1289,1290,1321,1322,1323,256,1128,1128]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if row['date_bhp'] != (pd.NA, pd.NA, pd.NA):\n","                dissolutions.at[row['pk_coac'], 'at_some_time_within'] = row['date_bhp']        \n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.3/ Add URI"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Only concerns the BHP informations\n","\n","dissolutions['uri'] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","for _, row in dissolutions_bhp_info.iterrows():\n","    dissolutions.at[row['pk_coac'], 'uri'] = row['uri']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.4/ Add certainty comment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions['certainty_comment'] = pd.NA\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for _, row in coacs.iterrows():\n","    if pd.notna(row['certainty_begin']) and row['certainty_begin'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_begin']) and row['certainty_begin'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","# From BHP info\n","for _, row in dissolutions_bhp_info.iterrows():\n","    if pd.notna(row['certainty_date']) and row['certainty_date'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_date']) and row['certainty_date'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.5/ Add Date complement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions['date_complement'] = pd.NA\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in dissolutions_bhp_info.iterrows():\n","    dissolutions.at[row['pk_coac'], 'date_complement'] = row['complement']\n","    \n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.6/ Add date note"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions['date_note'] = pd.NA\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in dissolutions_bhp_info.iterrows():\n","    dissolutions.at[row['pk_coac'], 'date_note'] = '[Note] ' + row['notes']\n","    \n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.5/ Remove dissolutions that should not be created"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions = dissolutions[\n","    pd.notna(dissolutions['begin_of_the_begin']) |\n","    pd.notna(dissolutions['begin_of_the_end']) |\n","    pd.notna(dissolutions['end_of_the_begin']) |\n","    pd.notna(dissolutions['end_of_the_end']) |\n","    pd.notna(dissolutions['ongoing_throughout']) |\n","    pd.notna(dissolutions['at_some_time_within']) |\n","    pd.notna(dissolutions['uri']) |\n","    pd.notna(dissolutions['certainty_comment']) |\n","    pd.notna(dissolutions['date_complement']) |\n","    pd.notna(dissolutions['date_note'])\n","].reset_index(drop=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.6/ Generate CSV for validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dissolutions.to_csv('./dissolutions.csv', index=False)\n","\n","a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["## 5/ Import new data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_duration(date):\n","    if pd.notna(date[0]) and pd.isna(date[1]) and pd.isna(date[2]): return '1 year'\n","    if pd.notna(date[0]) and pd.notna(date[1]) and pd.isna(date[2]): return '1 month'\n","    if pd.notna(date[0]) and pd.notna(date[1]) and pd.notna(date[2]): return '1 day'\n","    return pd.NA"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1/ Create formations / dissolutions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["formations[\"pk_formation\"] = db.resources.create(pks.classes.formation, len(formations))\n","dissolutions[\"pk_dissolution\"] = db.resources.create(pks.classes.dissolution, len(dissolutions))"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2/ Link formations / dissolutions to their group"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db.statements.create(formations['pk_formation'], pks.properties.formation_hasFormed_group, formations['pk_group'])\n","db.statements.create(dissolutions['pk_dissolution'], pks.properties.dissolution_dissolved_group, dissolutions['pk_group'])"]},{"cell_type":"markdown","metadata":{},"source":["### 5.3/ Begin of the begin"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['begin_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_begin'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_beginOfTheBegin_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['begin_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_begin'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_beginOfTheBegin_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.4/ Begin of the end"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['begin_of_the_end'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_end']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_end'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_beginOfTheEnd_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['begin_of_the_end'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_end']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_end'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_beginOfTheEnd_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.5/ End of the begin"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['end_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['end_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['end_of_the_begin'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_endOfTheBegin_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['end_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['end_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['end_of_the_begin'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_endOfTheBegin_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.6/ End of the end"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['end_of_the_end'])]\n","durations = [get_duration(date) for date in selection['end_of_the_end']]\n","time_prims = db.time_primitives.create(selection['end_of_the_end'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_endOfTheEnd_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['end_of_the_end'])]\n","durations = [get_duration(date) for date in selection['end_of_the_end']]\n","time_prims = db.time_primitives.create(selection['end_of_the_end'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_endOfTheEnd_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.7/ Ongoing throughout"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['ongoing_throughout'])]\n","durations = [get_duration(date) for date in selection['ongoing_throughout']]\n","time_prims = db.time_primitives.create(selection['ongoing_throughout'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timeSpan_ongoingThroughout_timePrimitive, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['ongoing_throughout'])]\n","durations = [get_duration(date) for date in selection['ongoing_throughout']]\n","time_prims = db.time_primitives.create(selection['ongoing_throughout'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timeSpan_ongoingThroughout_timePrimitive, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.8/ At some time within"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['at_some_time_within'])]\n","durations = [get_duration(date) for date in selection['at_some_time_within']]\n","time_prims = db.time_primitives.create(selection['at_some_time_within'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timeSpan_atSomeTimeWithin_timePrimitive, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['at_some_time_within'])]\n","durations = [get_duration(date) for date in selection['at_some_time_within']]\n","time_prims = db.time_primitives.create(selection['at_some_time_within'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timeSpan_atSomeTimeWithin_timePrimitive, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.9/ URI"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['uri'])]\n","db.shortcuts.add_uris(selection['pk_formation'], selection['uri'])\n","\n","selection = dissolutions[pd.notna(dissolutions['uri'])]\n","db.shortcuts.add_uris(selection['pk_dissolution'], selection['uri'])"]},{"cell_type":"markdown","metadata":{},"source":["### 5.10/ Certainty comment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['certainty_comment'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['certainty_comment'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 7953586)\n","db.statements.create(selection['pk_formation'], pks.properties.entity_hasComment_text, pk_comments)\n","\n","selection = dissolutions[pd.notna(dissolutions['certainty_comment'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['certainty_comment'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 7953586)\n","db.statements.create(selection['pk_dissolution'], pks.properties.entity_hasComment_text, pk_comments)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.11/ Date_complement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['date_complement'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_complement'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065621)\n","db.statements.create(selection['pk_formation'], pks.properties.entity_hasComment_text, pk_comments)\n","\n","selection = dissolutions[pd.notna(dissolutions['date_complement'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_complement'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065621)\n","db.statements.create(selection['pk_dissolution'], pks.properties.entity_hasComment_text, pk_comments)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.11/ Date note"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['date_note'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_note'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065632)\n","db.statements.create(selection['pk_formation'], pks.properties.entity_hasComment_text, pk_comments)\n","\n","selection = dissolutions[pd.notna(dissolutions['date_note'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_note'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065632)\n","db.statements.create(selection['pk_dissolution'], pks.properties.entity_hasComment_text, pk_comments)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
