{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = 'stag'\n",
    "pk_project = 6857901\n",
    "execute = False\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "\n",
    "import geovpylib.analysis as a\n",
    "import geovpylib.database as db\n",
    "import geovpylib.graphs as graphs\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.recordlinkage as rl\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.utils as u\n",
    "\n",
    "eta = u.Eta()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BHP actors into Geovistory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record linkage result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch record linkage and keep only the BHP key and the GEOV key\n",
    "record_linkage = pd.read_csv('../../data/record-linkage-bhp-actors-geov-persons-filled.csv')\n",
    "record_linkage = record_linkage[record_linkage['doublon'] == 'oui']\n",
    "record_linkage = record_linkage[['pk_bhp', 'pk_gv']]\n",
    "a.set_types(record_linkage, {'pk_bhp':'int', 'pk_gv':'int'})\n",
    "record_linkage = pd.concat([record_linkage, u.read_df('../../data/record-linkage-bhp-actors-geov-person-uris.csv').drop(columns=['name'])])\n",
    "\n",
    "# For usage: prepare lists for filtering\n",
    "pk_bhp_to_update = record_linkage['pk_bhp'].astype(int).tolist()\n",
    "pk_gv_to_update = record_linkage['pk_gv'].astype(int).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all information from BHP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pk_actor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = u.read_df('../../data/bhp/actor.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symogih URIs\n",
    "uris_symogih = pd.DataFrame()\n",
    "uris_symogih['pk_actor'] = actors['pk_actor']\n",
    "uris_symogih['uri'] = 'http://symogih.org/resource/Actr' + uris_symogih['pk_actor'].astype(str)\n",
    "\n",
    "# URIs that are in BHP\n",
    "db.connect_external(os.environ.get('YELLOW_BHP'))\n",
    "uris_bhp = db.query('select * from bhp.documentation')\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['fk_documented_object'])]\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['fk_documenting_entity'])]\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['identifier'])]\n",
    "uris_bhp = uris_bhp[uris_bhp['fk_documented_object'].str.contains('Actr')]\n",
    "uris_bhp = uris_bhp[uris_bhp['fk_documenting_entity'].str.contains('DiOb')]\n",
    "uris_bhp['fk_documented_object'] = uris_bhp['fk_documented_object'].str.replace('Actr', '')\n",
    "uris_bhp = uris_bhp[['fk_documented_object', 'fk_documenting_entity', 'identifier']]\n",
    "uris_bhp['fk_documenting_entity'] = uris_bhp['fk_documenting_entity'].str.replace('DiOb', '')\n",
    "u.parse_df(uris_bhp)\n",
    "resource_address_concat = u.parse_df(db.query('select * from bhp.resource_address_concatenation')[['fk_digital_object', 'fk_resource_address']])\n",
    "resource_address = u.parse_df(db.query('select * from bhp.resource_address')[['pk_resource_address', 'uri']])\n",
    "uris_bhp = uris_bhp.merge(resource_address_concat, left_on='fk_documenting_entity', right_on='fk_digital_object', how='left').drop(columns=['fk_documenting_entity', 'fk_digital_object'])\n",
    "uris_bhp = uris_bhp.merge(resource_address, left_on='fk_resource_address', right_on='pk_resource_address', how='left')\n",
    "uris_bhp['uri'] = uris_bhp['uri'] + uris_bhp['identifier']\n",
    "uris_bhp = uris_bhp[['fk_documented_object', 'uri']]\n",
    "uris_bhp.dropna(subset=['uri'], inplace=True)\n",
    "uris_bhp.rename(columns={'fk_documented_object': 'pk_actor'}, inplace=True)\n",
    "\n",
    "# All together\n",
    "uris = pd.concat([uris_symogih, uris_bhp]).sort_values('pk_actor').reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = u.read_df('../../data/bhp/actor_name.csv')[[ 'fk_actor', 'concat_name', 'lang_iso', 'begin_date', 'end_date']]\n",
    "names.columns = ['pk_actor', 'name', 'lang', 'begin', 'end']\n",
    "names['lang'].replace('None', 'fra', inplace=True, regex=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = actors[['pk_actor', 'gender_iso']]\n",
    "genders = genders[genders['gender_iso'] != 0]\n",
    "genders['gender'] = ['Male' if iso == 1 else 'Female' for iso in genders['gender_iso']]\n",
    "genders.drop(columns=['gender_iso'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = u.read_df('../../data/bhp/actor_text_property.csv')[['fk_actor', 'lang_iso_code', 'text']]\n",
    "definitions.rename(columns={'lang_iso_code': 'lang', 'fk_actor':'pk_actor'}, inplace=True)\n",
    "definitions['lang'] = definitions['lang'].str.replace('None', 'fra')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert new actors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Geovistory database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.connect_geovistory(env, pk_project, execute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out those who only need updates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_create = pd.DataFrame(columns=['pk_actor'])\n",
    "to_create['pk_actor'] = actors['pk_actor']\n",
    "to_create = to_create[[pk not in pk_bhp_to_update for pk in to_create['pk_actor'].tolist()]]\n",
    "\n",
    "print('Actor nb to create:', len(to_create))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_create['pk_person'] = db.resources.create(pks.classes.person, len(to_create))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add URIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris_to_create = to_create.merge(uris, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'uri'])\n",
    "\n",
    "graphs.add_uris(\n",
    "    uris_to_create['pk_person'].tolist(),\n",
    "    uris_to_create['uri'].tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add person names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Person appellation in a language\n",
    "names_to_create = to_create.merge(names, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'name'])\n",
    "names_to_create['lang'] = [pks.languages.from_iso_code(iso_code if pd.notna(iso_code) else 'fra') for iso_code in names_to_create['lang']]\n",
    "\n",
    "names_to_create['pk_paial'] = graphs.add_person_names(\n",
    "    names_to_create['pk_person'].tolist(),\n",
    "    names_to_create['name'].tolist(),\n",
    "    names_to_create['lang'].tolist(),\n",
    "    return_pk_paial=True\n",
    ")\n",
    "\n",
    "\n",
    "# For those with a begin date, add begin date\n",
    "names_with_begin = names_to_create[pd.notna(names_to_create['begin'])][['pk_paial', 'begin']]\n",
    "names_with_begin['begin'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_with_begin['begin']]\n",
    "\n",
    "time_prims_begin = db.time_primitives.create(\n",
    "    names_with_begin['begin'].tolist(),\n",
    "    '1 day'\n",
    ")\n",
    "db.statements.create(\n",
    "    names_with_begin['pk_paial'].tolist(),\n",
    "    pks.properties.timespan_endOfTheBegin_timePrim,\n",
    "    time_prims_begin\n",
    ")\n",
    "\n",
    "\n",
    "# For those with a end date, add end date\n",
    "names_with_end = names_to_create[pd.notna(names_to_create['end'])][['pk_paial', 'end']]\n",
    "names_with_end['end'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_with_end['end']]\n",
    "\n",
    "time_prims_end = db.time_primitives.create(\n",
    "    names_with_end['end'].tolist(),\n",
    "    '1 day'\n",
    ")\n",
    "db.statements.create(\n",
    "    names_with_end['pk_paial'].tolist(),\n",
    "    pks.properties.timespan_beginOfTheEnd_timePrim,\n",
    "    time_prims_end\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add genders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders_to_create = to_create.merge(genders, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'gender'])\n",
    "genders_to_create['gender'] = genders_to_create['gender'].replace('Male', pks.entities.pk_gender_male)\n",
    "genders_to_create['gender'] = genders_to_create['gender'].replace('Female', pks.entities.pk_gender_female)\n",
    "\n",
    "db.statements.create(\n",
    "    genders_to_create['pk_person'].tolist(),\n",
    "    pks.properties.person_hasGender_gender,\n",
    "    genders_to_create['gender'].tolist()   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions_to_create = to_create.merge(definitions, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'text'])\n",
    "definitions_to_create.dropna(subset=['pk_person', 'text'], inplace=True)\n",
    "definitions_to_create['pk_lang'] = [pks.languages.from_iso_code(iso_code if pd.notna(iso_code) else 'fra') for iso_code in definitions_to_create['lang']]\n",
    "\n",
    "graphs.add_definitions(\n",
    "    definitions_to_create['pk_person'].tolist(),\n",
    "    definitions_to_create['text'].tolist(),\n",
    "    definitions_to_create['pk_lang'].tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch information from Geovistory (for existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Geovistory database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.connect_geovistory('prod', pk_project, execute, skip_protection=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = '(' + ','.join(map(lambda s: str(s), pk_gv_to_update)) + ')'\n",
    "\n",
    "persons = db.query(f\"\"\"\n",
    "    select\n",
    "        r1.pk_entity as pk_person,\n",
    "        -- URIs\n",
    "        s1.fk_object_info as pk_uri,\n",
    "        a1.string as uri,\n",
    "        s1.pk_entity as uri_pk_statement_1,\n",
    "        s2.pk_entity as uri_pk_statement_2,\n",
    "        a1.pk_entity as uri_pk_appellation,\n",
    "        -- Names\n",
    "        s3.pk_entity as name_pk_statement_3,\n",
    "        s3.fk_subject_info as pk_paial,\n",
    "        s4.pk_entity as name_pk_statement_4,\n",
    "        a2.pk_entity as name_pk_appellation_2,\n",
    "        a2.string as name,\n",
    "        s5.pk_entity as name_pk_statement_5,\n",
    "        s5.fk_object_info as pk_name_lang,\n",
    "        s6.pk_entity as name_pk_statement_6,\n",
    "        tp1.pk_entity as name_pk_tp_begin,\n",
    "        tp1.julian_day as name_begin_jd,\n",
    "        s7.pk_entity as name_pk_statement_7,\n",
    "        tp2.pk_entity as name_pk_tp_end,\n",
    "        tp2.julian_day as name_end_jd,\n",
    "        -- Gender\n",
    "        s8.pk_entity as gender_pk_statement,\n",
    "        s8.fk_object_info as pk_gender,\n",
    "        -- Definition\n",
    "        s10.fk_object_info as pk_def_lang,\n",
    "        a3.string as definition\n",
    "    from information.resource r1\n",
    "    -- URIs\n",
    "    left join information.statement s1 on s1.fk_subject_info = r1.pk_entity and s1.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    left join information.statement s2 on s2.fk_subject_info = s1.fk_object_info and s2.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    left join information.appellation a1 on a1.pk_entity = s2.fk_object_info\n",
    "    -- Names\n",
    "    left join information.statement s3 on s3.fk_object_info = r1.pk_entity and s3.fk_property = {pks.properties.apial_isAppelationForLanguageOf_entity}\n",
    "    left join information.statement s4 on s4.fk_subject_info = s3.fk_subject_info and s4.fk_property = {pks.properties.aial_refersToName_appellation}\n",
    "    left join information.appellation a2 on a2.pk_entity = s4.fk_object_info\n",
    "    left join information.statement s5 on s5.fk_subject_info = s3.fk_subject_info and s5.fk_property = {pks.properties.apial_usedInLanguage_language}\n",
    "    left join information.statement s6 on s6.fk_subject_info = s3.fk_subject_info and s6.fk_property = {pks.properties.timespan_endOfTheBegin_timePrim}\n",
    "    left join information.time_primitive tp1 on tp1.pk_entity = s6.fk_object_info\n",
    "    left join information.statement s7 on s7.fk_subject_info = s3.fk_subject_info and s7.fk_property = {pks.properties.timespan_beginOfTheEnd_timePrim}\n",
    "    left join information.time_primitive tp2 on tp2.pk_entity = s7.fk_object_info\n",
    "    -- Genders\n",
    "    left join information.statement s8 on s8.fk_subject_info = r1.pk_entity and s8.fk_property = {pks.properties.person_hasGender_gender}\n",
    "    -- Definitions\n",
    "    left join information.statement s9 on s9.fk_subject_info = r1.pk_entity and s9.fk_property = {pks.properties.entity_hasDefinition_text}\n",
    "    left join information.statement s10 on s10.fk_subject_info = s9.fk_object_info and s10.fk_property = {pks.properties.linguisticObj_hasLanguage_language}\n",
    "    left join information.statement s11 on s11.fk_subject_info = s9.fk_object_info and s11.fk_property = {pks.properties.text_hasValueVersion_string}\n",
    "    left join information.appellation a3 on a3.pk_entity = s11.fk_object_info\n",
    "\n",
    "    where r1.pk_entity in {values}\n",
    "\"\"\")\n",
    "\n",
    "a.set_types(persons, {'pk_name_lang':'int', 'name_begin_jd':'int', 'name_end_jd':'int', 'pk_gender':'int', 'pk_def_lang':'int', 'definition':'string'})\n",
    "# URI                   \n",
    "a.set_types(persons, {'pk_uri': 'int', 'uri':'string', 'uri_pk_statement_1':'int', 'uri_pk_statement_2':'int', 'uri_pk_appellation':'int'})\n",
    "\n",
    "# Join the pk bhp\n",
    "persons = persons.merge(record_linkage, left_on='pk_person', right_on='pk_gv').drop(columns=['pk_gv']).drop_duplicates()\n",
    "\n",
    "a.infos(persons, random=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disconnect from production**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete existing information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Geovistory database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.connect_geovistory(env, pk_project, execute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add person to project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.info_proj_rels.create(pk_gv_to_update)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URIs: add to project existing, create non existing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the record linkage, we get the URIs that we want to create (or add to the project)\n",
    "record_linkage_uri_bhp = record_linkage.merge(uris, left_on='pk_bhp', right_on='pk_actor', how='inner').drop(columns=['pk_actor'])\n",
    "\n",
    "# Filter already existing URIs:\n",
    "uris_gv = persons[['pk_bhp', 'pk_person', 'pk_uri', 'uri', 'uri_pk_statement_1', 'uri_pk_statement_2', 'uri_pk_appellation']].dropna().drop_duplicates()\n",
    "uris_add_to_project = record_linkage_uri_bhp.merge(uris_gv.drop(columns=['pk_bhp', 'pk_person']), on='uri', how='inner')\n",
    "\n",
    "# Add info to project\n",
    "db.info_proj_rels.create(uris_add_to_project['pk_uri'].astype(int).tolist())\n",
    "db.info_proj_rels.create(uris_add_to_project['uri_pk_statement_1'].astype(int).tolist())\n",
    "db.info_proj_rels.create(uris_add_to_project['uri_pk_statement_2'].astype(int).tolist())\n",
    "db.info_proj_rels.create(uris_add_to_project['uri_pk_appellation'].astype(int).tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_existing = uris_add_to_project['uri'].tolist()\n",
    "uris_to_create = record_linkage_uri_bhp[[u not in uri_existing for u in record_linkage_uri_bhp['uri']]]\n",
    "\n",
    "graphs.add_uris(\n",
    "    uris_to_create['pk_gv'].astype(int).tolist(),\n",
    "    uris_to_create['uri'].astype(str).tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Names: add to project existing, create non existing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_gv = persons[['pk_person', 'pk_paial', 'pk_name_lang', 'name', 'name_begin_jd', 'name_end_jd', 'name_pk_statement_3', 'name_pk_statement_4', 'name_pk_appellation_2', 'name_pk_statement_5', 'name_pk_statement_6', 'name_pk_tp_begin', 'name_pk_statement_7', 'name_pk_tp_end']]\n",
    "names_add_to_project = []\n",
    "names_to_create = []\n",
    "\n",
    "for i, row in record_linkage.iterrows():\n",
    "    select_bhp = names[names['pk_actor'] == row['pk_bhp']].drop_duplicates(subset=['name', 'lang'])\n",
    "    select_gv = names_gv[names_gv['pk_person'] == row['pk_gv']].drop_duplicates(subset=['name', 'pk_name_lang'])\n",
    "\n",
    "    for j, row_bhp in select_bhp.iterrows(): \n",
    "        lang = pks.languages.from_iso_code(row_bhp['lang']) if pd.notna(row_bhp['lang']) else pks.languages.from_iso_code('fra')\n",
    "        name_bhp = ' '.join(sorted(row_bhp['name'].replace(',', '').lower().split(' '))) + ' - ' + str(lang)\n",
    "        # print(name_bhp)\n",
    "        found = False\n",
    "\n",
    "        if pd.isna(row_bhp['begin']) and pd.isna(row_bhp['end']):\n",
    "            for k, row_gv in select_gv.iterrows():\n",
    "                name_gv = ' '.join(sorted(row_gv['name'].replace(',', '').lower().split(' '))) + ' - ' + str(row_gv['pk_name_lang'])\n",
    "                # print(name_gv)\n",
    "                if name_bhp == name_gv:\n",
    "                    found = True\n",
    "                    names_add_to_project.append({\n",
    "                        'pk_person': row['pk_gv'],\n",
    "                        'name': row_bhp['name'],\n",
    "                        'begin': row_bhp['begin'],\n",
    "                        'end': row_bhp['end'],\n",
    "                        'pk_paial': row_gv['pk_paial'],\n",
    "                        'pk_tp_begin': row_gv['name_pk_tp_begin'],\n",
    "                        'pk_tp_end': row_gv['name_pk_tp_end'],\n",
    "                        'stmt3': row_gv['name_pk_statement_3'],\n",
    "                        'stmt4': row_gv['name_pk_statement_4'],\n",
    "                        'appe2': row_gv['name_pk_appellation_2'],\n",
    "                        'stmt5': row_gv['name_pk_statement_5'],\n",
    "                        'stmt6': row_gv['name_pk_statement_6'],\n",
    "                        'stmt7': row_gv['name_pk_statement_7'],\n",
    "                    })\n",
    "        \n",
    "        if not found:\n",
    "            names_to_create.append({\n",
    "                'pk_person': row['pk_gv'],\n",
    "                'name': row_bhp['name'],\n",
    "                'pk_lang': lang,\n",
    "                'begin': row_bhp['begin'],\n",
    "                'end': row_bhp['end'],\n",
    "            })\n",
    "\n",
    "    # display(select_bhp)\n",
    "    # display(select_gv)\n",
    "\n",
    "    \n",
    "names_to_create = pd.DataFrame(data=names_to_create)\n",
    "names_add_to_project = pd.DataFrame(data=names_add_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create information - new names\n",
    "names_to_create['pk_paial'] = graphs.add_person_names(\n",
    "    names_to_create['pk_person'].astype(int).tolist(),\n",
    "    names_to_create['name'].astype(str).tolist(),\n",
    "    names_to_create['pk_lang'].astype(int).tolist()\n",
    ")\n",
    "\n",
    "# Create information - new names - begin\n",
    "names_begin = names_to_create[pd.notna(names_to_create['begin'])].copy()\n",
    "names_begin['begin'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_begin['begin']]\n",
    "names_begin.dropna(subset=['pk_paial', 'end'], inplace=True)\n",
    "tp_begin = db.time_primitives.create(names_begin['begin'].tolist(), '1 day')\n",
    "db.statements.create(\n",
    "    names_begin['pk_paial'].astype(int).tolist(),\n",
    "    pks.properties.timespan_endOfTheBegin_timePrim,\n",
    "    tp_begin    \n",
    ")\n",
    "\n",
    "# Create information - new names - end\n",
    "names_end = names_to_create[pd.notna(names_to_create['end'])].copy()\n",
    "names_end['end'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_end['end']]\n",
    "names_end.dropna(subset=['pk_paial', 'end'], inplace=True)\n",
    "tp_end = db.time_primitives.create(names_end['end'].tolist(), '1 day')\n",
    "db.statements.create(\n",
    "    names_end['pk_paial'].astype(int).tolist(),\n",
    "    pks.properties.timespan_endOfTheBegin_timePrim,\n",
    "    tp_end    \n",
    ")\n",
    "\n",
    "\n",
    "# Create information - add to project - names\n",
    "db.info_proj_rels.create(names_add_to_project['stmt3'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['pk_paial'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['stmt4'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['appe2'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['stmt5'].astype(int).tolist())\n",
    "\n",
    "# Create information - add to project - begin\n",
    "names_add_to_project_dates_begin = names_add_to_project.dropna(subset=['pk_tp_begin'])\n",
    "db.info_proj_rels.create(names_add_to_project_dates_begin['stmt6'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project_dates_begin['pk_tp_begin'].astype(int).tolist())\n",
    "\n",
    "# Create information - add to project - end\n",
    "names_add_to_project_dates_end = names_add_to_project.dropna(subset=['pk_tp_end'])\n",
    "db.info_proj_rels.create(names_add_to_project_dates_end['stmt7'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project_dates_end['pk_tp_end'].astype(int).tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender: add to project existing, create non existing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_linkage_gender_bhp = record_linkage.merge(genders, left_on='pk_bhp', right_on='pk_actor', how='inner').drop(columns=['pk_actor'])\n",
    "genders_gv = persons[['pk_person', 'pk_gender', 'gender_pk_statement']].drop_duplicates()\n",
    "\n",
    "genders_to_update = record_linkage_gender_bhp.merge(genders_gv, left_on='pk_gv', right_on='pk_gender', how='left')\n",
    "genders_to_update['gender'] = genders_to_update['gender'].replace('Male', pks.entities.pk_gender_male)\n",
    "genders_to_update['gender'] = genders_to_update['gender'].replace('Female', pks.entities.pk_gender_female)\n",
    "\n",
    "genders_add_to_project = genders_to_update[pd.notna(genders_to_update['pk_gender'])]\n",
    "genders_to_create = genders_to_update[pd.isna(genders_to_update['pk_gender'])]\n",
    "\n",
    "# Add existing\n",
    "if len(genders_add_to_project) > 0: db.info_proj_rels.create(genders_add_to_project['gender_pk_statement'].astype(int).tolist())\n",
    "\n",
    "# Create new ones\n",
    "db.statements.create(\n",
    "    genders_to_create['pk_gv'].astype(int).tolist(),\n",
    "    pks.properties.person_hasGender_gender,\n",
    "    genders_to_create['gender'].astype(int).tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_linkage_definition_bhp = record_linkage.merge(definitions, left_on='pk_bhp', right_on='pk_actor', how='inner').drop(columns=['pk_actor'])\n",
    "record_linkage_definition_bhp['pk_lang'] = [pks.languages.from_iso_code(code) for code in record_linkage_definition_bhp['lang']]\n",
    "\n",
    "# Create new definitions\n",
    "graphs.add_definitions(\n",
    "    record_linkage_definition_bhp['pk_gv'].astype(int).tolist(),\n",
    "    record_linkage_definition_bhp['text'].astype(str).tolist(),\n",
    "    record_linkage_definition_bhp['pk_lang'].astype(int).tolist(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
