{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = ''\n",
    "pk_project = 6857901\n",
    "execute = True\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "\n",
    "import geovpylib.analysis as a\n",
    "import geovpylib.database as db\n",
    "import geovpylib.graphs as graphs\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.recordlinkage as rl\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.utils as u\n",
    "\n",
    "eta = u.Eta()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BHP actors into Geovistory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record linkage result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch record linkage and keep only the BHP key and the GEOV key\n",
    "record_linkage = pd.read_csv('../../data/record-linkage-bhp-actors-geov-persons-filled.csv')\n",
    "record_linkage = record_linkage[record_linkage['doublon'] == 'oui']\n",
    "record_linkage = record_linkage[['pk_bhp', 'pk_gv']]\n",
    "a.set_types(record_linkage, {'pk_bhp':'int', 'pk_gv':'int'})\n",
    "record_linkage = pd.concat([record_linkage, u.read_df('../../data/record-linkage-bhp-actors-geov-person-uris.csv').drop(columns=['name'])])\n",
    "\n",
    "# For usage: prepare lists for filtering\n",
    "pk_bhp_to_update = record_linkage['pk_bhp'].astype(int).tolist()\n",
    "pk_gv_to_update = record_linkage['pk_gv'].astype(int).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all information from BHP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pk_actor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = u.read_df('../../data/bhp/actor.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Connecting to PGSQL Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "# Symogih URIs\n",
    "uris_symogih = pd.DataFrame()\n",
    "uris_symogih['pk_actor'] = actors['pk_actor']\n",
    "uris_symogih['uri'] = 'http://symogih.org/resource/Actr' + uris_symogih['pk_actor'].astype(str)\n",
    "\n",
    "# URIs that are in BHP\n",
    "db.connect_external(os.environ.get('YELLOW_BHP'))\n",
    "uris_bhp = db.query('select * from bhp.documentation')\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['fk_documented_object'])]\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['fk_documenting_entity'])]\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['identifier'])]\n",
    "uris_bhp = uris_bhp[uris_bhp['fk_documented_object'].str.contains('Actr')]\n",
    "uris_bhp = uris_bhp[uris_bhp['fk_documenting_entity'].str.contains('DiOb')]\n",
    "uris_bhp['fk_documented_object'] = uris_bhp['fk_documented_object'].str.replace('Actr', '')\n",
    "uris_bhp = uris_bhp[['fk_documented_object', 'fk_documenting_entity', 'identifier']]\n",
    "uris_bhp['fk_documenting_entity'] = uris_bhp['fk_documenting_entity'].str.replace('DiOb', '')\n",
    "u.parse_df(uris_bhp)\n",
    "resource_address_concat = u.parse_df(db.query('select * from bhp.resource_address_concatenation')[['fk_digital_object', 'fk_resource_address']])\n",
    "resource_address = u.parse_df(db.query('select * from bhp.resource_address')[['pk_resource_address', 'uri']])\n",
    "uris_bhp = uris_bhp.merge(resource_address_concat, left_on='fk_documenting_entity', right_on='fk_digital_object', how='left').drop(columns=['fk_documenting_entity', 'fk_digital_object'])\n",
    "uris_bhp = uris_bhp.merge(resource_address, left_on='fk_resource_address', right_on='pk_resource_address', how='left')\n",
    "uris_bhp['uri'] = uris_bhp['uri'] + uris_bhp['identifier']\n",
    "uris_bhp = uris_bhp[['fk_documented_object', 'uri']]\n",
    "uris_bhp.dropna(subset=['uri'], inplace=True)\n",
    "uris_bhp.rename(columns={'fk_documented_object': 'pk_actor'}, inplace=True)\n",
    "\n",
    "# All together\n",
    "uris = pd.concat([uris_symogih, uris_bhp]).sort_values('pk_actor').reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = u.read_df('../../data/bhp/actor_name.csv')[[ 'fk_actor', 'concat_name', 'lang_iso', 'begin_date', 'end_date']]\n",
    "names.columns = ['pk_actor', 'name', 'lang', 'begin', 'end']\n",
    "names['lang'].replace('None', 'fra', inplace=True, regex=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = actors[['pk_actor', 'gender_iso']]\n",
    "genders = genders[genders['gender_iso'] != 0]\n",
    "genders['gender'] = ['Male' if iso == 1 else 'Female' for iso in genders['gender_iso']]\n",
    "genders.drop(columns=['gender_iso'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = u.read_df('../../data/bhp/actor_text_property.csv')[['fk_actor', 'lang_iso_code', 'text']]\n",
    "definitions.rename(columns={'lang_iso_code': 'lang', 'fk_actor':'pk_actor'}, inplace=True)\n",
    "definitions['lang'] = definitions['lang'].str.replace('None', 'fra')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert new actors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Geovistory database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Connecting to PRODUCTION Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory(env, pk_project, execute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out those who only need updates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor nb to create: 58344\n"
     ]
    }
   ],
   "source": [
    "to_create = pd.DataFrame(columns=['pk_actor'])\n",
    "to_create['pk_actor'] = actors['pk_actor']\n",
    "to_create = to_create[[pk not in pk_bhp_to_update for pk in to_create['pk_actor'].tolist()]]\n",
    "\n",
    "print('Actor nb to create:', len(to_create))\n",
    "\n",
    "# 1m10s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 58344 resources of class [21] ... Done in [00h00'04]\n",
      "Creating info_proj_rel of 58344 entities with project <6857901> ... Done in [00h00'12]\n"
     ]
    }
   ],
   "source": [
    "to_create['pk_person'] = db.resources.create(pks.classes.person, len(to_create))\n",
    "\n",
    "# 14s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add URIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 63521 resources of class [967] ... Done in [00h00'05]\n",
      "Creating info_proj_rel of 63521 entities with project <6857901> ... Done in [00h00'12]\n",
      "Creating 63521 appellations ... Done in [00h00'31]\n",
      "Creating info_proj_rel of 63521 entities with project <6857901> ... Done in [00h00'13]\n",
      "Creating 63521 statements ... Done in [00h00'10]\n",
      "Creating info_proj_rel of 63521 entities with project <6857901> ... Done in [00h00'13]\n",
      "Creating 63521 statements ... Done in [00h00'09]\n",
      "Creating info_proj_rel of 63521 entities with project <6857901> ... Done in [00h00'12]\n"
     ]
    }
   ],
   "source": [
    "uris_to_create = to_create.merge(uris, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'uri'])\n",
    "\n",
    "graphs.add_uris(\n",
    "    uris_to_create['pk_person'].tolist(),\n",
    "    uris_to_create['uri'].tolist()\n",
    ")\n",
    "\n",
    "# 1m50s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add person names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 63942 resources of class [868] ... Done in [00h00'04]\n",
      "Creating info_proj_rel of 63942 entities with project <6857901> ... Done in [00h00'13]\n",
      "Creating 63942 appellations ... Done in [00h00'15]\n",
      "Creating info_proj_rel of 63942 entities with project <6857901> ... Done in [00h00'13]\n",
      "Creating 63942 statements ... Done in [00h00'10]\n",
      "Creating info_proj_rel of 63942 entities with project <6857901> ... Done in [00h00'13]\n",
      "Creating 63942 statements ... Done in [00h01'26]\n",
      "Creating info_proj_rel of 63942 entities with project <6857901> ... Done in [00h00'12]\n",
      "Creating 63942 statements ... Done in [00h00'10]\n",
      "Creating info_proj_rel of 63942 entities with project <6857901> ... Done in [00h00'13]\n",
      "Creating 844 time primitives ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 844 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 844 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 844 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating 341 time primitives ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 341 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 341 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 341 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "# Create Person appellation in a language\n",
    "names_to_create = to_create.merge(names, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'name'])\n",
    "names_to_create['lang'] = [pks.languages.from_iso_code(iso_code if pd.notna(iso_code) else 'fra') for iso_code in names_to_create['lang']]\n",
    "\n",
    "names_to_create['pk_paial'] = graphs.add_person_names(\n",
    "    names_to_create['pk_person'].tolist(),\n",
    "    names_to_create['name'].tolist(),\n",
    "    names_to_create['lang'].tolist(),\n",
    "    return_pk_paial=True\n",
    ")\n",
    "\n",
    "\n",
    "# For those with a begin date, add begin date\n",
    "names_with_begin = names_to_create[pd.notna(names_to_create['begin'])][['pk_paial', 'begin']]\n",
    "names_with_begin['begin'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_with_begin['begin']]\n",
    "\n",
    "time_prims_begin = db.time_primitives.create(\n",
    "    names_with_begin['begin'].tolist(),\n",
    "    '1 day'\n",
    ")\n",
    "db.statements.create(\n",
    "    names_with_begin['pk_paial'].tolist(),\n",
    "    pks.properties.timespan_endOfTheBegin_timePrim,\n",
    "    time_prims_begin\n",
    ")\n",
    "\n",
    "\n",
    "# For those with a end date, add end date\n",
    "names_with_end = names_to_create[pd.notna(names_to_create['end'])][['pk_paial', 'end']]\n",
    "names_with_end['end'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_with_end['end']]\n",
    "\n",
    "time_prims_end = db.time_primitives.create(\n",
    "    names_with_end['end'].tolist(),\n",
    "    '1 day'\n",
    ")\n",
    "db.statements.create(\n",
    "    names_with_end['pk_paial'].tolist(),\n",
    "    pks.properties.timespan_beginOfTheEnd_timePrim,\n",
    "    time_prims_end\n",
    ")\n",
    "\n",
    "# 3m3s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add genders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 56672 statements ... Done in [00h02'23]\n",
      "Creating info_proj_rel of 56672 entities with project <6857901> ... Done in [00h00'11]\n"
     ]
    }
   ],
   "source": [
    "genders_to_create = to_create.merge(genders, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'gender'])\n",
    "genders_to_create['gender'] = genders_to_create['gender'].replace('Male', pks.entities.pk_gender_male)\n",
    "genders_to_create['gender'] = genders_to_create['gender'].replace('Female', pks.entities.pk_gender_female)\n",
    "\n",
    "db.statements.create(\n",
    "    genders_to_create['pk_person'].tolist(),\n",
    "    pks.properties.person_hasGender_gender,\n",
    "    genders_to_create['gender'].tolist()   \n",
    ")\n",
    "\n",
    "# 2m25s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 52162 resources of class [899] ... Done in [00h00'03]\n",
      "Creating info_proj_rel of 52162 entities with project <6857901> ... Done in [00h00'11]\n",
      "Creating 52162 appellations ... Done in [00h06'41]\n",
      "Creating info_proj_rel of 52162 entities with project <6857901> ... Done in [00h00'10]\n",
      "Creating 52162 statements ... Done in [00h00'08]\n",
      "Creating info_proj_rel of 52162 entities with project <6857901> ... Done in [00h00'11]\n",
      "Creating 52162 statements ... Done in [00h00'34]\n",
      "Creating info_proj_rel of 52162 entities with project <6857901> ... Done in [00h00'11]\n",
      "Creating 52162 statements ... Done in [00h00'08]\n",
      "Creating info_proj_rel of 52162 entities with project <6857901> ... Done in [00h00'11]\n"
     ]
    }
   ],
   "source": [
    "definitions_to_create = to_create.merge(definitions, on=\"pk_actor\", how=\"inner\").drop_duplicates().dropna(subset=['pk_person', 'text'])\n",
    "definitions_to_create.dropna(subset=['pk_person', 'text'], inplace=True)\n",
    "definitions_to_create['pk_lang'] = [pks.languages.from_iso_code(iso_code if pd.notna(iso_code) else 'fra') for iso_code in definitions_to_create['lang']]\n",
    "\n",
    "# To be sure\n",
    "definitions_to_create = definitions_to_create[~definitions_to_create['text'].str.contains('<xml>')]\n",
    "\n",
    "graphs.add_definitions(\n",
    "    definitions_to_create['pk_person'].tolist(),\n",
    "    definitions_to_create['text'].tolist(),\n",
    "    definitions_to_create['pk_lang'].tolist()\n",
    ")\n",
    "\n",
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch information from Geovistory (for existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Geovistory database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Connecting to PRODUCTION Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory('prod', pk_project, execute, skip_protection=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (5628, 29) - extract:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk_person</th>\n",
       "      <th>pk_uri</th>\n",
       "      <th>uri</th>\n",
       "      <th>uri_pk_statement_1</th>\n",
       "      <th>uri_pk_statement_2</th>\n",
       "      <th>uri_pk_appellation</th>\n",
       "      <th>name_pk_statement_3</th>\n",
       "      <th>pk_paial</th>\n",
       "      <th>name_pk_statement_4</th>\n",
       "      <th>name_pk_appellation_2</th>\n",
       "      <th>...</th>\n",
       "      <th>gender_pk_statement</th>\n",
       "      <th>pk_gender</th>\n",
       "      <th>pk_def_statement1</th>\n",
       "      <th>pk_def_statement2</th>\n",
       "      <th>pk_def_statement3</th>\n",
       "      <th>pk_def_appe</th>\n",
       "      <th>pk_def</th>\n",
       "      <th>pk_def_lang</th>\n",
       "      <th>definition</th>\n",
       "      <th>pk_bhp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>787095</td>\n",
       "      <td>6454282</td>\n",
       "      <td>http://d-nb.info/gnd/118974726</td>\n",
       "      <td>6468376</td>\n",
       "      <td>6475423</td>\n",
       "      <td>6461329</td>\n",
       "      <td>802027</td>\n",
       "      <td>790845</td>\n",
       "      <td>805777</td>\n",
       "      <td>795578</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1238413.0</td>\n",
       "      <td>2298910.0</td>\n",
       "      <td>2325694.0</td>\n",
       "      <td>2272126.0</td>\n",
       "      <td>2245342.0</td>\n",
       "      <td>19008</td>\n",
       "      <td>Prêtre. Cartographe. Mathématicien. Professeur...</td>\n",
       "      <td>51730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>786290</td>\n",
       "      <td>6452622</td>\n",
       "      <td>http://d-nb.info/gnd/118740016</td>\n",
       "      <td>6466716</td>\n",
       "      <td>6473763</td>\n",
       "      <td>6459669</td>\n",
       "      <td>801222</td>\n",
       "      <td>790040</td>\n",
       "      <td>804972</td>\n",
       "      <td>795450</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1237444.0</td>\n",
       "      <td>2298150.0</td>\n",
       "      <td>2324934.0</td>\n",
       "      <td>2271366.0</td>\n",
       "      <td>2244582.0</td>\n",
       "      <td>19008</td>\n",
       "      <td>Docteur en médecine de l'Université de Paris. ...</td>\n",
       "      <td>43618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>786290</td>\n",
       "      <td>6452966</td>\n",
       "      <td>http://wikidata.org/entity/Q347699</td>\n",
       "      <td>6467060</td>\n",
       "      <td>6474107</td>\n",
       "      <td>6460013</td>\n",
       "      <td>801222</td>\n",
       "      <td>790040</td>\n",
       "      <td>804972</td>\n",
       "      <td>795450</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1237444.0</td>\n",
       "      <td>2298150.0</td>\n",
       "      <td>2324934.0</td>\n",
       "      <td>2271366.0</td>\n",
       "      <td>2244582.0</td>\n",
       "      <td>19008</td>\n",
       "      <td>Docteur en médecine de l'Université de Paris. ...</td>\n",
       "      <td>43618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>786361</td>\n",
       "      <td>6434140</td>\n",
       "      <td>http://data.bnf.fr/ark:/12148/cb10633692p#about</td>\n",
       "      <td>6441728</td>\n",
       "      <td>6445522</td>\n",
       "      <td>6437934</td>\n",
       "      <td>801293</td>\n",
       "      <td>790111</td>\n",
       "      <td>805043</td>\n",
       "      <td>796528</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1239438.0</td>\n",
       "      <td>2299716.0</td>\n",
       "      <td>2326500.0</td>\n",
       "      <td>2272932.0</td>\n",
       "      <td>2246148.0</td>\n",
       "      <td>19008</td>\n",
       "      <td>Jésuite (à partir de 1628). Professeur de phil...</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>786290</td>\n",
       "      <td>6452235</td>\n",
       "      <td>http://d-nb.info/gnd/118740016</td>\n",
       "      <td>6466329</td>\n",
       "      <td>6473376</td>\n",
       "      <td>6459282</td>\n",
       "      <td>801222</td>\n",
       "      <td>790040</td>\n",
       "      <td>804972</td>\n",
       "      <td>795450</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1237444.0</td>\n",
       "      <td>2298150.0</td>\n",
       "      <td>2324934.0</td>\n",
       "      <td>2271366.0</td>\n",
       "      <td>2244582.0</td>\n",
       "      <td>19008</td>\n",
       "      <td>Docteur en médecine de l'Université de Paris. ...</td>\n",
       "      <td>43618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pk_person   pk_uri                                              uri  \\\n",
       "5495     787095  6454282                   http://d-nb.info/gnd/118974726   \n",
       "4115     786290  6452622                   http://d-nb.info/gnd/118740016   \n",
       "4459     786290  6452966               http://wikidata.org/entity/Q347699   \n",
       "4579     786361  6434140  http://data.bnf.fr/ark:/12148/cb10633692p#about   \n",
       "3728     786290  6452235                   http://d-nb.info/gnd/118740016   \n",
       "\n",
       "      uri_pk_statement_1  uri_pk_statement_2  uri_pk_appellation  \\\n",
       "5495             6468376             6475423             6461329   \n",
       "4115             6466716             6473763             6459669   \n",
       "4459             6467060             6474107             6460013   \n",
       "4579             6441728             6445522             6437934   \n",
       "3728             6466329             6473376             6459282   \n",
       "\n",
       "      name_pk_statement_3  pk_paial  name_pk_statement_4  \\\n",
       "5495               802027    790845               805777   \n",
       "4115               801222    790040               804972   \n",
       "4459               801222    790040               804972   \n",
       "4579               801293    790111               805043   \n",
       "3728               801222    790040               804972   \n",
       "\n",
       "      name_pk_appellation_2  ... gender_pk_statement  pk_gender  \\\n",
       "5495                 795578  ...                 NaN       <NA>   \n",
       "4115                 795450  ...                 NaN       <NA>   \n",
       "4459                 795450  ...                 NaN       <NA>   \n",
       "4579                 796528  ...                 NaN       <NA>   \n",
       "3728                 795450  ...                 NaN       <NA>   \n",
       "\n",
       "      pk_def_statement1  pk_def_statement2  pk_def_statement3  pk_def_appe  \\\n",
       "5495          1238413.0          2298910.0          2325694.0    2272126.0   \n",
       "4115          1237444.0          2298150.0          2324934.0    2271366.0   \n",
       "4459          1237444.0          2298150.0          2324934.0    2271366.0   \n",
       "4579          1239438.0          2299716.0          2326500.0    2272932.0   \n",
       "3728          1237444.0          2298150.0          2324934.0    2271366.0   \n",
       "\n",
       "         pk_def  pk_def_lang  \\\n",
       "5495  2245342.0        19008   \n",
       "4115  2244582.0        19008   \n",
       "4459  2244582.0        19008   \n",
       "4579  2246148.0        19008   \n",
       "3728  2244582.0        19008   \n",
       "\n",
       "                                             definition  pk_bhp  \n",
       "5495  Prêtre. Cartographe. Mathématicien. Professeur...   51730  \n",
       "4115  Docteur en médecine de l'Université de Paris. ...   43618  \n",
       "4459  Docteur en médecine de l'Université de Paris. ...   43618  \n",
       "4579  Jésuite (à partir de 1628). Professeur de phil...    2458  \n",
       "3728  Docteur en médecine de l'Université de Paris. ...   43618  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = '(' + ','.join(map(lambda s: str(s), pk_gv_to_update)) + ')'\n",
    "\n",
    "persons = db.query(f\"\"\"\n",
    "    select\n",
    "        r1.pk_entity as pk_person,\n",
    "        -- URIs\n",
    "        s1.fk_object_info as pk_uri,\n",
    "        a1.string as uri,\n",
    "        s1.pk_entity as uri_pk_statement_1,\n",
    "        s2.pk_entity as uri_pk_statement_2,\n",
    "        a1.pk_entity as uri_pk_appellation,\n",
    "        -- Names\n",
    "        s3.pk_entity as name_pk_statement_3,\n",
    "        s3.fk_subject_info as pk_paial,\n",
    "        s4.pk_entity as name_pk_statement_4,\n",
    "        a2.pk_entity as name_pk_appellation_2,\n",
    "        a2.string as name,\n",
    "        s5.pk_entity as name_pk_statement_5,\n",
    "        s5.fk_object_info as pk_name_lang,\n",
    "        s6.pk_entity as name_pk_statement_6,\n",
    "        tp1.pk_entity as name_pk_tp_begin,\n",
    "        tp1.julian_day as name_begin_jd,\n",
    "        s7.pk_entity as name_pk_statement_7,\n",
    "        tp2.pk_entity as name_pk_tp_end,\n",
    "        tp2.julian_day as name_end_jd,\n",
    "        -- Gender\n",
    "        s8.pk_entity as gender_pk_statement,\n",
    "        s8.fk_object_info as pk_gender,\n",
    "        -- Definition\n",
    "        s9.pk_entity as pk_def_statement1,\n",
    "        s10.pk_entity as pk_def_statement2,\n",
    "        s11.pk_entity as pk_def_statement3,\n",
    "        a3.pk_entity as pk_def_appe,\n",
    "        s9.fk_object_info as pk_def, \n",
    "        s10.fk_object_info as pk_def_lang,\n",
    "        a3.string as definition\n",
    "    from information.resource r1\n",
    "    -- URIs\n",
    "    left join information.statement s1 on s1.fk_subject_info = r1.pk_entity and s1.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    left join information.statement s2 on s2.fk_subject_info = s1.fk_object_info and s2.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    left join information.appellation a1 on a1.pk_entity = s2.fk_object_info\n",
    "    -- Names\n",
    "    left join information.statement s3 on s3.fk_object_info = r1.pk_entity and s3.fk_property = {pks.properties.apial_isAppelationForLanguageOf_entity}\n",
    "    left join information.statement s4 on s4.fk_subject_info = s3.fk_subject_info and s4.fk_property = {pks.properties.aial_refersToName_appellation}\n",
    "    left join information.appellation a2 on a2.pk_entity = s4.fk_object_info\n",
    "    left join information.statement s5 on s5.fk_subject_info = s3.fk_subject_info and s5.fk_property = {pks.properties.apial_usedInLanguage_language}\n",
    "    left join information.statement s6 on s6.fk_subject_info = s3.fk_subject_info and s6.fk_property = {pks.properties.timespan_endOfTheBegin_timePrim}\n",
    "    left join information.time_primitive tp1 on tp1.pk_entity = s6.fk_object_info\n",
    "    left join information.statement s7 on s7.fk_subject_info = s3.fk_subject_info and s7.fk_property = {pks.properties.timespan_beginOfTheEnd_timePrim}\n",
    "    left join information.time_primitive tp2 on tp2.pk_entity = s7.fk_object_info\n",
    "    -- Genders\n",
    "    left join information.statement s8 on s8.fk_subject_info = r1.pk_entity and s8.fk_property = {pks.properties.person_hasGender_gender}\n",
    "    -- Definitions\n",
    "    left join information.statement s9 on s9.fk_subject_info = r1.pk_entity and s9.fk_property = {pks.properties.entity_hasDefinition_text}\n",
    "    left join information.statement s10 on s10.fk_subject_info = s9.fk_object_info and s10.fk_property = {pks.properties.linguisticObj_hasLanguage_language}\n",
    "    left join information.statement s11 on s11.fk_subject_info = s9.fk_object_info and s11.fk_property = {pks.properties.text_hasValueVersion_string}\n",
    "    left join information.appellation a3 on a3.pk_entity = s11.fk_object_info\n",
    "\n",
    "    where r1.pk_entity in {values}\n",
    "\"\"\")\n",
    "\n",
    "a.set_types(persons, {'pk_name_lang':'int', 'name_begin_jd':'int', 'name_end_jd':'int', 'pk_gender':'int', 'pk_def_lang':'int', 'definition':'string'})\n",
    "# URI                   \n",
    "a.set_types(persons, {'pk_uri': 'int', 'uri':'string', 'uri_pk_statement_1':'int', 'uri_pk_statement_2':'int', 'uri_pk_appellation':'int'})\n",
    "\n",
    "# Join the pk bhp\n",
    "persons = persons.merge(record_linkage, left_on='pk_person', right_on='pk_gv').drop(columns=['pk_gv']).drop_duplicates()\n",
    "\n",
    "a.infos(persons, random=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disconnect from production**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database correctly disconnected.\n"
     ]
    }
   ],
   "source": [
    "db.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete existing information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Geovistory database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Connecting to PRODUCTION Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory(env, pk_project, execute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add person to project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating info_proj_rel of 1222 entities with project <6857901> ... Done in [00h00'01]\n"
     ]
    }
   ],
   "source": [
    "db.info_proj_rels.create(pk_gv_to_update)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URIs: add to project existing, create non existing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating info_proj_rel of 1373 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1373 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1373 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1373 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "# From the record linkage, we get the URIs that we want to create (or add to the project)\n",
    "record_linkage_uri_bhp = record_linkage.merge(uris, left_on='pk_bhp', right_on='pk_actor', how='inner').drop(columns=['pk_actor'])\n",
    "\n",
    "# Filter already existing URIs:\n",
    "uris_gv = persons[['pk_bhp', 'pk_person', 'pk_uri', 'uri', 'uri_pk_statement_1', 'uri_pk_statement_2', 'uri_pk_appellation']].dropna().drop_duplicates()\n",
    "uris_add_to_project = record_linkage_uri_bhp.merge(uris_gv.drop(columns=['pk_bhp', 'pk_person']), on='uri', how='inner')\n",
    "\n",
    "# Add info to project\n",
    "db.info_proj_rels.create(uris_add_to_project['pk_uri'].astype(int).tolist())\n",
    "db.info_proj_rels.create(uris_add_to_project['uri_pk_statement_1'].astype(int).tolist())\n",
    "db.info_proj_rels.create(uris_add_to_project['uri_pk_statement_2'].astype(int).tolist())\n",
    "db.info_proj_rels.create(uris_add_to_project['uri_pk_appellation'].astype(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IntegerArray>\n",
       "[2188346,  784464,  784870,  784889,  784908,  300539,  785943,  786900,\n",
       "  787162,  787225,  784001,  784070,  784397,  783808,  783825,  785399,\n",
       "  785490,  785357,  785378,  785478,  786014,  786027,  786664,  785590,\n",
       "  786156,  786384,  786479,  786517,  786854,  784036,  786851,  787036]\n",
       "Length: 32, dtype: Int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1313 resources of class [967] ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1313 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1313 appellations ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1313 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1313 statements ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1313 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1313 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1313 entities with project <6857901> ... Done in [00h00'01]\n"
     ]
    }
   ],
   "source": [
    "uri_existing = uris_add_to_project['uri'].tolist()\n",
    "uris_to_create = record_linkage_uri_bhp[[u not in uri_existing for u in record_linkage_uri_bhp['uri']]]\n",
    "\n",
    "graphs.add_uris(\n",
    "    uris_to_create['pk_gv'].astype(int).tolist(),\n",
    "    uris_to_create['uri'].astype(str).tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Names: add to project existing, create non existing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_gv = persons[['pk_person', 'pk_paial', 'pk_name_lang', 'name', 'name_begin_jd', 'name_end_jd', 'name_pk_statement_3', 'name_pk_statement_4', 'name_pk_appellation_2', 'name_pk_statement_5', 'name_pk_statement_6', 'name_pk_tp_begin', 'name_pk_statement_7', 'name_pk_tp_end']]\n",
    "names_add_to_project = []\n",
    "names_to_create = []\n",
    "\n",
    "for i, row in record_linkage.iterrows():\n",
    "    select_bhp = names[names['pk_actor'] == row['pk_bhp']].drop_duplicates(subset=['name', 'lang'])\n",
    "    select_gv = names_gv[names_gv['pk_person'] == row['pk_gv']].drop_duplicates(subset=['name', 'pk_name_lang'])\n",
    "\n",
    "    for j, row_bhp in select_bhp.iterrows(): \n",
    "        lang = pks.languages.from_iso_code(row_bhp['lang']) if pd.notna(row_bhp['lang']) else pks.languages.from_iso_code('fra')\n",
    "        name_bhp = ' '.join(sorted(row_bhp['name'].replace(',', '').lower().split(' '))) + ' - ' + str(lang)\n",
    "        # print(name_bhp)\n",
    "        found = False\n",
    "\n",
    "        if pd.isna(row_bhp['begin']) and pd.isna(row_bhp['end']):\n",
    "            for k, row_gv in select_gv.iterrows():\n",
    "                name_gv = ' '.join(sorted(row_gv['name'].replace(',', '').lower().split(' '))) + ' - ' + str(row_gv['pk_name_lang'])\n",
    "                # print(name_gv)\n",
    "                if name_bhp == name_gv:\n",
    "                    found = True\n",
    "                    names_add_to_project.append({\n",
    "                        'pk_person': row['pk_gv'],\n",
    "                        'name': row_bhp['name'],\n",
    "                        'begin': row_bhp['begin'],\n",
    "                        'end': row_bhp['end'],\n",
    "                        'pk_paial': row_gv['pk_paial'],\n",
    "                        'pk_tp_begin': row_gv['name_pk_tp_begin'],\n",
    "                        'pk_tp_end': row_gv['name_pk_tp_end'],\n",
    "                        'stmt3': row_gv['name_pk_statement_3'],\n",
    "                        'stmt4': row_gv['name_pk_statement_4'],\n",
    "                        'appe2': row_gv['name_pk_appellation_2'],\n",
    "                        'stmt5': row_gv['name_pk_statement_5'],\n",
    "                        'stmt6': row_gv['name_pk_statement_6'],\n",
    "                        'stmt7': row_gv['name_pk_statement_7'],\n",
    "                    })\n",
    "        \n",
    "        if not found:\n",
    "            names_to_create.append({\n",
    "                'pk_person': row['pk_gv'],\n",
    "                'name': row_bhp['name'],\n",
    "                'pk_lang': lang,\n",
    "                'begin': row_bhp['begin'],\n",
    "                'end': row_bhp['end'],\n",
    "            })\n",
    "\n",
    "    # display(select_bhp)\n",
    "    # display(select_gv)\n",
    "\n",
    "    \n",
    "names_to_create = pd.DataFrame(data=names_to_create)\n",
    "names_add_to_project = pd.DataFrame(data=names_add_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1336 resources of class [868] ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1336 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating 1336 appellations ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1336 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1336 statements ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1336 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1336 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1336 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating 1336 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1336 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 13 time primitives ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 13 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating 13 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 13 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 8 time primitives ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 8 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 8 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 8 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 54 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 54 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 54 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 54 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 54 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 2 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 2 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 2 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 2 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "# Create information - new names\n",
    "names_to_create['pk_paial'] = graphs.add_person_names(\n",
    "    names_to_create['pk_person'].astype(int).tolist(),\n",
    "    names_to_create['name'].astype(str).tolist(),\n",
    "    names_to_create['pk_lang'].astype(int).tolist(),\n",
    "    return_pk_paial = True\n",
    ")\n",
    "\n",
    "# Create information - new names - begin\n",
    "names_begin = names_to_create[pd.notna(names_to_create['begin'])].copy()\n",
    "names_begin['begin'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_begin['begin']]\n",
    "names_begin.dropna(subset=['pk_paial', 'begin'], inplace=True)\n",
    "tp_begin = db.time_primitives.create(names_begin['begin'].tolist(), '1 day')\n",
    "db.statements.create(\n",
    "    names_begin['pk_paial'].astype(int).tolist(),\n",
    "    pks.properties.timespan_endOfTheBegin_timePrim,\n",
    "    tp_begin    \n",
    ")\n",
    "\n",
    "# Create information - new names - end\n",
    "names_end = names_to_create[pd.notna(names_to_create['end'])].copy()\n",
    "names_end['end'] = [(tuple([int(n.strip()) for n in d[1:-1].split(',')])) for d in names_end['end']]\n",
    "names_end.dropna(subset=['pk_paial', 'end'], inplace=True)\n",
    "tp_end = db.time_primitives.create(names_end['end'].tolist(), '1 day')\n",
    "db.statements.create(\n",
    "    names_end['pk_paial'].astype(int).tolist(),\n",
    "    pks.properties.timespan_endOfTheBegin_timePrim,\n",
    "    tp_end    \n",
    ")\n",
    "\n",
    "\n",
    "# Create information - add to project - names\n",
    "db.info_proj_rels.create(names_add_to_project['stmt3'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['pk_paial'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['stmt4'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['appe2'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project['stmt5'].astype(int).tolist())\n",
    "\n",
    "# Create information - add to project - begin\n",
    "names_add_to_project_dates_begin = names_add_to_project.dropna(subset=['pk_tp_begin'])\n",
    "db.info_proj_rels.create(names_add_to_project_dates_begin['stmt6'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project_dates_begin['pk_tp_begin'].astype(int).tolist())\n",
    "\n",
    "# Create information - add to project - end\n",
    "names_add_to_project_dates_end = names_add_to_project.dropna(subset=['pk_tp_end'])\n",
    "db.info_proj_rels.create(names_add_to_project_dates_end['stmt7'].astype(int).tolist())\n",
    "db.info_proj_rels.create(names_add_to_project_dates_end['pk_tp_end'].astype(int).tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender: add to project existing, create non existing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1121 statements ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1121 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "record_linkage_gender_bhp = record_linkage.merge(genders, left_on='pk_bhp', right_on='pk_actor', how='inner').drop(columns=['pk_actor'])\n",
    "genders_gv = persons[['pk_person', 'pk_gender', 'gender_pk_statement']].drop_duplicates()\n",
    "\n",
    "genders_to_update = record_linkage_gender_bhp.merge(genders_gv, left_on='pk_gv', right_on='pk_gender', how='left')\n",
    "genders_to_update['gender'] = genders_to_update['gender'].replace('Male', pks.entities.pk_gender_male)\n",
    "genders_to_update['gender'] = genders_to_update['gender'].replace('Female', pks.entities.pk_gender_female)\n",
    "\n",
    "genders_add_to_project = genders_to_update[pd.notna(genders_to_update['pk_gender'])]\n",
    "genders_to_create = genders_to_update[pd.isna(genders_to_update['pk_gender'])]\n",
    "\n",
    "# Add existing\n",
    "if len(genders_add_to_project) > 0: db.info_proj_rels.create(genders_add_to_project['gender_pk_statement'].astype(int).tolist())\n",
    "\n",
    "# Create new ones\n",
    "db.statements.create(\n",
    "    genders_to_create['pk_gv'].astype(int).tolist(),\n",
    "    pks.properties.person_hasGender_gender,\n",
    "    genders_to_create['gender'].astype(int).tolist()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_linkage_definition_bhp = record_linkage.merge(definitions, left_on='pk_bhp', right_on='pk_actor', how='inner').drop(columns=['pk_actor'])\n",
    "record_linkage_definition_bhp['pk_lang'] = [pks.languages.from_iso_code(code) for code in record_linkage_definition_bhp['lang']]\n",
    "definitions_gv = persons[['pk_person', 'definition', 'pk_def_lang', 'pk_def', 'pk_def_appe', 'pk_def_statement1', 'pk_def_statement2', 'pk_def_statement3']].drop_duplicates().dropna(subset=['definition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching definitions is done - Elapsed: [00h00'31]                   \n"
     ]
    }
   ],
   "source": [
    "definition_add_to_project = []\n",
    "definition_to_create = []\n",
    "\n",
    "eta.begin(len(record_linkage_definition_bhp), 'Matching definitions')\n",
    "for i, row_bhp in record_linkage_definition_bhp.iterrows():\n",
    "    found = False\n",
    "    for j, row_gv in definitions_gv.iterrows():\n",
    "         if row_bhp['pk_gv'] == row_gv['pk_person'] and row_bhp['text'] == row_gv['definition'] and row_bhp['pk_lang'] == row_gv['pk_def_lang']:\n",
    "              found = True\n",
    "              definition_add_to_project.append({\n",
    "                    'pk_def': row_gv['pk_def'],\n",
    "                    'pk_appe': row_gv['pk_def_appe'],\n",
    "                    'pk_stmt1': row_gv['pk_def_statement1'],\n",
    "                    'pk_stmt2': row_gv['pk_def_statement2'],\n",
    "                    'pk_stmt3': row_gv['pk_def_statement3'],\n",
    "                    'definition': row_gv['definition'],\n",
    "                    'pk_person': row_gv['pk_person'],\n",
    "               })\n",
    "              break\n",
    "    \n",
    "    if not found:\n",
    "     definition_to_create.append({\n",
    "          'pk_gv':row_bhp['pk_gv'],\n",
    "          'text': row_bhp['text'],\n",
    "          'pk_lang': row_bhp['pk_lang'],\n",
    "     })\n",
    "     \n",
    "     eta.iter()\n",
    "eta.end()\n",
    "\n",
    "definition_add_to_project = pd.DataFrame(data=definition_add_to_project).drop_duplicates()\n",
    "definition_to_create = pd.DataFrame(data=definition_to_create).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating info_proj_rel of 3 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 3 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 3 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 3 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 3 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "# Add existing definitions\n",
    "db.info_proj_rels.create(definition_add_to_project['pk_def'].astype(int).tolist())\n",
    "db.info_proj_rels.create(definition_add_to_project['pk_appe'].astype(int).tolist())\n",
    "db.info_proj_rels.create(definition_add_to_project['pk_stmt1'].astype(int).tolist())\n",
    "db.info_proj_rels.create(definition_add_to_project['pk_stmt2'].astype(int).tolist())\n",
    "db.info_proj_rels.create(definition_add_to_project['pk_stmt3'].astype(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1252 resources of class [899] ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1252 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1252 appellations ... Done in [00h00'02]\n",
      "Creating info_proj_rel of 1252 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating 1252 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1252 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1252 statements ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 1252 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 1252 statements ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 1252 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "# Create new definitions\n",
    "graphs.add_definitions(\n",
    "    record_linkage_definition_bhp['pk_gv'].astype(int).tolist(),\n",
    "    record_linkage_definition_bhp['text'].astype(str).tolist(),\n",
    "    record_linkage_definition_bhp['pk_lang'].astype(int).tolist(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
