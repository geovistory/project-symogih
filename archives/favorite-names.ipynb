{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Environment not connected.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = 'prod'\n",
    "pk_project = 6857901\n",
    "execute = False\n",
    "metadata_str = ''\n",
    "import_manner = 'one-shot' # 'batch'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "\n",
    "import geovpylib.analysis as a\n",
    "import geovpylib.database as db\n",
    "import geovpylib.queries as q\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.utils as u\n",
    "\n",
    "eta = u.Eta()\n",
    "\n",
    "# db.connect_external(os.getenv(''))\n",
    "db.connect_geovistory(env, pk_project, execute)\n",
    "db.set_metadata({'import-id': datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n",
    "db.set_insert_manner(import_manner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve Favorite names for BHP entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data from BHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/bhp/actor_name.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m actr_names \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49mread_df(\u001b[39m'\u001b[39;49m\u001b[39m../../data/bhp/actor_name.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)[[\u001b[39m'\u001b[39m\u001b[39mfk_actor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis_standard_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mconcat_name\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m actr_names[\u001b[39m'\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m actr_names[\u001b[39m'\u001b[39m\u001b[39mfk_actor\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(pd\u001b[39m.\u001b[39mStringDtype()) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m actr_names[\u001b[39m'\u001b[39m\u001b[39mconcat_name\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m actr_names \u001b[39m=\u001b[39m actr_names[actr_names[\u001b[39m'\u001b[39m\u001b[39mis_standard_name\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/Desktop/geovpylib/utils/dataframe.py:44\u001b[0m, in \u001b[0;36mread_df\u001b[0;34m(path, skip_info, sep, quoting)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_df\u001b[39m(path: \u001b[39mstr\u001b[39m, skip_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, quoting\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[1;32m     43\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read a DataFrame from CSV format.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path, sep\u001b[39m=\u001b[39;49msep, quoting\u001b[39m=\u001b[39;49mquoting, low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m     df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(colname) \u001b[39mif\u001b[39;00m can_be_int(colname) \u001b[39melse\u001b[39;00m colname \u001b[39mfor\u001b[39;00m colname \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns]\n\u001b[1;32m     47\u001b[0m     df \u001b[39m=\u001b[39m parse_df(df)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/bhp/actor_name.csv'"
     ]
    }
   ],
   "source": [
    "actr_names = u.read_df('../../data/bhp/actor_name.csv')[['fk_actor', 'is_standard_name', 'concat_name']]\n",
    "actr_names['key'] = actr_names['fk_actor'].astype(pd.StringDtype()) + '-' + actr_names['concat_name']\n",
    "actr_names = actr_names[actr_names['is_standard_name']]\n",
    "actr_std_names_set = set(actr_names['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coac_names = u.read_df('../../data/bhp/collective-actor-name.csv')[['fk_collective_actor', 'is_standard_name', 'name']]\n",
    "coac_names['key'] = coac_names['fk_collective_actor'].astype(pd.StringDtype()) + '-' + coac_names['name']\n",
    "coac_names = coac_names[coac_names['is_standard_name']]\n",
    "coac_std_names_set = set(coac_names['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from GV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = db.query(f\"\"\"\n",
    "    select\n",
    "        r1.pk_entity as pk_gv, r1.fk_class, a4.string as name, ipr2.pk_entity as pk_ipr, a7.string as uri\n",
    "    from information.resource r1\n",
    "    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n",
    "    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.aial_isAppelationForLanguageOf_entity}\n",
    "    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n",
    "    inner join information.statement s3 on s3.fk_subject_info = s2.fk_subject_info and s3.fk_property = {pks.properties.aial_refersToName_appellation}\n",
    "    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n",
    "    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n",
    "    inner join information.statement s5 on s5.fk_subject_info = r1.pk_entity and s5.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    inner join projects.info_proj_rel ipr5 on ipr5.fk_entity = s5.pk_entity and ipr5.fk_project = {pk_project} and ipr5.is_in_project = true\n",
    "    inner join information.statement s6 on s6.fk_subject_info = s5.fk_object_info and s6.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    inner join projects.info_proj_rel ipr6 on ipr6.fk_entity = s6.pk_entity and ipr6.fk_project = {pk_project} and ipr6.is_in_project = true\n",
    "    inner join information.appellation a7 on a7.pk_entity = s6.fk_object_info\n",
    "    where r1.fk_class = {pks.classes.person}\n",
    "\"\"\").sort_values('pk_gv')\n",
    "\n",
    "persons = persons[persons['uri'].str.contains('http://symogih.org')]\n",
    "persons['pk_bhp'] = [s.replace('http://symogih.org/resource/Actr', '') for s in persons['uri']]\n",
    "persons['key'] = persons['pk_bhp'] + '-' + persons['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = db.query(f\"\"\"\n",
    "    select\n",
    "        r1.pk_entity as pk_gv, r1.fk_class, a4.string as name, ipr2.pk_entity as pk_ipr, a7.string as uri\n",
    "    from information.resource r1\n",
    "    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n",
    "    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.aial_isAppelationForLanguageOf_entity}\n",
    "    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n",
    "    inner join information.statement s3 on s3.fk_subject_info = s2.fk_subject_info and s3.fk_property = {pks.properties.aial_refersToName_appellation}\n",
    "    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n",
    "    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n",
    "    inner join information.statement s5 on s5.fk_subject_info = r1.pk_entity and s5.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    inner join projects.info_proj_rel ipr5 on ipr5.fk_entity = s5.pk_entity and ipr5.fk_project = {pk_project} and ipr5.is_in_project = true\n",
    "    inner join information.statement s6 on s6.fk_subject_info = s5.fk_object_info and s6.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    inner join projects.info_proj_rel ipr6 on ipr6.fk_entity = s6.pk_entity and ipr6.fk_project = {pk_project} and ipr6.is_in_project = true\n",
    "    inner join information.appellation a7 on a7.pk_entity = s6.fk_object_info\n",
    "    where r1.fk_class = {pks.classes.group}\n",
    "\"\"\").sort_values('pk_gv')\n",
    "\n",
    "groups = groups[groups['uri'].str.contains('http://symogih.org')]\n",
    "groups['pk_bhp'] = [s.replace('http://symogih.org/resource/CoAc', '') for s in groups['uri']]\n",
    "groups['key'] = groups['pk_bhp'] + '-' + groups['name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_names_actr = actr_names[actr_names['is_standard_name']]['key'].tolist()\n",
    "persons['is_standard'] = [row['key'] in std_names_actr for _, row in persons.iterrows()]\n",
    "\n",
    "# 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_names_coac = coac_names[coac_names['is_standard_name']]['key'].tolist()\n",
    "groups['is_standard'] = [row['key'] in std_names_coac for _, row in groups.iterrows()]\n",
    "\n",
    "# 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean IPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr_list =  u.get_sql_ready_str(persons['pk_ipr'])\n",
    "\n",
    "db.execute(f\"\"\"\n",
    "    update projects.info_proj_rel\n",
    "        set ord_num_of_domain = NULL\n",
    "    where pk_entity in {ipr_list};\n",
    "\"\"\")\n",
    "\n",
    "# 1m17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr_list =  u.get_sql_ready_str(groups['pk_ipr'])\n",
    "\n",
    "db.execute(f\"\"\"\n",
    "    update projects.info_proj_rel\n",
    "        set ord_num_of_domain = NULL\n",
    "    where pk_entity in {ipr_list};\n",
    "\"\"\")\n",
    "\n",
    "# 23s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_std = persons[persons['is_standard']]\n",
    "ipr_list =  u.get_sql_ready_str(persons_std['pk_ipr'])\n",
    "\n",
    "db.execute(f\"\"\"\n",
    "    update projects.info_proj_rel\n",
    "        set ord_num_of_domain = 1\n",
    "    where pk_entity in {ipr_list};\n",
    "\"\"\")\n",
    "\n",
    "# 1m8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_std = groups[groups['is_standard']]\n",
    "ipr_list =  u.get_sql_ready_str(groups_std['pk_ipr'])\n",
    "\n",
    "db.execute(f\"\"\"\n",
    "    update projects.info_proj_rel\n",
    "        set ord_num_of_domain = 1\n",
    "    where pk_entity in {ipr_list};\n",
    "\"\"\")\n",
    "\n",
    "# 24s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
