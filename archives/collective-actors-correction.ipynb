{"cells":[{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["# %load /home/gaetan/Desktop/geovpylib/templates/heading.py\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Common imports\n","import os\n","import pandas as pd, numpy as np\n","import datetime\n","#import json\n","#import request\n","#import duckdb\n","#import plotly.express as px\n","\n","# Geovpylib library\n","import geovpylib.analysis as a\n","import geovpylib.database as db\n","import geovpylib.decorators as d\n","import geovpylib.magics\n","import geovpylib.pks as pks\n","import geovpylib.queries as q\n","import geovpylib.sparql as sparql\n","import geovpylib.utils as u\n","eta = u.Eta()\n","\n","\n","env = 'prod'\n","pk_project = pks.projects.symogih\n","execute = True\n","metadata_str = 'collective-actor-correction'\n","import_manner = 'one-shot'"]},{"cell_type":"markdown","metadata":{},"source":["# Collective Actors Correction"]},{"cell_type":"markdown","metadata":{},"source":["## 1./ Delete everything"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Connecting to PRODUCTION Database ... Connected!\n"]}],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.0/ Fetch general data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["existing_groups = db.query(f\"\"\"\n","    select \n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","existing_groups['pk_bhp'] = [string[string.rfind('/') + 1:] for string in existing_groups['uri']]\n","existing_groups.drop(columns='uri', inplace=True)\n","existing_groups = existing_groups[existing_groups['pk_bhp'].str.contains('CoAc')]\n","\n","pk_groups_str = u.get_sql_ready_str(existing_groups['pk_group'])\n","\n","# a.infos(existing_groups)\n","\n","#########\n","\n","existing_formations = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_formation,\n","        ipr2.pk_entity as pk_ipr_has_formed_group\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.formation_hasFormed_group} and s2.fk_object_info in {pk_groups_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    where r1.fk_class = {pks.classes.formation}\n","\"\"\")\n","pk_formations_str = u.get_sql_ready_str(existing_formations['pk_formation'])\n","\n","# a.infos(existing_formations)\n","\n","#########\n","\n","existing_dissolutions = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_dissolution,\n","        ipr2.pk_entity as pk_ipr_has_dissolved_group\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.dissolution_dissolved_group} and s2.fk_object_info in {pk_groups_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    where r1.fk_class = {pks.classes.dissolution}\n","\"\"\")\n","pk_dissolution_str = u.get_sql_ready_str(existing_dissolutions['pk_dissolution'])\n","\n","# a.infos(existing_dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.1.a/ Delete `has_formed_group` statement"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["pk_ipr_has_formed_group = u.get_sql_ready_str(existing_formations['pk_ipr_has_formed_group'])\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_has_formed_group};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.1.b/ Delete `has_dissolved_group` statement"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["pk_ipr_has_dissolved_group = u.get_sql_ready_str(existing_dissolutions['pk_ipr_has_dissolved_group'])\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_has_dissolved_group};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2/ Delete time information"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["time_properties = f\"({pks.properties.timeSpan_atSomeTimeWithin_timePrimitive}, {pks.properties.timespan_beginOfTheBegin_timePrim}, {pks.properties.timespan_endOfTheEnd_timePrim}, {pks.properties.timeSpan_ongoingThroughout_timePrimitive}, {pks.properties.timespan_endOfTheBegin_timePrim}, {pks.properties.timespan_beginOfTheEnd_timePrim})\""]},{"cell_type":"markdown","metadata":{},"source":["#### 1.2.a/ Delete time information for formation"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["formation_time_stmts = db.query(f\"\"\"\n","    select\n","        ipr.pk_entity as pk_ipr_time_info_form\n","    from information.statement s\n","    inner join projects.info_proj_rel ipr on ipr.fk_entity = s.pk_entity and ipr.fk_project = {pk_project} and ipr.is_in_project = true\n","    where s.fk_subject_info in {pk_formations_str} and s.fk_property in {time_properties}\n","\"\"\")\n","pk_ipr_time_info_form = u.get_sql_ready_str(formation_time_stmts['pk_ipr_time_info_form'])\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_time_info_form};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.2.b/ Delete time information for dissolution"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["dissolution_time_stmts = db.query(f\"\"\"\n","    select\n","        ipr.pk_entity as pk_ipr_time_info_diss\n","    from information.statement s\n","    inner join projects.info_proj_rel ipr on ipr.fk_entity = s.pk_entity and ipr.fk_project = {pk_project} and ipr.is_in_project = true\n","    where s.fk_subject_info in {pk_dissolution_str} and s.fk_property in {time_properties}\n","\"\"\")\n","pk_ipr_time_info_diss = u.get_sql_ready_str(dissolution_time_stmts['pk_ipr_time_info_diss'])\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_ipr_time_info_diss};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3/ Delete certainty comment"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.a/ Delete certainty comment for formations"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["formation_comments = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 7953586\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_comment_form = formation_comments['pk_ipr_comment'].tolist() + formation_comments['pk_ipr_stmt_has_comment'].tolist() + formation_comments['pk_ipr_stmt_comment_has_type'].tolist() + formation_comments['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_comment_form = u.get_sql_ready_str(pk_iprs_cert_comment_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_comment_form};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.b/ Delete certainty comment for dissolutions"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["dissolution_comments = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 7953586\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_comment_diss = dissolution_comments['pk_ipr_comment'].tolist() + dissolution_comments['pk_ipr_stmt_has_comment'].tolist() + dissolution_comments['pk_ipr_stmt_comment_has_type'].tolist() + dissolution_comments['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_comment_diss = u.get_sql_ready_str(pk_iprs_cert_comment_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_comment_diss};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.4/ Delete URI informations"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.4.a/ Delete URI informations for formations"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["formation_uri = db.query(f\"\"\"\n","    select\n","        ipr1.pk_entity as pk_ipr_uri,\n","        ipr2.pk_entity as pk_ipr_stmt_same_as_uri,\n","        ipr3.pk_entity as pk_ipr_stmt_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    where r1.fk_class = {pks.classes.uri}\n","\"\"\")\n","\n","pk_iprs_uri_form = formation_uri['pk_ipr_uri'].tolist() + formation_uri['pk_ipr_stmt_same_as_uri'].tolist() + formation_uri['pk_ipr_stmt_has_value'].tolist()\n","pk_iprs_uri_form = u.get_sql_ready_str(pk_iprs_uri_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_uri_form};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.4.b/ Delete URI informations for dissolution"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["dissolution_uri = db.query(f\"\"\"\n","    select\n","        ipr1.pk_entity as pk_ipr_uri,\n","        ipr2.pk_entity as pk_ipr_stmt_same_as_uri,\n","        ipr3.pk_entity as pk_ipr_stmt_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    where r1.fk_class = {pks.classes.uri}\n","\"\"\")\n","\n","pk_iprs_uri_diss = dissolution_uri['pk_ipr_uri'].tolist() + dissolution_uri['pk_ipr_stmt_same_as_uri'].tolist() + dissolution_uri['pk_ipr_stmt_has_value'].tolist()\n","pk_iprs_uri_diss = u.get_sql_ready_str(pk_iprs_uri_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_uri_diss};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5/ Delete date complements"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.5.a/ Delete date complements on formations"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["formation_date_cmplt = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065621\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_date_cmplt_form = formation_date_cmplt['pk_ipr_comment'].tolist() + formation_date_cmplt['pk_ipr_stmt_has_comment'].tolist() + formation_date_cmplt['pk_ipr_stmt_comment_has_type'].tolist() + formation_date_cmplt['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_date_cmplt_form = u.get_sql_ready_str(pk_iprs_cert_date_cmplt_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_date_cmplt_form};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.5.b/ Delete date complements on dissolution"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["dissolution_date_cmplt = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065621\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_date_cmplt_diss = dissolution_date_cmplt['pk_ipr_comment'].tolist() + dissolution_date_cmplt['pk_ipr_stmt_has_comment'].tolist() + dissolution_date_cmplt['pk_ipr_stmt_comment_has_type'].tolist() + dissolution_date_cmplt['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_date_cmplt_diss = u.get_sql_ready_str(pk_iprs_cert_date_cmplt_diss)\n","\n","if len(dissolution_date_cmplt) > 0:\n","    db.execute(f\"\"\"\n","        update projects.info_proj_rel \n","            set is_in_project = false\n","            where pk_entity in {pk_iprs_cert_date_cmplt_diss};\n","    \"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.6/ Delete note on dates"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.6.a/ Delete note on dates on formations"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["formation_note_dates = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_formations_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065632\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_note_dates_form = formation_note_dates['pk_ipr_comment'].tolist() + formation_note_dates['pk_ipr_stmt_has_comment'].tolist() + formation_note_dates['pk_ipr_stmt_comment_has_type'].tolist() + formation_note_dates['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_note_dates_form = u.get_sql_ready_str(pk_iprs_cert_note_dates_form)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_note_dates_form};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.6.b/ Delete note on dates on dissolution"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["dissolution_note_dates = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_comment,\n","        ipr1.pk_entity as pk_ipr_comment,\n","        ipr2.pk_entity as pk_ipr_stmt_has_comment,\n","        ipr3.pk_entity as pk_ipr_stmt_comment_has_type,\n","        ipr4.pk_entity as pk_ipr_stmt_comment_has_value\n","    from information.resource r1\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_object_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_hasComment_text} and s2.fk_subject_info in {pk_dissolution_str}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = r1.pk_entity and s3.fk_property = {pks.properties.comment_hasCommentType_CommentType} and s3.fk_object_info = 8065632\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.statement s4 on s4.fk_subject_info = r1.pk_entity and s4.fk_property = {pks.properties.text_hasValueVersion_string}\n","    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n","    where r1.fk_class = {pks.classes.comment}\n","\"\"\")\n","\n","pk_iprs_cert_note_dates_diss = dissolution_note_dates['pk_ipr_comment'].tolist() + dissolution_note_dates['pk_ipr_stmt_has_comment'].tolist() + dissolution_note_dates['pk_ipr_stmt_comment_has_type'].tolist() + dissolution_note_dates['pk_ipr_stmt_comment_has_value'].tolist()\n","pk_iprs_cert_note_dates_diss = u.get_sql_ready_str(pk_iprs_cert_note_dates_diss)\n","\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_iprs_cert_note_dates_diss};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["### 1.7/ Delete Formation, Dissolutions"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_formations_str};\n","\"\"\")\n","db.execute(f\"\"\"\n","    update projects.info_proj_rel \n","        set is_in_project = false\n","        where pk_entity in {pk_dissolution_str};\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["## 2./ Find and create missing Groups"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1/ Find all existing groups in symogih project in Geovistory"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Connecting to PRODUCTION Database ... Connected!\n"]}],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Find existing group on GV\n","groups = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1 \n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2a on ipr2a.fk_entity = s2.pk_entity and ipr2a.fk_project = {pk_project} and ipr2a.is_in_project = true\n","    inner join projects.info_proj_rel ipr2b on ipr2b.fk_entity = s2.fk_object_info and ipr2b.fk_project = {pk_project} and ipr2b.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","groups = groups[groups['uri'].str.contains('CoAc')]\n","groups['pk_coac'] = [string[string.rfind('/') + 1:] for string in groups['uri']]\n","groups.drop(columns='uri', inplace=True)\n","\n","# a.infos(groups)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2/ Find all Collective actors from BHP (CSV file)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n","coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n","coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n","\n","coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n","coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n","coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n","coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n","\n","coacs['pk_collective_actor'] = 'CoAc' + coacs['pk_collective_actor'].astype(str)\n","coacs.rename(columns={'pk_collective_actor':'pk_coac'}, inplace=True)\n","\n","# a.infos(coacs)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3/ Get record linkage result (CSV file)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["record_linkage = pd.read_csv('../../data/record-linkage-collective-actors-filled.csv')\n","record_linkage = record_linkage[record_linkage['Doublon'] == 'oui']\n","record_linkage = record_linkage[['pk_bhp', 'pk_gv']]\n","record_linkage['pk_bhp'] = 'CoAc' + record_linkage['pk_bhp'].astype(str)\n","record_linkage.rename(columns={'pk_bhp': 'pk_coac', 'pk_gv': 'pk_group'}, inplace=True)\n","\n","# a.infos(record_linkage)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.4/ Find missing ones"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["existing_groups = groups['pk_coac'].tolist()\n","record_linkage_groups = record_linkage['pk_coac'].tolist()\n","missing_groups = coacs[[pk_coac not in existing_groups and pk_coac not in record_linkage_groups for pk_coac in coacs['pk_coac']]].reset_index(drop=True)\n","\n","# All missing ones should be either existing or in the record linkage\n","assert len(missing_groups) == 0"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5/ Add record linkage result to project"]},{"cell_type":"markdown","metadata":{},"source":["The problem was actually that all information about the entities where added, but not the entity itself.. Dumb me.."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating info_proj_rel of 132 entities with project <6857901> ... Done in [00h00m00s]\n"]}],"source":["db.info_proj_rels.create(record_linkage['pk_group'])"]},{"cell_type":"markdown","metadata":{},"source":["## 3./ Add Formations"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1/ Fetch local file about coacs"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n","coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n","coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n","\n","coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n","coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n","coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n","coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n","\n","coacs['pk_collective_actor'] = 'CoAc' + coacs['pk_collective_actor'].astype(str)\n","coacs.rename(columns={'pk_collective_actor':'pk_coac'}, inplace=True)\n","\n","# a.infos(coacs)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2/ Fetch Geovistory existing groups"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Connecting to PRODUCTION Database ... Connected!\n"]}],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["# Find existing group on GV\n","groups = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1 \n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2a on ipr2a.fk_entity = s2.pk_entity and ipr2a.fk_project = {pk_project} and ipr2a.is_in_project = true\n","    inner join projects.info_proj_rel ipr2b on ipr2b.fk_entity = s2.fk_object_info and ipr2b.fk_project = {pk_project} and ipr2b.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","groups = groups[groups['uri'].str.contains('CoAc')]\n","groups['pk_coac'] = [string[string.rfind('/') + 1:] for string in groups['uri']]\n","groups.drop(columns='uri', inplace=True)\n","\n","coacs_str = u.get_sql_ready_str(groups['pk_coac'])\n","\n","# a.infos(groups)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3/ Fetch BHP related formation"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Requests will not be executed\n","[DB] Connecting to PGSQL Database ... Connected!\n"]}],"source":["# Connect to BHP database\n","db_url_env_var_name = 'YELLOW_BHP' # Name of an environment variable holding the Postgres database URL\n","db.connect_external(os.getenv(db_url_env_var_name), execute=False)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["formations_bhp_info = db.query(f\"\"\"\n","    select\n","        ir.fk_associated_object as pk_coac, \n","        i.pk_information, \n","        id.year, id.month, id.day,\n","        id.fk_abob_type_information_date,\n","        id.complement as complement, \n","        id.notes as notes,\n","        id.certainty_date\n","    from bhp.information_role ir\n","    inner join bhp.information i on i.pk_information = ir.fk_information and i.fk_type_information = 30\n","    inner join bhp.information_date id on id.fk_information = i.pk_information\n","    where ir.fk_associated_object in {coacs_str} and ir.fk_type_role = 49\n","\"\"\")\n","\n","formations_bhp_info['year'] = formations_bhp_info['year'].astype(pd.Int64Dtype())\n","formations_bhp_info['month'] = formations_bhp_info['month'].astype(pd.Int64Dtype())\n","formations_bhp_info['day'] = formations_bhp_info['day'].astype(pd.Int64Dtype())\n","formations_bhp_info['fk_abob_type_information_date'] = formations_bhp_info['fk_abob_type_information_date'].astype(pd.Int64Dtype())\n","formations_bhp_info['date_bhp'] = [(row.year, row.month, row.day) for i, row in formations_bhp_info.iterrows()]\n","formations_bhp_info['uri'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in formations_bhp_info['pk_information']]\n","formations_bhp_info.drop(columns=['year', 'month', 'day', 'pk_information'], inplace=True)\n","formations_bhp_info['complement'] = [pd.NA if pd.isna(row['complement']) or row['complement'].strip() == '' else row['complement'] for _,row in formations_bhp_info.iterrows()]\n","formations_bhp_info['notes'] = [pd.NA if pd.isna(row['notes']) or row['notes'].strip() == '' else row['notes'] for _,row in formations_bhp_info.iterrows()]\n","formations_bhp_info['notes'] = [s.replace('<p>', '').replace('</p>', '') if pd.notna(s) else pd.NA for s in formations_bhp_info['notes']]\n","formations_bhp_info['notes'] = [s.replace('<em>', '').replace('</em>', '') if pd.notna(s) else pd.NA for s in formations_bhp_info['notes']]\n","formations_bhp_info['complement'] = [e.replace('<p>', '').replace('</p>', '') if pd.notna(e) else pd.NA for e in formations_bhp_info['complement']]\n","\n","# a.infos(formations_bhp_info)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.4/ Build import table"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.1/ Create table, and link with GV id"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["formations = pd.DataFrame()\n","formations['pk_coac'] = np.unique(coacs['pk_coac'].tolist() + formations_bhp_info['pk_coac'].tolist())\n","formations = formations.merge(groups)\n","\n","formations.sort_values('pk_coac', inplace=True)\n","formations = formations[['pk_group', 'pk_coac']]\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.2/ Add URI"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["# Only concerns the BHP informations\n","\n","formations['uri'] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","for _, row in formations_bhp_info.iterrows():\n","    formations.at[row['pk_coac'], 'uri'] = row['uri']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.3/ Add date informations"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["### Begin of the begin ###\n","note_begin = 4\n","property_name = 'begin_of_the_begin'\n","abob_types = [1125,1321,1322]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']) and pd.isna(formations.at[row['pk_coac'], 'uri']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["### Begin of the end ###\n","property_name = 'begin_of_the_end'\n","abob_types = [1290]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.isna(formations.at[row['pk_coac'], 'uri']):\n","            formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["### End of the begin ###\n","property_name = 'end_of_the_begin'\n","abob_types = [1323]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.isna(formations.at[row['pk_coac'], 'uri']):\n","            formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["### End of the end ###\n","note_begin = 1\n","property_name = 'end_of_the_end'\n","abob_types = [256,1126,1128]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']) and pd.isna(formations.at[row['pk_coac'], 'uri']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["### Ongoing throughout ###\n","note_begin = 3\n","property_name = 'ongoing_throughout'\n","abob_types = [258]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']) and pd.isna(formations.at[row['pk_coac'], 'uri']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["### At some time within ###\n","note_begin = 2\n","property_name = 'at_some_time_within'\n","abob_types = [246]\n","\n","formations[property_name] = pd.NA\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_begin'] == note_begin][['pk_coac', 'begin_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['begin_year_coac']) and pd.isna(formations.at[row['pk_coac'], 'uri']):\n","            formations.at[row['pk_coac'], property_name] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = formations_bhp_info[formations_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        formations.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["# Default cases: At some time within\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for i, row in coacs.iterrows():\n","    if pd.isna(row['notes_begin']) or row['notes_begin'] not in [1,2,3,4]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if pd.notna(row['begin_year_coac']) and pd.isna(formations.at[row['pk_coac'], 'uri']):\n","                formations.at[row['pk_coac'], 'at_some_time_within'] = (row['begin_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infos\n","for i, row in formations_bhp_info.iterrows():\n","    if pd.isna(row['fk_abob_type_information_date']) or row['fk_abob_type_information_date'] not in [246,1125,1126,258,1289,1290,1321,1322,1323,256,1128,1128]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if pd.notna(row['date_bhp']) and pd.notna(row['date_bhp'][0]) and pd.notna(row['date_bhp'][1]) and pd.notna(row['date_bhp'][2]):\n","                formations.at[row['pk_coac'], 'at_some_time_within'] = row['date_bhp']        \n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.4/ Add certainty comment"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["formations['certainty_comment'] = pd.NA\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for _, row in coacs.iterrows():\n","    if pd.notna(row['certainty_begin']) and row['certainty_begin'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_begin']) and row['certainty_begin'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","# From BHP info\n","for _, row in formations_bhp_info.iterrows():\n","    if pd.notna(row['certainty_date']) and row['certainty_date'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_date']) and row['certainty_date'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            formations.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.5/ Add Date complement"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["formations['date_complement'] = pd.NA\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in formations_bhp_info.iterrows():\n","    formations.at[row['pk_coac'], 'date_complement'] = row['complement']\n","    \n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4.6/ Add date note"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["formations['date_note'] = pd.NA\n","\n","formations.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in formations_bhp_info.iterrows():\n","    formations.at[row['pk_coac'], 'date_note'] = '[Note] ' + row['notes']\n","    \n","formations.reset_index(inplace=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.5/ Remove formation that should not be created"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["formations = formations[\n","    pd.notna(formations['begin_of_the_begin']) |\n","    pd.notna(formations['begin_of_the_end']) |\n","    pd.notna(formations['end_of_the_begin']) |\n","    pd.notna(formations['end_of_the_end']) |\n","    pd.notna(formations['ongoing_throughout']) |\n","    pd.notna(formations['at_some_time_within']) |\n","    pd.notna(formations['uri']) |\n","    pd.notna(formations['certainty_comment']) |\n","    pd.notna(formations['date_complement']) |\n","    pd.notna(formations['date_note'])\n","].reset_index(drop=True)\n","\n","# a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.6/ Generate CSV for validation"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape:  (10833, 12) - extract:\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pk_coac</th>\n","      <th>pk_group</th>\n","      <th>uri</th>\n","      <th>begin_of_the_begin</th>\n","      <th>begin_of_the_end</th>\n","      <th>end_of_the_begin</th>\n","      <th>end_of_the_end</th>\n","      <th>ongoing_throughout</th>\n","      <th>at_some_time_within</th>\n","      <th>certainty_comment</th>\n","      <th>date_complement</th>\n","      <th>date_note</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CoAc10000</td>\n","      <td>8366930</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1926, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CoAc10003</td>\n","      <td>8366703</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1907, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CoAc10007</td>\n","      <td>8366858</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1854, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CoAc10008</td>\n","      <td>8366583</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1896, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CoAc10009</td>\n","      <td>8366690</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1944, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     pk_coac  pk_group   uri begin_of_the_begin begin_of_the_end  \\\n","0  CoAc10000   8366930  <NA>               <NA>             <NA>   \n","1  CoAc10003   8366703  <NA>               <NA>             <NA>   \n","2  CoAc10007   8366858  <NA>               <NA>             <NA>   \n","3  CoAc10008   8366583  <NA>               <NA>             <NA>   \n","4  CoAc10009   8366690  <NA>               <NA>             <NA>   \n","\n","  end_of_the_begin end_of_the_end ongoing_throughout at_some_time_within  \\\n","0             <NA>           <NA>               <NA>  (1926, <NA>, <NA>)   \n","1             <NA>           <NA>               <NA>  (1907, <NA>, <NA>)   \n","2             <NA>           <NA>               <NA>  (1854, <NA>, <NA>)   \n","3             <NA>           <NA>               <NA>  (1896, <NA>, <NA>)   \n","4             <NA>           <NA>               <NA>  (1944, <NA>, <NA>)   \n","\n","  certainty_comment date_complement date_note  \n","0              <NA>            <NA>      <NA>  \n","1              <NA>            <NA>      <NA>  \n","2              <NA>            <NA>      <NA>  \n","3              <NA>            <NA>      <NA>  \n","4              <NA>            <NA>      <NA>  "]},"metadata":{},"output_type":"display_data"}],"source":["formations.to_csv('./formation.csv', index=False)\n","\n","a.infos(formations)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.7/ Date validation"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["line: 443 CoAc11679 at_some_time_within (1819, 0, 19) 1819 0 19\n","line: 9806 CoAc228 at_some_time_within (1827, 99, <NA>) 1827 99 <NA>\n","Year range: 0 - 2019\n"]}],"source":["dates_prop = ['begin_of_the_begin', 'begin_of_the_begin', 'end_of_the_begin', 'end_of_the_end', 'ongoing_throughout', 'at_some_time_within']\n","min_year = float('inf')\n","max_year = float('-inf')\n","for i, row in formations.iterrows():\n","    for prop in dates_prop:\n","        if pd.isna(row[prop]): continue\n","\n","        year = row[prop][0]\n","        month = row[prop][1]\n","        day = row[prop][2]\n","        \n","        is_pb = False\n","\n","        # Year checkings\n","        if pd.notna(year):\n","            if year < min_year: min_year = year\n","            if year > max_year: max_year = year\n","\n","        # Month checkings\n","        if pd.notna(month):\n","            if month != round(month): is_pb = True\n","            if month < 1 or month > 12: is_pb = True\n","\n","        # Day checkings\n","        if pd.notna(day):    \n","            if day != round(day): is_pb = True\n","            if day < 1 or day > 31: is_pb = True\n","\n","        if is_pb:\n","            print('line:', i, row['pk_coac'], prop, row[prop], year, month, day)\n","\n","print('Year range:', min_year, '-', max_year)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["# Manual editing\n","formations.at[439, 'at_some_time_within'] = (1819, pd.NA, pd.NA)\n","formations.at[9806, 'at_some_time_within'] = (1827, pd.NA, pd.NA)   "]},{"cell_type":"markdown","metadata":{},"source":["## 4./ Add Dissolutions"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1/ Fetch local file about coacs"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n","coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n","coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n","\n","coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n","coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n","coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n","coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n","\n","coacs['pk_collective_actor'] = 'CoAc' + coacs['pk_collective_actor'].astype(str)\n","coacs.rename(columns={'pk_collective_actor':'pk_coac'}, inplace=True)\n","\n","# a.infos(coacs)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2/ Fetch Geovistory existing groups"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Connecting to PRODUCTION Database ... Connected!\n"]}],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["# Find existing group on GV\n","groups = db.query(f\"\"\"\n","    select\n","        r1.pk_entity as pk_group,\n","        a4.string as uri\n","    from information.resource r1 \n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = r1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = r1.pk_entity and s2.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr2a on ipr2a.fk_entity = s2.pk_entity and ipr2a.fk_project = {pk_project} and ipr2a.is_in_project = true\n","    inner join projects.info_proj_rel ipr2b on ipr2b.fk_entity = s2.fk_object_info and ipr2b.fk_project = {pk_project} and ipr2b.is_in_project = true\n","    inner join information.statement s3 on s3.fk_subject_info = s2.fk_object_info and s3.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = s3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n","    inner join information.appellation a4 on a4.pk_entity = s3.fk_object_info\n","    where r1.fk_class = {pks.classes.group}\n","\"\"\")\n","groups = groups[groups['uri'].str.contains('CoAc')]\n","groups['pk_coac'] = [string[string.rfind('/') + 1:] for string in groups['uri']]\n","groups.drop(columns='uri', inplace=True)\n","\n","coacs_str = u.get_sql_ready_str(groups['pk_coac'])\n","\n","# a.infos(groups)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4.3/ Fetch BHP related formation"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Requests will not be executed\n","[DB] Connecting to PGSQL Database ... Connected!\n"]}],"source":["# Connect to BHP database\n","db_url_env_var_name = 'YELLOW_BHP' # Name of an environment variable holding the Postgres database URL\n","db.connect_external(os.getenv(db_url_env_var_name), execute=False)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape:  (51, 7) - extract:\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pk_coac</th>\n","      <th>fk_abob_type_information_date</th>\n","      <th>complement</th>\n","      <th>notes</th>\n","      <th>certainty_date</th>\n","      <th>date_bhp</th>\n","      <th>uri</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CoAc225</td>\n","      <td>246</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>1</td>\n","      <td>(1847, 4, 30)</td>\n","      <td>http://symogih.org/resource/Info41791</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CoAc15408</td>\n","      <td>246</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;p&gt;Non documenté après 1935&lt;/p&gt;</td>\n","      <td>3</td>\n","      <td>(1935, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>http://symogih.org/resource/Info112695</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CoAc9137</td>\n","      <td>246</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>1</td>\n","      <td>(1934, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>http://symogih.org/resource/Info45645</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CoAc227</td>\n","      <td>246</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>1</td>\n","      <td>(1882, 1, &lt;NA&gt;)</td>\n","      <td>http://symogih.org/resource/Info41902</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CoAc231</td>\n","      <td>246</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>1</td>\n","      <td>(1852, 6, 30)</td>\n","      <td>http://symogih.org/resource/Info42155</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     pk_coac  fk_abob_type_information_date complement  \\\n","0    CoAc225                            246       <NA>   \n","1  CoAc15408                            246       <NA>   \n","2   CoAc9137                            246       <NA>   \n","3    CoAc227                            246       <NA>   \n","4    CoAc231                            246       <NA>   \n","\n","                             notes  certainty_date            date_bhp  \\\n","0                             <NA>               1       (1847, 4, 30)   \n","1  <p>Non documenté après 1935</p>               3  (1935, <NA>, <NA>)   \n","2                             <NA>               1  (1934, <NA>, <NA>)   \n","3                             <NA>               1     (1882, 1, <NA>)   \n","4                             <NA>               1       (1852, 6, 30)   \n","\n","                                      uri  \n","0   http://symogih.org/resource/Info41791  \n","1  http://symogih.org/resource/Info112695  \n","2   http://symogih.org/resource/Info45645  \n","3   http://symogih.org/resource/Info41902  \n","4   http://symogih.org/resource/Info42155  "]},"metadata":{},"output_type":"display_data"}],"source":["dissolutions_bhp_info = db.query(f\"\"\"\n","    select\n","        ir.fk_associated_object as pk_coac, \n","        i.pk_information, \n","        id.year, id.month, id.day,\n","        id.fk_abob_type_information_date,\n","        id.complement as complement, \n","        id.notes as notes,\n","        id.certainty_date\n","    from bhp.information_role ir\n","    inner join bhp.information i on i.pk_information = ir.fk_information and i.fk_type_information = 33\n","    inner join bhp.information_date id on id.fk_information = i.pk_information\n","    where ir.fk_associated_object in {coacs_str} and ir.fk_type_role = 54\n","\"\"\")\n","\n","dissolutions_bhp_info['year'] = dissolutions_bhp_info['year'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['month'] = dissolutions_bhp_info['month'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['day'] = dissolutions_bhp_info['day'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['fk_abob_type_information_date'] = dissolutions_bhp_info['fk_abob_type_information_date'].astype(pd.Int64Dtype())\n","dissolutions_bhp_info['date_bhp'] = [(row.year, row.month, row.day) for i, row in dissolutions_bhp_info.iterrows()]\n","dissolutions_bhp_info['uri'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in dissolutions_bhp_info['pk_information']]\n","dissolutions_bhp_info.drop(columns=['year', 'month', 'day', 'pk_information'], inplace=True)\n","dissolutions_bhp_info['complement'] = [pd.NA if pd.isna(row['complement']) or row['complement'].strip() == '' else row['complement'] for _,row in dissolutions_bhp_info.iterrows()]\n","dissolutions_bhp_info['notes'] = [pd.NA if pd.isna(row['notes']) or row['notes'].strip() == '' else row['notes'] for _,row in dissolutions_bhp_info.iterrows()]\n","dissolutions_bhp_info['complement'] = [e.replace('<p>', '').replace('</p>', '') if pd.notna(e) else pd.NA for e in dissolutions_bhp_info['complement']]\n","\n","a.infos(dissolutions_bhp_info)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.4/ Build import table"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.1/ Create table, and link with GV id"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["dissolutions = pd.DataFrame()\n","dissolutions['pk_coac'] = np.unique(coacs['pk_coac'].tolist() + dissolutions_bhp_info['pk_coac'].tolist())\n","dissolutions = dissolutions.merge(groups)\n","\n","dissolutions.sort_values('pk_coac', inplace=True)\n","dissolutions = dissolutions[['pk_group', 'pk_coac']]\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.2/ Add URI"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["# Only concerns the BHP informations\n","\n","dissolutions['uri'] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","for _, row in dissolutions_bhp_info.iterrows():\n","    dissolutions.at[row['pk_coac'], 'uri'] = row['uri']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.3/ Add date informations"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["### Begin of the begin ###\n","note_end = 4\n","property_name = 'begin_of_the_begin'\n","abob_types = [1125,1321,1322]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_end'] == note_end][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']) and pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["### Begin of the end ###\n","property_name = 'begin_of_the_end'\n","abob_types = [1290]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","            dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["### End of the begin ###\n","property_name = 'end_of_the_begin'\n","abob_types = [1323]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","            dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["### End of the end ###\n","note_end = 1\n","property_name = 'end_of_the_end'\n","abob_types = [256,1126,1128]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_end'] == note_end][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']) and pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["### Ongoing throughout ###\n","note_end = 3\n","property_name = 'ongoing_throughout'\n","abob_types = [258]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_end'] == note_end][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']) and pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["### At some time within ###\n","note_end = 2\n","property_name = 'at_some_time_within'\n","abob_types = [246]\n","\n","dissolutions[property_name] = pd.NA\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","selection = coacs[coacs['notes_end'] == note_end][['pk_coac', 'end_year_coac']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        if pd.notna(row['end_year_coac']) and pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","            dissolutions.at[row['pk_coac'], property_name] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infomation (has prio over CSV)\n","selection = dissolutions_bhp_info[dissolutions_bhp_info['fk_abob_type_information_date'].isin(abob_types)][['pk_coac', 'date_bhp']]\n","for _, row in selection.iterrows():\n","    if row['pk_coac'] in groups['pk_coac'].tolist():\n","        dissolutions.at[row['pk_coac'], property_name] = row['date_bhp']\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["# Default cases: At some time within\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for i, row in coacs.iterrows():\n","    if pd.isna(row['notes_end']) or row['notes_end'] not in [1,2,3,4]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if pd.notna(row['end_year_coac']) and pd.isna(dissolutions.at[row['pk_coac'], 'uri']):\n","                dissolutions.at[row['pk_coac'], 'at_some_time_within'] = (row['end_year_coac'], pd.NA, pd.NA)\n","\n","# From BHP infos\n","for i, row in dissolutions_bhp_info.iterrows():\n","    if row['fk_abob_type_information_date'] not in [246,1125,1126,258,1289,1290,1321,1322,1323,256,1128,1128]:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            if row['date_bhp'] != (pd.NA, pd.NA, pd.NA):\n","                dissolutions.at[row['pk_coac'], 'at_some_time_within'] = row['date_bhp']        \n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.4/ Add certainty comment"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["dissolutions['certainty_comment'] = pd.NA\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From CSV file\n","for _, row in coacs.iterrows():\n","    if pd.notna(row['certainty_end']) and row['certainty_end'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_end']) and row['certainty_end'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","# From BHP info\n","for _, row in dissolutions_bhp_info.iterrows():\n","    if pd.notna(row['certainty_date']) and row['certainty_date'] == 2:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date reconstituée\"\n","    elif pd.notna(row['certainty_date']) and row['certainty_date'] == 3:\n","        if row['pk_coac'] in groups['pk_coac'].tolist():\n","            dissolutions.at[row['pk_coac'], 'certainty_comment'] = \"Date postulée\"\n","\n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.5/ Add Date complement"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["dissolutions['date_complement'] = pd.NA\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in dissolutions_bhp_info.iterrows():\n","    dissolutions.at[row['pk_coac'], 'date_complement'] = row['complement']\n","    \n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.6/ Add date note"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["dissolutions['date_note'] = pd.NA\n","\n","dissolutions.set_index('pk_coac', inplace=True)\n","\n","# From BHP info\n","for _, row in dissolutions_bhp_info.iterrows():\n","    dissolutions.at[row['pk_coac'], 'date_note'] = '[Note] ' + row['notes']\n","    \n","dissolutions.reset_index(inplace=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.5/ Remove dissolutions that should not be created"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["dissolutions = dissolutions[\n","    pd.notna(dissolutions['begin_of_the_begin']) |\n","    pd.notna(dissolutions['begin_of_the_end']) |\n","    pd.notna(dissolutions['end_of_the_begin']) |\n","    pd.notna(dissolutions['end_of_the_end']) |\n","    pd.notna(dissolutions['ongoing_throughout']) |\n","    pd.notna(dissolutions['at_some_time_within']) |\n","    pd.notna(dissolutions['uri']) |\n","    pd.notna(dissolutions['certainty_comment']) |\n","    pd.notna(dissolutions['date_complement']) |\n","    pd.notna(dissolutions['date_note'])\n","].reset_index(drop=True)\n","\n","# a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.6/ Generate CSV for validation"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape:  (6850, 12) - extract:\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pk_coac</th>\n","      <th>pk_group</th>\n","      <th>uri</th>\n","      <th>begin_of_the_begin</th>\n","      <th>begin_of_the_end</th>\n","      <th>end_of_the_begin</th>\n","      <th>end_of_the_end</th>\n","      <th>ongoing_throughout</th>\n","      <th>at_some_time_within</th>\n","      <th>certainty_comment</th>\n","      <th>date_complement</th>\n","      <th>date_note</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CoAc10035</td>\n","      <td>8366787</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1968, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CoAc10041</td>\n","      <td>8366582</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1968, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CoAc10122</td>\n","      <td>8367031</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1976, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CoAc10126</td>\n","      <td>8376521</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1954, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CoAc10132</td>\n","      <td>8366972</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>(1987, &lt;NA&gt;, &lt;NA&gt;)</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","      <td>&lt;NA&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     pk_coac  pk_group   uri begin_of_the_begin begin_of_the_end  \\\n","0  CoAc10035   8366787  <NA>               <NA>             <NA>   \n","1  CoAc10041   8366582  <NA>               <NA>             <NA>   \n","2  CoAc10122   8367031  <NA>               <NA>             <NA>   \n","3  CoAc10126   8376521  <NA>               <NA>             <NA>   \n","4  CoAc10132   8366972  <NA>               <NA>             <NA>   \n","\n","  end_of_the_begin end_of_the_end ongoing_throughout at_some_time_within  \\\n","0             <NA>           <NA>               <NA>  (1968, <NA>, <NA>)   \n","1             <NA>           <NA>               <NA>  (1968, <NA>, <NA>)   \n","2             <NA>           <NA>               <NA>  (1976, <NA>, <NA>)   \n","3             <NA>           <NA>               <NA>  (1954, <NA>, <NA>)   \n","4             <NA>           <NA>               <NA>  (1987, <NA>, <NA>)   \n","\n","  certainty_comment date_complement date_note  \n","0              <NA>            <NA>      <NA>  \n","1              <NA>            <NA>      <NA>  \n","2              <NA>            <NA>      <NA>  \n","3              <NA>            <NA>      <NA>  \n","4              <NA>            <NA>      <NA>  "]},"metadata":{},"output_type":"display_data"}],"source":["dissolutions.to_csv('./dissolutions.csv', index=False)\n","\n","a.infos(dissolutions)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.7/ Date validation"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Year range: 700 - 2016\n"]}],"source":["dates_prop = ['begin_of_the_begin', 'begin_of_the_begin', 'end_of_the_begin', 'end_of_the_end', 'ongoing_throughout', 'at_some_time_within']\n","min_year = float('inf')\n","max_year = float('-inf')\n","for i, row in dissolutions.iterrows():\n","    for prop in dates_prop:\n","        if pd.isna(row[prop]): continue\n","\n","        year = row[prop][0]\n","        month = row[prop][1]\n","        day = row[prop][2]\n","        \n","        is_pb = False\n","\n","        # Year checkings\n","        if pd.notna(year):\n","            if year < min_year: min_year = year\n","            if year > max_year: max_year = year\n","\n","        # Month checkings\n","        if pd.notna(month):\n","            if month != round(month): is_pb = True\n","            if month < 1 or month > 12: is_pb = True\n","\n","        # Day checkings\n","        if pd.notna(day):    \n","            if day != round(day): is_pb = True\n","            if day < 1 or day > 31: is_pb = True\n","\n","        if is_pb:\n","            print(row['pk_coac'], prop, row[prop], year, month, day)\n","\n","print('Year range:', min_year, '-', max_year)"]},{"cell_type":"markdown","metadata":{},"source":["## 5/ Import new data"]},{"cell_type":"markdown","metadata":{},"source":["If both CSVs has been validated, import"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[DB] Connecting to PRODUCTION Database ... Connected!\n"]}],"source":["# Connect to Geovistory database\n","db.connect_geovistory(env, pk_project, execute)\n","db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","db.set_insert_manner(import_manner)"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["def get_duration(date):\n","    if pd.notna(date[0]) and pd.isna(date[1]) and pd.isna(date[2]): return '1 year'\n","    if pd.notna(date[0]) and pd.notna(date[1]) and pd.isna(date[2]): return '1 month'\n","    if pd.notna(date[0]) and pd.notna(date[1]) and pd.notna(date[2]): return '1 day'\n","    return pd.NA"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1/ Create formations / dissolutions"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 10833 resources of class [60] ... Done in [00h00m03s]\n","Creating info_proj_rel of 10833 entities with project <6857901> ... Done in [00h00m05s]\n","Creating 6850 resources of class [62] ... Done in [00h00m02s]\n","Creating info_proj_rel of 6850 entities with project <6857901> ... Done in [00h00m03s]\n"]}],"source":["formations[\"pk_formation\"] = db.resources.create(pks.classes.formation, len(formations))\n","dissolutions[\"pk_dissolution\"] = db.resources.create(pks.classes.dissolution, len(dissolutions))"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2/ Link formations / dissolutions to their group"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 10833 statements ... Updating metadata ... Done in [00h00m11s]\n","Creating info_proj_rel of 10833 entities with project <6857901> ... Done in [00h00m05s]\n","Creating 6850 statements ... Updating metadata ... Done in [00h00m06s]\n","Creating info_proj_rel of 6850 entities with project <6857901> ... Done in [00h00m03s]\n"]}],"source":["db.statements.create(formations['pk_formation'], pks.properties.formation_hasFormed_group, formations['pk_group'])\n","db.statements.create(dissolutions['pk_dissolution'], pks.properties.dissolution_dissolved_group, dissolutions['pk_group'])"]},{"cell_type":"markdown","metadata":{},"source":["### 5.3/ Begin of the begin"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 491 time primitives ... Done in [00h00m00s]\n","Creating 491 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 491 entities with project <6857901> ... Done in [00h00m01s]\n","Creating 3622 time primitives ... Done in [00h00m00s]\n","Creating 3622 statements ... Updating metadata ... Done in [00h00m04s]\n","Creating info_proj_rel of 3622 entities with project <6857901> ... Done in [00h00m02s]\n"]}],"source":["selection = formations[pd.notna(formations['begin_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_begin'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_beginOfTheBegin_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['begin_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_begin'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_beginOfTheBegin_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.4/ Begin of the end"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['begin_of_the_end'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_end']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_end'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_beginOfTheEnd_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['begin_of_the_end'])]\n","durations = [get_duration(date) for date in selection['begin_of_the_end']]\n","time_prims = db.time_primitives.create(selection['begin_of_the_end'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_beginOfTheEnd_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.5/ End of the begin"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":["selection = formations[pd.notna(formations['end_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['end_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['end_of_the_begin'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_endOfTheBegin_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['end_of_the_begin'])]\n","durations = [get_duration(date) for date in selection['end_of_the_begin']]\n","time_prims = db.time_primitives.create(selection['end_of_the_begin'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_endOfTheBegin_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.6/ End of the end"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 424 time primitives ... Done in [00h00m00s]\n","Creating 424 statements ... Updating metadata ... Done in [00h00m01s]\n","Creating info_proj_rel of 424 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 122 time primitives ... Done in [00h00m00s]\n","Creating 122 statements ... Updating metadata ... Done in [00h00m01s]\n","Creating info_proj_rel of 122 entities with project <6857901> ... Done in [00h00m00s]\n"]}],"source":["selection = formations[pd.notna(formations['end_of_the_end'])]\n","durations = [get_duration(date) for date in selection['end_of_the_end']]\n","time_prims = db.time_primitives.create(selection['end_of_the_end'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timespan_endOfTheEnd_timePrim, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['end_of_the_end'])]\n","durations = [get_duration(date) for date in selection['end_of_the_end']]\n","time_prims = db.time_primitives.create(selection['end_of_the_end'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timespan_endOfTheEnd_timePrim, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.7/ Ongoing throughout"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 288 time primitives ... Done in [00h00m00s]\n","Creating 288 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 288 entities with project <6857901> ... Done in [00h00m01s]\n","Creating 115 time primitives ... Done in [00h00m00s]\n","Creating 115 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 115 entities with project <6857901> ... Done in [00h00m00s]\n"]}],"source":["selection = formations[pd.notna(formations['ongoing_throughout'])]\n","durations = [get_duration(date) for date in selection['ongoing_throughout']]\n","time_prims = db.time_primitives.create(selection['ongoing_throughout'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timeSpan_ongoingThroughout_timePrimitive, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['ongoing_throughout'])]\n","durations = [get_duration(date) for date in selection['ongoing_throughout']]\n","time_prims = db.time_primitives.create(selection['ongoing_throughout'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timeSpan_ongoingThroughout_timePrimitive, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.8/ At some time within"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 9765 time primitives ... Done in [00h00m01s]\n","Creating 9765 statements ... Updating metadata ... Done in [00h00m10s]\n","Creating info_proj_rel of 9765 entities with project <6857901> ... Done in [00h00m04s]\n","Creating 2988 time primitives ... Done in [00h00m00s]\n","Creating 2988 statements ... Updating metadata ... Done in [00h00m04s]\n","Creating info_proj_rel of 2988 entities with project <6857901> ... Done in [00h00m01s]\n"]}],"source":["selection = formations[pd.notna(formations['at_some_time_within'])]\n","durations = [get_duration(date) for date in selection['at_some_time_within']]\n","time_prims = db.time_primitives.create(selection['at_some_time_within'], durations)\n","db.statements.create(selection['pk_formation'], pks.properties.timeSpan_atSomeTimeWithin_timePrimitive, time_prims)\n","\n","selection = dissolutions[pd.notna(dissolutions['at_some_time_within'])]\n","durations = [get_duration(date) for date in selection['at_some_time_within']]\n","time_prims = db.time_primitives.create(selection['at_some_time_within'], durations)\n","db.statements.create(selection['pk_dissolution'], pks.properties.timeSpan_atSomeTimeWithin_timePrimitive, time_prims)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.9/ URI"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 6855 resources of class [967] ... Done in [00h00m02s]\n","Creating info_proj_rel of 6855 entities with project <6857901> ... Done in [00h00m03s]\n","Creating 6855 appellations ... Done in [00h00m07s]\n","Creating 6855 statements ... Updating metadata ... Done in [00h00m07s]\n","Creating info_proj_rel of 6855 entities with project <6857901> ... Done in [00h00m04s]\n","Creating 6855 statements ... Updating metadata ... Done in [00h00m08s]\n","Creating info_proj_rel of 6855 entities with project <6857901> ... Done in [00h00m03s]\n","Creating 48 resources of class [967] ... Done in [00h00m01s]\n","Creating info_proj_rel of 48 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 48 appellations ... Done in [00h00m00s]\n","Creating 48 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 48 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 48 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 48 entities with project <6857901> ... Done in [00h00m01s]\n"]}],"source":["selection = formations[pd.notna(formations['uri'])]\n","db.shortcuts.add_uris(selection['pk_formation'], selection['uri'])\n","\n","selection = dissolutions[pd.notna(dissolutions['uri'])]\n","db.shortcuts.add_uris(selection['pk_dissolution'], selection['uri'])"]},{"cell_type":"markdown","metadata":{},"source":["### 5.10/ Certainty comment"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 4070 resources of class [900] ... Done in [00h00m01s]\n","Creating info_proj_rel of 4070 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4070 appellations ... Done in [00h00m01s]\n","Creating 4070 statements ... Updating metadata ... Done in [00h00m05s]\n","Creating info_proj_rel of 4070 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4070 statements ... Updating metadata ... Done in [00h00m05s]\n","Creating info_proj_rel of 4070 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4070 statements ... Updating metadata ... Done in [00h00m04s]\n","Creating info_proj_rel of 4070 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4073 resources of class [900] ... Done in [00h00m01s]\n","Creating info_proj_rel of 4073 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4073 appellations ... Done in [00h00m02s]\n","Creating 4073 statements ... Updating metadata ... Done in [00h00m04s]\n","Creating info_proj_rel of 4073 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4073 statements ... Updating metadata ... Done in [00h00m04s]\n","Creating info_proj_rel of 4073 entities with project <6857901> ... Done in [00h00m02s]\n","Creating 4073 statements ... Updating metadata ... Done in [00h00m05s]\n","Creating info_proj_rel of 4073 entities with project <6857901> ... Done in [00h00m02s]\n"]}],"source":["selection = formations[pd.notna(formations['certainty_comment'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['certainty_comment'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 7953586)\n","db.statements.create(selection['pk_formation'], pks.properties.entity_hasComment_text, pk_comments)\n","\n","selection = dissolutions[pd.notna(dissolutions['certainty_comment'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['certainty_comment'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 7953586)\n","db.statements.create(selection['pk_dissolution'], pks.properties.entity_hasComment_text, pk_comments)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.11/ Date_complement"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 62 resources of class [900] ... Done in [00h00m00s]\n","Creating info_proj_rel of 62 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 62 appellations ... Done in [00h00m00s]\n","Creating 62 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 62 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 62 statements ... Updating metadata ... Done in [00h00m01s]\n","Creating info_proj_rel of 62 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 62 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 62 entities with project <6857901> ... Done in [00h00m00s]\n"]}],"source":["selection = formations[pd.notna(formations['date_complement'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_complement'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065621)\n","db.statements.create(selection['pk_formation'], pks.properties.entity_hasComment_text, pk_comments)\n","\n","selection = dissolutions[pd.notna(dissolutions['date_complement'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_complement'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065621)\n","db.statements.create(selection['pk_dissolution'], pks.properties.entity_hasComment_text, pk_comments)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.11/ Date note"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating 43 resources of class [900] ... Done in [00h00m00s]\n","Creating info_proj_rel of 43 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 43 appellations ... Done in [00h00m00s]\n","Creating 43 statements ... Updating metadata ... Done in [00h00m01s]\n","Creating info_proj_rel of 43 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 43 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 43 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 43 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 43 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 1 resources of class [900] ... Done in [00h00m00s]\n","Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m01s]\n","Creating 1 appellations ... Done in [00h00m00s]\n","Creating 1 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 1 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m00s]\n","Creating 1 statements ... Updating metadata ... Done in [00h00m00s]\n","Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m00s]\n"]}],"source":["selection = formations[pd.notna(formations['date_note'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_note'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065632)\n","db.statements.create(selection['pk_formation'], pks.properties.entity_hasComment_text, pk_comments)\n","\n","selection = dissolutions[pd.notna(dissolutions['date_note'])]\n","pk_comments = db.resources.create(pks.classes.comment, len(selection))\n","pk_appes = db.appellations.create(selection['date_note'])\n","db.statements.create(pk_comments, pks.properties.appe_hasValue_string, pk_appes)\n","db.statements.create(pk_comments, pks.properties.comment_hasCommentType_CommentType, 8065632)\n","db.statements.create(selection['pk_dissolution'], pks.properties.entity_hasComment_text, pk_comments)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
