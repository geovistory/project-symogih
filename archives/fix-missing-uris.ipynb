{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = 'prod'\n",
    "pk_project = 6857901\n",
    "execute = True\n",
    "metadata_str = 'fix-missing-uri'\n",
    "import_manner = 'one-shot'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "\n",
    "import geovpylib.analysis as a\n",
    "import geovpylib.database as db\n",
    "import geovpylib.graphs as graphs\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.recordlinkage as rl\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.utils as u\n",
    "\n",
    "eta = u.Eta()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix missing URIs in some temporal entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch all URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PGSQL Database ... Connected!\n",
      "Information nb: 28195\n",
      "[DB] Database correctly disconnected.\n"
     ]
    }
   ],
   "source": [
    "db.connect_external(os.environ.get('YELLOW_BHP'))\n",
    "\n",
    "births_deaths_bhp = db.query(f\"\"\"\n",
    "    select \n",
    "        ir.fk_associated_object as pk_bhp,\n",
    "        ir.fk_information as fk_info,\n",
    "        ir.fk_type_role\n",
    "    from bhp.information_role ir\n",
    "    inner join bhp.information_date id on ir.fk_information = id.fk_information\n",
    "    where ir.fk_type_role = 40 or ir.fk_type_role = 45\n",
    "\"\"\")\n",
    "\n",
    "births_deaths_bhp = births_deaths_bhp[pd.notna(births_deaths_bhp['pk_bhp'])]  \n",
    "births_deaths_bhp = births_deaths_bhp[births_deaths_bhp['pk_bhp'].str.contains('Actr')]\n",
    "births_deaths_bhp['pk_bhp'] = births_deaths_bhp['pk_bhp'].str.replace('Actr', '', regex=False)\n",
    "births_deaths_bhp['pk_bhp'] = births_deaths_bhp['pk_bhp'].astype(pd.Int64Dtype())\n",
    "births_deaths_bhp['uri_evt'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in births_deaths_bhp['fk_info']]\n",
    "births_deaths_bhp['teen'] = ['birth' if role == 40 else 'death' for role in births_deaths_bhp.fk_type_role]\n",
    "births_deaths_bhp = births_deaths_bhp[['pk_bhp', 'teen', 'uri_evt']]\n",
    "\n",
    "print('Information nb:', len(births_deaths_bhp))\n",
    "\n",
    "db.disconnect()\n",
    "\n",
    "# 1s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Geovistory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PRODUCTION Database ... Connected!\n",
      "[DB] Database correctly disconnected.\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory(env, pk_project, execute, skip_protection=True)\n",
    "\n",
    "persons = db.query(f\"\"\"\n",
    "    select distinct\n",
    "        r.pk_entity as pk_gv,\n",
    "        a3.string as uri,\n",
    "        s4.fk_subject_info as pk_birth,\n",
    "        a4.string as birth_uri,\n",
    "        s5.fk_subject_info as pk_death,\n",
    "        a5.string as death_uri\n",
    "    from information.resource r\n",
    "    inner join projects.info_proj_rel ipr on ipr.fk_entity = r.pk_entity and ipr.fk_project = {pk_project} and ipr.is_in_project = true\n",
    "    -- URI\n",
    "    inner join information.statement s1 on s1.fk_subject_info = r.pk_entity and s1.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = s1.pk_entity and ipr1.fk_project = {pk_project} and ipr1.is_in_project = true\n",
    "    inner join information.statement s2 on s2.fk_subject_info = s1.fk_object_info and s2.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.fk_project = {pk_project} and ipr2.is_in_project = true\n",
    "    inner join information.appellation a3 on a3.pk_entity = s2.fk_object_info\n",
    "    inner join projects.info_proj_rel ipr3 on ipr3.fk_entity = a3.pk_entity and ipr3.fk_project = {pk_project} and ipr3.is_in_project = true\n",
    "    -- Birth\n",
    "    left join information.statement s4 on s4.fk_object_info = r.pk_entity and s4.fk_property = {pks.properties.birth_broughtIntoLife_person}\n",
    "    inner join projects.info_proj_rel ipr4 on ipr4.fk_entity = s4.pk_entity and ipr4.fk_project = {pk_project} and ipr4.is_in_project = true\n",
    "    left join information.statement s4b on s4b.fk_subject_info = s4.fk_subject_info and s4b.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    left join information.statement s4c on s4c.fk_subject_info = s4b.fk_object_info and s4c.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    left join information.appellation a4 on a4.pk_entity = s4c.fk_object_info\n",
    "    -- Death\n",
    "    left join information.statement s5 on s5.fk_object_info = r.pk_entity and s5.fk_property = {pks.properties.death_wasDeathOf_person}\n",
    "    inner join projects.info_proj_rel ipr5 on ipr5.fk_entity = s5.pk_entity and ipr5.fk_project = {pk_project} and ipr5.is_in_project = true\n",
    "    left join information.statement s5b on s5b.fk_subject_info = s5.fk_subject_info and s5b.fk_property = {pks.properties.entity_sameAsURI_URI}\n",
    "    left join information.statement s5c on s5c.fk_subject_info = s5b.fk_object_info and s5c.fk_property = {pks.properties.appe_hasValue_string}\n",
    "    left join information.appellation a5 on a5.pk_entity = s5c.fk_object_info\n",
    "\n",
    "    where r.fk_class = {pks.classes.person}\n",
    "\"\"\")\n",
    "persons = persons[persons.uri.str.contains('symogih.org')]\n",
    "persons['pk_bhp'] = persons.uri.str.replace('http://symogih.org/resource/Actr', '', regex=False).astype(int)\n",
    "persons.drop(columns=['uri'], inplace=True)\n",
    "\n",
    "persons.sort_values('pk_bhp', inplace=True)\n",
    "persons.drop_duplicates(inplace=True)\n",
    "persons.reset_index(inplace=True, drop=True)\n",
    "# persons = persons[['pk_bhp', 'pk_gv', 'pk_birth',  'pk_death']].drop_duplicates()\n",
    "\n",
    "db.disconnect()\n",
    "\n",
    "# 12s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get wrong data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = births_deaths_bhp.merge(persons)\n",
    "table['pk_teen'] = [row['pk_birth'] if row['teen'] == 'birth' else row['pk_death'] for _,row in table.iterrows()]\n",
    "table['teen_uri'] = [row['birth_uri'] if row['teen'] == 'birth' else row['death_uri'] for _,row in table.iterrows()]\n",
    "# table = table[['pk_bhp', 'pk_teen', 'teen_uri', 'uri_evt']]\n",
    "table.rename(columns={'uri_evt': 'uri_should', 'teen_uri': 'uri_has'}, inplace=True)\n",
    "table = table[table['uri_has'] != table['uri_should']].drop_duplicates()\n",
    "# table = table[table['teen_uri'] != table['uri_evt']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbs = []\n",
    "\n",
    "for pk in table.pk_bhp.unique():\n",
    "    \n",
    "    # birth\n",
    "    selection = table[(table['teen'] == 'birth') & (table['pk_bhp'] == pk)]\n",
    "    uri_should = np.unique(selection['uri_should'].tolist())\n",
    "    uri_has = np.unique(selection['uri_has'].dropna().tolist())\n",
    "    if(len(uri_should) == 0): pass\n",
    "    elif (len(uri_has) == 0): pbs.append({'pk_bhp':pk, 'teen':'birth'})\n",
    "    else: \n",
    "        for uri in uri_should: \n",
    "            if uri not in uri_has: pbs.append({'pk_bhp':pk, 'teen':'birth'})\n",
    "\n",
    "    # death\n",
    "    selection = table[(table['teen'] == 'death') & (table['pk_bhp'] == pk)]\n",
    "    uri_should = np.unique(selection['uri_should'].tolist())\n",
    "    uri_has = np.unique(selection['uri_has'].dropna().tolist())\n",
    "    if(len(uri_should) == 0): pass\n",
    "    elif (len(uri_has) == 0): pbs.append({'pk_bhp':pk, 'teen':'death'})\n",
    "    else: \n",
    "        for uri in uri_should: \n",
    "            if uri not in uri_has: pbs.append({'pk_bhp':pk, 'teen':'death'})\n",
    "\n",
    "indexes = []\n",
    "for pb in pbs: indexes += table[(table['pk_bhp'] == pb['pk_bhp']) & (table['teen'] == pb['teen'])].index.tolist()\n",
    "\n",
    "pbs = table.loc[indexes]\n",
    "pbs = pbs[pd.isna(pbs['uri_has'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PRODUCTION Database ... Connected!\n",
      "Creating 330 resources of class [967] ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 330 entities with project <6857901> ... Done in [00h00'01]\n",
      "Creating 330 appellations ... Done in [00h00'00]\n",
      "Creating info_proj_rel of 330 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 330 statements ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 330 entities with project <6857901> ... Done in [00h00'00]\n",
      "Creating 330 statements ... Done in [00h00'01]\n",
      "Creating info_proj_rel of 330 entities with project <6857901> ... Done in [00h00'00]\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory(env, pk_project, execute)\n",
    "\n",
    "graphs.add_uris(pbs['pk_teen'], pbs['uri_should'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
