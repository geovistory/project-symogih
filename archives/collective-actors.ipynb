{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PRODUCTION Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "# %load ~/Desktop/geovpylib/heading.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = 'prod'\n",
    "pk_project = 6857901\n",
    "execute = True\n",
    "metadata_str = 'collective-actors'\n",
    "import_manner = 'one-shot' # 'batch'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "\n",
    "import geovpylib.analysis as a\n",
    "import geovpylib.database as db\n",
    "import geovpylib.queries as q\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.utils as u\n",
    "\n",
    "eta = u.Eta()\n",
    "\n",
    "# db.connect_external(os.getenv(''))\n",
    "db.connect_geovistory(env, pk_project, execute)\n",
    "db.set_metadata({'import-id': datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n",
    "db.set_insert_manner(import_manner)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import collective actors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BHP infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PGSQL Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "db.connect_external(os.environ.get('YELLOW_BHP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coac = u.read_df('../../data/bhp/collective-actor.csv', skip_info=True).rename(columns={'notes':'notes_coac', 'begin_year':'begin_year_coac', 'end_year':'end_year_coac'}).drop(columns=['concat_standard_name'])\n",
    "coac_name = u.read_df('../../data/bhp/collective-actor-name.csv', skip_info=True).rename(columns={'notes':'notes_name', 'lang_iso':'lang_name', 'comment_begin_year':'comment_begin_year_name', 'comment_end_year':'comment_end_year_name', 'begin_date':'begin_date_name', 'end_date':'end_date_name'})\n",
    "coac_text_property = u.read_df('../../data/bhp/collective-actor-text-property.csv', skip_info=True).rename(columns={'notes':'notes_text_prop', 'lang_iso_code':'lang_text_prop'})\n",
    "\n",
    "coacs = coac.merge(coac_name, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_name'])\n",
    "coacs = coacs.merge(coac_text_property, left_on='pk_collective_actor', right_on='fk_collective_actor', how='left').drop(columns=['fk_collective_actor', 'pk_collective_actor_text_property'])\n",
    "coacs['begin_date_name'] = [u.parse_tuple_date(d) for d in coacs['begin_date_name']]\n",
    "coacs['end_date_name'] = [u.parse_tuple_date(d) for d in coacs['end_date_name']]\n",
    "\n",
    "# a.infos(coacs)\n",
    "\n",
    "# For formation and dissolution\n",
    "values = '(' + ','.join([\"'CoAc\" + str(e) + \"'\" for e in coacs['pk_collective_actor'].unique()]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formations_info = db.query(f\"\"\"\n",
    "    select\n",
    "        ir.fk_associated_object as pk_coac, \n",
    "        i.pk_information, \n",
    "                           i.info_label,\n",
    "        id.year, id.month, id.day,\n",
    "        id.fk_abob_type_information_date,\n",
    "        id.complement as complement, \n",
    "        id.notes as notes,\n",
    "        id.certainty_date\n",
    "    from bhp.information_role ir\n",
    "    inner join bhp.information i on i.pk_information = ir.fk_information and i.fk_type_information = 30\n",
    "    inner join bhp.information_date id on id.fk_information = i.pk_information\n",
    "    where ir.fk_associated_object in {values}\n",
    "\"\"\")\n",
    "\n",
    "formations_info['pk_coac'] = formations_info['pk_coac'].str.replace('CoAc', '', regex=False)\n",
    "formations_info['pk_coac'] = formations_info['pk_coac'].astype(pd.Int64Dtype())\n",
    "formations_info['year'] = formations_info['year'].astype(pd.Int64Dtype())\n",
    "formations_info['month'] = formations_info['month'].astype(pd.Int64Dtype())\n",
    "formations_info['day'] = formations_info['day'].astype(pd.Int64Dtype())\n",
    "formations_info['fk_abob_type_information_date'] = formations_info['fk_abob_type_information_date'].astype(pd.Int64Dtype())\n",
    "formations_info['date_bhp'] = [(row.year, row.month, row.day) for i, row in formations_info.iterrows()]\n",
    "formations_info['uri'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in formations_info['pk_information']]\n",
    "formations_info.drop(columns=['year', 'month', 'day', 'pk_information'], inplace=True)\n",
    "formations_info['complement'] = [pd.NA if pd.isna(row['complement']) or row['complement'].strip() == '' else row['complement'] for _,row in formations_info.iterrows()]\n",
    "formations_info['notes'] = [pd.NA if pd.isna(row['notes']) or row['notes'].strip() == '' else row['notes'] for _,row in formations_info.iterrows()]\n",
    "formations_info['notes'] = [s.replace('<p>', '').replace('</p>', '') if pd.notna(s) else pd.NA for s in formations_info['notes']]\n",
    "formations_info['notes'] = [s.replace('<em>', '').replace('</em>', '') if pd.notna(s) else pd.NA for s in formations_info['notes']]\n",
    "formations_info['complement'] = [e.replace('<p>', '').replace('</p>', '') if pd.notna(e) else pd.NA for e in formations_info['complement']]\n",
    "\n",
    "# a.infos(formations)\n",
    "\n",
    "# 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolutions_info = db.query(f\"\"\"\n",
    "    select\n",
    "        ir.fk_associated_object as pk_coac, \n",
    "        i.pk_information, \n",
    "        id.year, id.month, id.day,\n",
    "        id.fk_abob_type_information_date,\n",
    "        id.complement as complement, \n",
    "        id.notes as notes,\n",
    "        id.certainty_date\n",
    "    from bhp.information_role ir\n",
    "    inner join bhp.information i on i.pk_information = ir.fk_information and i.fk_type_information = 33\n",
    "    inner join bhp.information_date id on id.fk_information = i.pk_information\n",
    "    where ir.fk_associated_object in {values}\n",
    "\"\"\")\n",
    "\n",
    "dissolutions_info['pk_coac'] = dissolutions_info['pk_coac'].str.replace('CoAc', '', regex=False)\n",
    "dissolutions_info['pk_coac'] = dissolutions_info['pk_coac'].astype(pd.Int64Dtype())\n",
    "dissolutions_info['year'] = dissolutions_info['year'].astype(pd.Int64Dtype())\n",
    "dissolutions_info['month'] = dissolutions_info['month'].astype(pd.Int64Dtype())\n",
    "dissolutions_info['day'] = dissolutions_info['day'].astype(pd.Int64Dtype())\n",
    "dissolutions_info['fk_abob_type_information_date'] = dissolutions_info['fk_abob_type_information_date'].astype(pd.Int64Dtype())\n",
    "dissolutions_info['date_bhp'] = [(row.year, row.month, row.day) for i, row in dissolutions_info.iterrows()]\n",
    "dissolutions_info['uri'] = ['http://symogih.org/resource/Info' + str(fk_info) for fk_info in dissolutions_info['pk_information']]\n",
    "dissolutions_info.drop(columns=['year', 'month', 'day', 'pk_information'], inplace=True)\n",
    "dissolutions_info['complement'] = [pd.NA if pd.isna(row['complement']) or row['complement'].strip() == '' else row['complement'] for _,row in dissolutions_info.iterrows()]\n",
    "dissolutions_info['notes'] = [pd.NA if pd.isna(row['notes']) or row['notes'].strip() == '' else row['notes'] for _,row in dissolutions_info.iterrows()]\n",
    "dissolutions_info['complement'] = [e.replace('<p>', '').replace('</p>', '') if pd.notna(e) else pd.NA for e in dissolutions_info['complement']]\n",
    "\n",
    "# a.infos(dissolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URIs that are in BHP\n",
    "uris_bhp = db.query('select * from bhp.documentation')\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['fk_documented_object'])]\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['fk_documenting_entity'])]\n",
    "uris_bhp = uris_bhp[pd.notna(uris_bhp['identifier'])]\n",
    "uris_bhp = uris_bhp[uris_bhp['fk_documented_object'].str.contains('CoAc')]\n",
    "uris_bhp = uris_bhp[uris_bhp['fk_documenting_entity'].str.contains('DiOb')]\n",
    "uris_bhp['fk_documented_object'] = uris_bhp['fk_documented_object'].str.replace('CoAc', '')\n",
    "uris_bhp = uris_bhp[['fk_documented_object', 'fk_documenting_entity', 'identifier']]\n",
    "uris_bhp['fk_documenting_entity'] = uris_bhp['fk_documenting_entity'].str.replace('DiOb', '')\n",
    "u.parse_df(uris_bhp)\n",
    "resource_address_concat = u.parse_df(db.query('select * from bhp.resource_address_concatenation')[['fk_digital_object', 'fk_resource_address']])\n",
    "resource_address = u.parse_df(db.query('select * from bhp.resource_address')[['pk_resource_address', 'uri']])\n",
    "uris_bhp = uris_bhp.merge(resource_address_concat, left_on='fk_documenting_entity', right_on='fk_digital_object', how='left').drop(columns=['fk_documenting_entity', 'fk_digital_object'])\n",
    "uris_bhp = uris_bhp.merge(resource_address, left_on='fk_resource_address', right_on='pk_resource_address', how='left')\n",
    "uris_bhp['uri'] = uris_bhp['uri'] + uris_bhp['identifier']\n",
    "uris_bhp = uris_bhp[['fk_documented_object', 'uri']]\n",
    "uris_bhp.dropna(subset=['uri'], inplace=True)\n",
    "uris_bhp.rename(columns={'fk_documented_object': 'pk_collective_actor'}, inplace=True)\n",
    "\n",
    "# All together\n",
    "uris = pd.concat([uris_bhp]).sort_values('pk_collective_actor').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Database correctly disconnected.\n"
     ]
    }
   ],
   "source": [
    "db.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GV infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Requests will not be executed\n",
      "[DB] Connecting to PRODUCTION Database ... Connected!\n",
      "[DB] Database correctly disconnected.\n",
      "Shape:  (7020, 2) - extract:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk_gv</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>648250</td>\n",
       "      <td>Großherzogtum Baden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>648267</td>\n",
       "      <td>Königreich Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648267</td>\n",
       "      <td>Würtemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>648284</td>\n",
       "      <td>Kanton Aargau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>837196</td>\n",
       "      <td>ordo senatorius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pk_gv                    name\n",
       "0  648250     Großherzogtum Baden\n",
       "1  648267  Königreich Württemberg\n",
       "2  648267              Würtemberg\n",
       "3  648284           Kanton Aargau\n",
       "4  837196         ordo senatorius"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.connect_geovistory('prod', skip_protection=True)\n",
    "\n",
    "groups = db.query(f\"\"\"\n",
    "    select\n",
    "        r.pk_entity as pk_gv,\n",
    "        a3.string as name\n",
    "    from information.resource r\n",
    "    inner join information.statement s1 on s1.fk_object_info = r.pk_entity and s1.fk_property = {pks.properties.aial_isAppelationForLanguageOf_entity}\n",
    "    inner join information.statement s2 on s2.fk_subject_info = s1.fk_subject_info and s2.fk_property = {pks.properties.aial_refersToName_appellation}\n",
    "    inner join information.appellation a3 on a3.pk_entity = s2.fk_object_info\n",
    "    where r.fk_class = {pks.classes.group}                  \n",
    "\"\"\")\n",
    "db.disconnect()\n",
    "\n",
    "a.infos(groups)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collective_actors = coacs[['pk_collective_actor', 'name']].rename(columns={'pk_collective_actor':'pk_bhp'})\n",
    "\n",
    "# # Prepare strings\n",
    "# collective_actors['name_compare'] = [unidecode(s.lower()) for s in collective_actors.name]\n",
    "# groups['name_compare'] = [unidecode(s.lower()) for s in groups.name]\n",
    "\n",
    "# threshold = 0.5\n",
    "\n",
    "# similars = []\n",
    "# eta.begin(len(collective_actors), 'Finding similars')\n",
    "# for i, row_bhp in collective_actors.iterrows():\n",
    "#     for j, row_gv in groups.iterrows():\n",
    "#         score = u.trigram_similarity(row_bhp['name_compare'], row_gv['name_compare'])\n",
    "#         if score >= threshold: \n",
    "#             # eta.print(f'Found, score {score}: (' + str(row_bhp['pk_bhp']) + ') <' + str(row_bhp['name']) + '> - <' + str(row_gv['name']) + '> (' + str(row_gv['pk_gv']) + ')')\n",
    "#             similars.append({\n",
    "#                 'score': score,\n",
    "#                 'pk_bhp': row_bhp['pk_bhp'],\n",
    "#                 'name_bhp': row_bhp['name'],\n",
    "#                 'name_gv': row_gv['name'],\n",
    "#                 'pk_gv': row_gv['pk_gv']\n",
    "#             })\n",
    "#     eta.iter()\n",
    "# eta.end()\n",
    "\n",
    "# similars = pd.DataFrame(data=similars)\n",
    "# similars.sort_values('score', ascending=False, inplace=True)\n",
    "# similars.drop_duplicates(['pk_bhp', 'pk_gv'], inplace=True)\n",
    "# u.save_df(similars, '../../data/record-linkage-collective-actors.csv')\n",
    "\n",
    "# a.infos(similars)\n",
    "\n",
    "# 2h22m15s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (127, 6) - extract:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pk_bhp</th>\n",
       "      <th>Doublon</th>\n",
       "      <th>name_bhp</th>\n",
       "      <th>name_gv</th>\n",
       "      <th>pk_gv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13762</td>\n",
       "      <td>oui</td>\n",
       "      <td>Carmel de Saint-Joseph et de Sainte-Thérèse (N...</td>\n",
       "      <td>Carmel de Saint-Joseph et de Sainte-Thérèse (N...</td>\n",
       "      <td>6141350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13761</td>\n",
       "      <td>oui</td>\n",
       "      <td>Carmel de Notre-Dame des neiges (Nancy I)</td>\n",
       "      <td>Carmel de Notre-Dame des Neiges (Nancy I)</td>\n",
       "      <td>6141170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13760</td>\n",
       "      <td>oui</td>\n",
       "      <td>Carmel de Morlaix I</td>\n",
       "      <td>Carmel de Morlaix I</td>\n",
       "      <td>6141577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>oui</td>\n",
       "      <td>Ordo fratrum praedicatorum</td>\n",
       "      <td>Ordo Fratrum Prædicatorum</td>\n",
       "      <td>1859975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13756</td>\n",
       "      <td>oui</td>\n",
       "      <td>Carmel de Saint-Joseph et de Sainte-Thérèse (C...</td>\n",
       "      <td>Carmel de Saint-Joseph et de Sainte-Thérèse (C...</td>\n",
       "      <td>6140898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  pk_bhp Doublon                                           name_bhp  \\\n",
       "0    1.0   13762     oui  Carmel de Saint-Joseph et de Sainte-Thérèse (N...   \n",
       "1    1.0   13761     oui          Carmel de Notre-Dame des neiges (Nancy I)   \n",
       "2    1.0   13760     oui                                Carmel de Morlaix I   \n",
       "3    1.0      25     oui                         Ordo fratrum praedicatorum   \n",
       "4    1.0   13756     oui  Carmel de Saint-Joseph et de Sainte-Thérèse (C...   \n",
       "\n",
       "                                             name_gv    pk_gv  \n",
       "0  Carmel de Saint-Joseph et de Sainte-Thérèse (N...  6141350  \n",
       "1          Carmel de Notre-Dame des Neiges (Nancy I)  6141170  \n",
       "2                                Carmel de Morlaix I  6141577  \n",
       "3                          Ordo Fratrum Prædicatorum  1859975  \n",
       "4  Carmel de Saint-Joseph et de Sainte-Thérèse (C...  6140898  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "record_linkage = u.read_df('../../data/record-linkage-collective-actors-filled.csv', sep=',', skip_info=True)\n",
    "record_linkage = record_linkage[record_linkage['Doublon'] == \"oui\"]\n",
    "# record_linkage = record_linkage[['pk_bhp', 'pk_gv']]\n",
    "\n",
    "a.infos(record_linkage)\n",
    "not_to_create = record_linkage['pk_bhp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pk_bhp</th>\n",
       "      <th>Doublon</th>\n",
       "      <th>name_bhp</th>\n",
       "      <th>name_gv</th>\n",
       "      <th>pk_gv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [score, pk_bhp, Doublon, name_bhp, name_gv, pk_gv]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to verify that a single bhp entity correspond at most at one gv entity\n",
    "record_linkage[record_linkage.duplicated('pk_bhp', keep=False)].sort_values('pk_bhp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>pk_bhp</th>\n",
       "      <th>Doublon</th>\n",
       "      <th>name_bhp</th>\n",
       "      <th>name_gv</th>\n",
       "      <th>pk_gv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13855</td>\n",
       "      <td>oui</td>\n",
       "      <td>Carmel de la Vierge du Carmel (Avignon)</td>\n",
       "      <td>Carmel de la Vierge du Carmel (Avignon)</td>\n",
       "      <td>6141667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    score  pk_bhp Doublon                                 name_bhp  \\\n",
       "55    1.0   13855     oui  Carmel de la Vierge du Carmel (Avignon)   \n",
       "\n",
       "                                    name_gv    pk_gv  \n",
       "55  Carmel de la Vierge du Carmel (Avignon)  6141667  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_linkage[record_linkage['pk_gv'] == 6141667]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PRODUCTION Database ... Connected!\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory(env, pk_project, execute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 21882 resources of class [68] ... Done in [00h00m05s]\n",
      "Creating info_proj_rel of 21882 entities with project <6857901> ... Done in [00h00m21s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "selection = coacs[['pk_collective_actor']].drop_duplicates()\n",
    "\n",
    "# Remove those already existing (record linkage)\n",
    "selection = selection[[pk_bhp not in not_to_create for pk_bhp in selection['pk_collective_actor']]]\n",
    "\n",
    "# Create data\n",
    "selection['pk_gv'] = db.resources.create(pks.classes.group, len(selection))\n",
    "\n",
    "# Merge into dataframe - new ones\n",
    "coacs = coacs.merge(selection, on='pk_collective_actor', how='left')\n",
    "# Merge into dataframe - from record linkage\n",
    "coacs = coacs.merge(record_linkage, left_on='pk_collective_actor', right_on='pk_bhp', how='left').drop(columns=['pk_bhp'])\n",
    "\n",
    "coacs['pk_gv'] = [row['pk_gv_x'] if pd.notna(row['pk_gv_x']) else row['pk_gv_y'] for _,row in coacs.iterrows()]\n",
    "coacs['pk_gv'] = coacs['pk_gv'].astype(pd.Int64Dtype())\n",
    "coacs.drop(columns=['pk_gv_x', 'pk_gv_y'], inplace=True)\n",
    "\n",
    "# 13s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 22009 resources of class [967] ... Done in [00h00m09s]\n",
      "Creating info_proj_rel of 22009 entities with project <6857901> ... Done in [00h00m27s]\n",
      "Creating 22009 appellations ... Done in [00h00m44s]\n",
      "Creating 22009 statements ... Updating metadata ... Done in [00h00m40s]\n",
      "Creating info_proj_rel of 22009 entities with project <6857901> ... Done in [00h00m23s]\n",
      "Creating 22009 statements ... Updating metadata ... Done in [00h00m44s]\n",
      "Creating info_proj_rel of 22009 entities with project <6857901> ... Done in [00h00m21s]\n",
      "Creating 18 resources of class [967] ... Done in [00h00m01s]\n",
      "Creating info_proj_rel of 18 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 18 appellations ... Done in [00h00m00s]\n",
      "Creating 18 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 18 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 18 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 18 entities with project <6857901> ... Done in [00h00m00s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare - URI Symogih\n",
    "coacs['uri'] = 'http://symogih.org/resource/CoAc' + coacs['pk_collective_actor'].astype(str)\n",
    "selection = coacs[['pk_gv', 'uri']].drop_duplicates()\n",
    "\n",
    "# Prepare - URI externe\n",
    "pk_coacs = coacs['pk_collective_actor'].tolist()\n",
    "uris = uris.merge(coacs[['pk_gv', 'pk_collective_actor']])\n",
    "uris.drop_duplicates(inplace=True)\n",
    "uris.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create data\n",
    "db.shortcuts.add_uris(selection['pk_gv'], selection['uri'])\n",
    "db.shortcuts.add_uris(uris['pk_gv'], uris['uri'])\n",
    "\n",
    "# 1m30s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appe_type(fk_abob_type):\n",
    "    if pd.isna(fk_abob_type): return pd.NA\n",
    "    if fk_abob_type == 1253: return 1645890\n",
    "    if fk_abob_type == 1051: return 1645890 # cf discussion avec VA sur Discord\n",
    "    if fk_abob_type == 1270: return 1661195\n",
    "    if fk_abob_type == 1063: return 8067077\n",
    "    return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Connecting to PRODUCTION Database ... Connected!\n",
      "Creating 23699 statements ... Updating metadata ... Done in [00h00m37s]\n",
      "Creating info_proj_rel of 23699 entities with project <6857901> ... Done in [00h00m19s]\n",
      "Creating 23699 appellations ... Done in [00h00m50s]\n",
      "Creating 23699 statements ... Updating metadata ... Done in [00h00m40s]\n",
      "Creating info_proj_rel of 23699 entities with project <6857901> ... Done in [00h00m19s]\n"
     ]
    }
   ],
   "source": [
    "db.connect_geovistory(env, pk_project, True, False)\n",
    "\n",
    "# Prepare\n",
    "selection = coacs[['pk_gv', 'name', 'lang_name', 'comment_begin_year_name', 'comment_end_year_name', 'notes_name', 'fk_abob_coac_name_type', 'begin_date_name', 'end_date_name']].drop_duplicates(['pk_gv', 'name']).copy()\n",
    "selection['pk_lang_name'] = [pks.languages.from_iso_code(lang) if pd.notna(lang) else pd.NA for lang in selection['lang_name']]\n",
    "selection['pk_aial_type'] = [get_appe_type(t) for t in selection.fk_abob_coac_name_type]\n",
    "\n",
    "\n",
    "selection['pk_aial'] = [i for i in range(8465681, 8489380)]\n",
    "db.statements.create(selection['pk_aial'], pks.properties.aial_isAppelationForLanguageOf_entity, selection['pk_gv'])\n",
    "\n",
    "# Create - Appellation\n",
    "selection['pk_appe_name'] = db.appellations.create(selection['name'])\n",
    "db.statements.create(selection['pk_aial'], pks.properties.aial_refersToName_appellation, selection['pk_appe_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 10598 statements ... Updating metadata ... Done in [00h00m23s]\n",
      "Creating info_proj_rel of 10598 entities with project <6857901> ... Done in [00h00m10s]\n",
      "Creating 8985 statements ... Updating metadata ... Done in [00h00m18s]\n",
      "Creating info_proj_rel of 8985 entities with project <6857901> ... Done in [00h00m09s]\n",
      "Creating 1312 time primitives ... Done in [00h00m00s]\n",
      "Creating 1312 statements ... Updating metadata ... Done in [00h00m04s]\n",
      "Creating info_proj_rel of 1312 entities with project <6857901> ... Done in [00h00m01s]\n",
      "Creating 586 time primitives ... Done in [00h00m00s]\n",
      "Creating 586 statements ... Updating metadata ... Done in [00h00m02s]\n",
      "Creating info_proj_rel of 586 entities with project <6857901> ... Done in [00h00m00s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "selection = coacs[['pk_gv', 'name', 'lang_name', 'comment_begin_year_name', 'comment_end_year_name', 'notes_name', 'fk_abob_coac_name_type', 'begin_date_name', 'end_date_name']].drop_duplicates(['pk_gv', 'name']).copy()\n",
    "selection['pk_lang_name'] = [pks.languages.from_iso_code(lang) if pd.notna(lang) else pd.NA for lang in selection['lang_name']]\n",
    "selection['pk_aial_type'] = [get_appe_type(t) for t in selection.fk_abob_coac_name_type]\n",
    "\n",
    "# Create - AiaL\n",
    "selection['pk_aial'] = db.resources.create(pks.classes.aial, len(selection))\n",
    "db.statements.create(selection['pk_aial'], pks.properties.aial_isAppelationForLanguageOf_entity, selection['pk_gv'])\n",
    "\n",
    "# Create - Appellation\n",
    "selection['pk_appe_name'] = db.appellations.create(selection['name'])\n",
    "db.statements.create(selection['pk_aial'], pks.properties.aial_refersToName_appellation, selection['pk_appe_name'])\n",
    "\n",
    "# Create - Language\n",
    "selection_lang = selection[pd.notna(selection['pk_lang_name'])] # Because here, we have names without a language\n",
    "db.statements.create(selection_lang['pk_aial'], pks.properties.aial_usedInLanguage_language, selection_lang['pk_lang_name'])\n",
    "\n",
    "# Create - Type\n",
    "selection_type = selection[pd.notna(selection['pk_aial_type'])]\n",
    "db.statements.create(selection_type['pk_aial'], pks.properties.aial_hasType_aialType, selection_type['pk_aial_type'])\n",
    "\n",
    "# Create - Dates\n",
    "def get_duration(date):\n",
    "    if pd.notna(date[0]) and pd.isna(date[1]) and pd.isna(date[2]): return '1 year'\n",
    "    if pd.notna(date[0]) and pd.notna(date[1]) and pd.isna(date[2]): return '1 month'\n",
    "    if pd.notna(date[0]) and pd.notna(date[1]) and pd.notna(date[2]): return '1 day'\n",
    "    return pd.NA\n",
    "\n",
    "# Create - Dates - Begin\n",
    "selection_date_begin = selection[pd.notna(selection['begin_date_name'])].copy()\n",
    "selection_date_begin['duration'] = [get_duration(d) for d in selection_date_begin['begin_date_name']]\n",
    "selection_date_begin['pk_tp'] = db.time_primitives.create(selection_date_begin['begin_date_name'], selection_date_begin['duration'])\n",
    "db.statements.create(selection_date_begin['pk_aial'], pks.properties.timespan_endOfTheBegin_timePrim, selection_date_begin['pk_tp'])\n",
    "\n",
    "# Create - Dates - End\n",
    "selection_date_end = selection[pd.notna(selection['end_date_name'])].copy()\n",
    "selection_date_end['duration'] = [get_duration(d) for d in selection_date_end['end_date_name']]\n",
    "selection_date_end['pk_tp'] = db.time_primitives.create(selection_date_end['end_date_name'], selection_date_end['duration'])\n",
    "db.statements.create(selection_date_end['pk_aial'], pks.properties.timespan_endOfTheEnd_timePrim, selection_date_end['pk_tp'])\n",
    "\n",
    "# 2m15s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 16005 resources of class [899] ... Done in [00h00m07s]\n",
      "Creating info_proj_rel of 16005 entities with project <6857901> ... Done in [00h00m14s]\n",
      "Creating 16005 appellations ... Done in [00h04m05s]\n",
      "Creating 16005 statements ... Updating metadata ... Done in [00h00m30s]\n",
      "Creating info_proj_rel of 16005 entities with project <6857901> ... Done in [00h00m16s]\n",
      "Creating 16005 statements ... Updating metadata ... Done in [00h00m32s]\n",
      "Creating info_proj_rel of 16005 entities with project <6857901> ... Done in [00h00m16s]\n",
      "Creating 16005 statements ... Updating metadata ... Done in [00h00m39s]\n",
      "Creating info_proj_rel of 16005 entities with project <6857901> ... Done in [00h00m16s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "selection = coacs[['pk_gv', 'notes_coac', 'text', 'property_type', 'lang_text_prop']].copy().drop_duplicates()\n",
    "selection = selection[pd.notna(selection['notes_coac']) | pd.notna(selection['text'])]\n",
    "selection = selection[selection['notes_coac'].astype(str) != selection['text'].astype(str)]\n",
    "selection['notes_coac'] = [u.clean_text(s) for s in selection['notes_coac'].astype(str)]\n",
    "selection['note'] = ['[Note] ' + s if pd.notna(s) and s != '' else pd.NA for s in selection['notes_coac']]\n",
    "selection['lang_note'] = 'fra'\n",
    "selection['text_prop'] = ['[Complément] ' + str(row['text']) if pd.notna(row['property_type']) and row['property_type'] == 'complément' else row['text'] for _,row in selection.iterrows()]\n",
    "selection['lang_text_prop'] = [pd.NA if pd.notna(d) and d == 'None' else d for d in selection['lang_text_prop']]\n",
    "\n",
    "definitions = pd.concat([\n",
    "    selection[['pk_gv', 'note', 'lang_note']].dropna(subset='note').rename(columns={'note':'definition', 'lang_note':'lang'}), \n",
    "    selection[['pk_gv', 'text_prop', 'lang_text_prop']].dropna(subset='text_prop').rename(columns={'text_prop':'definition', 'lang_text_prop':'lang'})\n",
    "])\n",
    "definitions['lang'].fillna('fra', inplace=True)\n",
    "definitions['pk_lang'] = [pks.languages.from_iso_code(c) for c in definitions['lang']]\n",
    "definitions['definition'] = [u.clean_text(text) for text in definitions['definition']]\n",
    "\n",
    "# Create data\n",
    "db.shortcuts.add_definitions(definitions['pk_gv'].tolist(), definitions['definition'].tolist(), definitions['pk_lang'].tolist())\n",
    "\n",
    "# 3m10s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation = coacs[['pk_gv', 'pk_collective_actor', 'begin_year_coac', 'certainty_begin', 'notes_begin']]\n",
    "formation = formation.merge(formations_info, left_on='pk_collective_actor', right_on='pk_coac', how='left').drop(columns=['pk_coac'])\n",
    "\n",
    "# Date\n",
    "formation['date'] = [row['date_bhp'] if pd.notna(row['date_bhp']) else ((row['begin_year_coac'], pd.NA, pd.NA) if pd.notna(row['begin_year_coac']) else pd.NA) for _,row in formation.iterrows()]\n",
    "formation.drop(columns=['date_bhp', 'begin_year_coac'], inplace=True)\n",
    "\n",
    "# Property\n",
    "def get_property(note_begin, fk_type):\n",
    "    if pd.notna(fk_type):\n",
    "        if fk_type == 246: return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive # P82\n",
    "        if fk_type == 1125: return pks.properties.timespan_beginOfTheBegin_timePrim # P82a\n",
    "        if fk_type == 1126: return pks.properties.timespan_endOfTheEnd_timePrim # P82b\n",
    "        if fk_type == 258: return pks.properties.timeSpan_ongoingThroughout_timePrimitive #P81\n",
    "        if fk_type == 1289: return pks.properties.timespan_endOfTheBegin_timePrim # P81a\n",
    "        if fk_type == 1290: return pks.properties.timespan_beginOfTheEnd_timePrim # P81b\n",
    "        if fk_type == 1321: return pks.properties.timespan_beginOfTheBegin_timePrim\n",
    "        if fk_type == 1322: return pks.properties.timespan_beginOfTheBegin_timePrim\n",
    "        if fk_type == 1323: return pks.properties.timespan_endOfTheBegin_timePrim\n",
    "        if fk_type == 256: return pks.properties.timespan_endOfTheEnd_timePrim\n",
    "        if fk_type == 1127: return pks.properties.timespan_beginOfTheEnd_timePrim\n",
    "        if fk_type == 1128: return pks.properties.timespan_endOfTheEnd_timePrim\n",
    "        return pd.NA\n",
    "    elif pd.notna(note_begin):\n",
    "        if note_begin == 1: return pks.properties.timespan_endOfTheEnd_timePrim\n",
    "        if note_begin == 2: return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive\n",
    "        if note_begin == 3: return pks.properties.timeSpan_ongoingThroughout_timePrimitive\n",
    "        if note_begin == 4: return pks.properties.timespan_beginOfTheBegin_timePrim\n",
    "        return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive\n",
    "    else: return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive\n",
    "formation['pk_property'] = [get_property(row['notes_begin'], row['fk_abob_type_information_date']) for _,row in formation.iterrows()]\n",
    "formation.drop(columns=['fk_abob_type_information_date', 'notes_begin'], inplace=True)\n",
    "\n",
    "# Certainty\n",
    "formation['certainty'] = [row['certainty_date'] if pd.notna(row['certainty_date']) else row['certainty_begin'] for _,row in formation.iterrows()]\n",
    "formation.drop(columns=['certainty_begin', 'certainty_date'], inplace=True)\n",
    "\n",
    "# Notes \n",
    "formation['notes'] = ['[Note] ' + str(e) if pd.notna(e) else pd.NA for e in formation['notes']]\n",
    "\n",
    "# 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 11485 resources of class [60] ... Done in [00h00m03s]\n",
      "Creating info_proj_rel of 11485 entities with project <6857901> ... Done in [00h00m12s]\n",
      "Creating 11485 statements ... Updating metadata ... Done in [00h00m22s]\n",
      "Creating info_proj_rel of 11485 entities with project <6857901> ... Done in [00h00m10s]\n",
      "Creating 11485 time primitives ... Done in [00h00m03s]\n",
      "Creating 11485 statements ... Updating metadata ... Done in [00h00m29s]\n",
      "Creating info_proj_rel of 11485 entities with project <6857901> ... Done in [00h00m12s]\n",
      "Creating 4120 resources of class [900] ... Done in [00h00m02s]\n",
      "Creating info_proj_rel of 4120 entities with project <6857901> ... Done in [00h00m05s]\n",
      "Creating 4120 appellations ... Done in [00h00m03s]\n",
      "Creating 4120 statements ... Updating metadata ... Done in [00h00m07s]\n",
      "Creating info_proj_rel of 4120 entities with project <6857901> ... Done in [00h00m03s]\n",
      "Creating 4120 statements ... Updating metadata ... Done in [00h00m06s]\n",
      "Creating info_proj_rel of 4120 entities with project <6857901> ... Done in [00h00m04s]\n",
      "Creating 4120 statements ... Updating metadata ... Done in [00h00m09s]\n",
      "Creating info_proj_rel of 4120 entities with project <6857901> ... Done in [00h00m03s]\n",
      "Creating 7622 resources of class [967] ... Done in [00h00m03s]\n",
      "Creating info_proj_rel of 7622 entities with project <6857901> ... Done in [00h00m05s]\n",
      "Creating 7622 appellations ... Done in [00h00m15s]\n",
      "Creating 7622 statements ... Updating metadata ... Done in [00h00m15s]\n",
      "Creating info_proj_rel of 7622 entities with project <6857901> ... Done in [00h00m07s]\n",
      "Creating 7622 statements ... Updating metadata ... Done in [00h00m14s]\n",
      "Creating info_proj_rel of 7622 entities with project <6857901> ... Done in [00h00m06s]\n",
      "Creating 121 resources of class [900] ... Done in [00h00m01s]\n",
      "Creating info_proj_rel of 121 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 121 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 121 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 121 appellations ... Done in [00h00m00s]\n",
      "Creating 121 statements ... Updating metadata ... Done in [00h00m01s]\n",
      "Creating info_proj_rel of 121 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 121 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 121 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 75 resources of class [900] ... Done in [00h00m01s]\n",
      "Creating info_proj_rel of 75 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 75 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 75 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 75 appellations ... Done in [00h00m00s]\n",
      "Creating 75 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 75 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 75 statements ... Updating metadata ... Done in [00h00m01s]\n",
      "Creating info_proj_rel of 75 entities with project <6857901> ... Done in [00h00m00s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "selection = formation[pd.notna(formation['date'])].copy()\n",
    "selection = selection.drop_duplicates(subset=['pk_gv', 'date'])\n",
    "\n",
    "# Create Formation\n",
    "selection['pk_formation'] = db.resources.create(pks.classes.formation, len(selection))\n",
    "db.statements.create(selection['pk_formation'], pks.properties.formation_hasFormed_group, selection['pk_gv'])\n",
    "\n",
    "# Link to date\n",
    "selection['duration'] = [get_duration(d) for d in selection['date']]\n",
    "selection['pk_time_prim'] = db.time_primitives.create(selection['date'], selection['duration'])\n",
    "db.statements.create(selection['pk_formation'], selection['pk_property'], selection['pk_time_prim'])\n",
    "\n",
    "# Certainty comment\n",
    "selection_certainty_comment = selection[(selection['certainty'] == 2) | (selection['certainty'] == 3)].copy()\n",
    "selection_certainty_comment['comment'] = ['Date reconstituée' if c == 2 else 'Date postulée' for c in selection_certainty_comment['certainty']]\n",
    "selection_certainty_comment['pk_certainty_comment'] = db.resources.create(pks.classes.comment, len(selection_certainty_comment))\n",
    "selection_certainty_comment['pk_appe'] = db.appellations.create(selection_certainty_comment['comment'])\n",
    "db.statements.create(selection_certainty_comment['pk_certainty_comment'], pks.properties.text_hasValueVersion_string, selection_certainty_comment['pk_appe'])\n",
    "db.statements.create(selection_certainty_comment['pk_certainty_comment'], pks.properties.comment_hasCommentType_CommentType, 7953586)\n",
    "db.statements.create(selection_certainty_comment['pk_formation'], pks.properties.entity_hasComment_text, selection_certainty_comment['pk_certainty_comment'])\n",
    "\n",
    "# URI\n",
    "selection_uri = selection[pd.notna(selection['uri'])]\n",
    "db.shortcuts.add_uris(selection_uri['pk_formation'], selection_uri['uri'])\n",
    "\n",
    "# Compléments sur la date\n",
    "selection_cplmt_date = selection[pd.notna(selection['complement'])].copy()\n",
    "selection_cplmt_date['pk_comment'] = db.resources.create(pks.classes.comment, len(selection_cplmt_date))\n",
    "db.statements.create(selection_cplmt_date['pk_comment'], pks.properties.comment_hasCommentType_CommentType, 8065621) # Complément sur la date\n",
    "selection_cplmt_date['pk_appe'] = db.appellations.create(selection_cplmt_date['complement'])\n",
    "db.statements.create(selection_cplmt_date['pk_comment'], pks.properties.text_hasValueVersion_string, selection_cplmt_date['pk_appe'])\n",
    "db.statements.create(selection_cplmt_date['pk_formation'], pks.properties.entity_hasComment_text, selection_cplmt_date['pk_comment'])\n",
    "\n",
    "# Notes sur la date\n",
    "selection_notes_date = selection[pd.notna(selection['notes'])].copy()\n",
    "selection_notes_date['pk_comment'] = db.resources.create(pks.classes.comment, len(selection_notes_date))\n",
    "db.statements.create(selection_notes_date['pk_comment'], pks.properties.comment_hasCommentType_CommentType, 8065632) # Notes sur la date\n",
    "selection_notes_date['pk_appe'] = db.appellations.create(selection_notes_date['notes'])\n",
    "db.statements.create(selection_notes_date['pk_comment'], pks.properties.text_hasValueVersion_string, selection_notes_date['pk_appe'])\n",
    "db.statements.create(selection_notes_date['pk_formation'], pks.properties.entity_hasComment_text, selection_notes_date['pk_comment'])\n",
    "\n",
    "\n",
    "# 1m33s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dissolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolution = coacs[['pk_gv', 'pk_collective_actor', 'end_year_coac', 'certainty_end', 'notes_end']]\n",
    "dissolution = dissolution.merge(dissolutions_info, left_on='pk_collective_actor', right_on='pk_coac', how='left').drop(columns=['pk_coac'])\n",
    "\n",
    "# Date\n",
    "dissolution['date'] = [row['date_bhp'] if pd.notna(row['date_bhp']) else ((row['end_year_coac'], pd.NA, pd.NA) if pd.notna(row['end_year_coac']) else pd.NA) for _,row in dissolution.iterrows()]\n",
    "dissolution.drop(columns=['date_bhp', 'end_year_coac'], inplace=True)\n",
    "\n",
    "# Property\n",
    "def get_property(note_end, fk_type):\n",
    "    if pd.notna(fk_type):\n",
    "        if fk_type == 246: return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive # P82\n",
    "        if fk_type == 1125: return pks.properties.timespan_beginOfTheBegin_timePrim # P82a\n",
    "        if fk_type == 1126: return pks.properties.timespan_endOfTheEnd_timePrim # P82b\n",
    "        if fk_type == 258: return pks.properties.timeSpan_ongoingThroughout_timePrimitive #P81\n",
    "        if fk_type == 1289: return pks.properties.timespan_endOfTheBegin_timePrim # P81a\n",
    "        if fk_type == 1290: return pks.properties.timespan_beginOfTheEnd_timePrim # P81b\n",
    "        if fk_type == 1321: return pks.properties.timespan_beginOfTheBegin_timePrim\n",
    "        if fk_type == 1322: return pks.properties.timespan_beginOfTheBegin_timePrim\n",
    "        if fk_type == 1323: return pks.properties.timespan_endOfTheBegin_timePrim\n",
    "        if fk_type == 256: return pks.properties.timespan_endOfTheEnd_timePrim\n",
    "        if fk_type == 1127: return pks.properties.timespan_beginOfTheEnd_timePrim\n",
    "        if fk_type == 1128: return pks.properties.timespan_endOfTheEnd_timePrim\n",
    "        return pd.NA\n",
    "    elif pd.notna(note_end):\n",
    "        if note_end == 1: return pks.properties.timespan_endOfTheEnd_timePrim\n",
    "        if note_end == 2: return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive\n",
    "        if note_end == 3: return pks.properties.timeSpan_ongoingThroughout_timePrimitive\n",
    "        if note_end == 4: return pks.properties.timespan_beginOfTheBegin_timePrim\n",
    "        return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive\n",
    "    else: return pks.properties.timeSpan_atSomeTimeWithin_timePrimitive\n",
    "dissolution['pk_property'] = [get_property(row['notes_end'], row['fk_abob_type_information_date']) for _,row in dissolution.iterrows()]\n",
    "dissolution.drop(columns=['fk_abob_type_information_date', 'notes_end'], inplace=True)\n",
    "\n",
    "# Certainty\n",
    "dissolution['certainty'] = [row['certainty_date'] if pd.notna(row['certainty_date']) else row['certainty_end'] for _,row in dissolution.iterrows()]\n",
    "dissolution.drop(columns=['certainty_end', 'certainty_date'], inplace=True)\n",
    "\n",
    "# Notes \n",
    "dissolution['notes'] = ['[Note] ' + str(e) if pd.notna(e) else pd.NA for e in dissolution['notes']]\n",
    "\n",
    "# 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 6847 resources of class [62] ... Done in [00h00m02s]\n",
      "Creating info_proj_rel of 6847 entities with project <6857901> ... Done in [00h00m05s]\n",
      "Creating 6847 statements ... Updating metadata ... Done in [00h00m12s]\n",
      "Creating info_proj_rel of 6847 entities with project <6857901> ... Done in [00h00m07s]\n",
      "Creating 6847 time primitives ... Done in [00h00m00s]\n",
      "Creating 6847 statements ... Updating metadata ... Done in [00h00m16s]\n",
      "Creating info_proj_rel of 6847 entities with project <6857901> ... Done in [00h00m08s]\n",
      "Creating 4068 resources of class [900] ... Done in [00h00m01s]\n",
      "Creating info_proj_rel of 4068 entities with project <6857901> ... Done in [00h00m04s]\n",
      "Creating 4068 appellations ... Done in [00h00m03s]\n",
      "Creating 4068 statements ... Updating metadata ... Done in [00h00m08s]\n",
      "Creating info_proj_rel of 4068 entities with project <6857901> ... Done in [00h00m04s]\n",
      "Creating 4068 statements ... Updating metadata ... Done in [00h00m08s]\n",
      "Creating info_proj_rel of 4068 entities with project <6857901> ... Done in [00h00m03s]\n",
      "Creating 4068 statements ... Updating metadata ... Done in [00h00m08s]\n",
      "Creating info_proj_rel of 4068 entities with project <6857901> ... Done in [00h00m05s]\n",
      "Creating 49 resources of class [967] ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 49 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 49 appellations ... Done in [00h00m01s]\n",
      "Creating 49 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 49 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 49 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 49 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 1 resources of class [900] ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 1 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m01s]\n",
      "Creating 1 appellations ... Done in [00h00m00s]\n",
      "Creating 1 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m00s]\n",
      "Creating 1 statements ... Updating metadata ... Done in [00h00m00s]\n",
      "Creating info_proj_rel of 1 entities with project <6857901> ... Done in [00h00m00s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "selection = dissolution[pd.notna(dissolution['date'])].copy()\n",
    "selection = selection.drop_duplicates(subset=['pk_gv', 'date'])\n",
    "\n",
    "# Create dissolution\n",
    "selection['pk_dissolution'] = db.resources.create(pks.classes.dissolution, len(selection))\n",
    "db.statements.create(selection['pk_dissolution'], pks.properties.dissolution_dissolved_group, selection['pk_gv'])\n",
    "\n",
    "# Link to date\n",
    "selection['duration'] = [get_duration(d) for d in selection['date']]\n",
    "selection['pk_time_prim'] = db.time_primitives.create(selection['date'], selection['duration'])\n",
    "db.statements.create(selection['pk_dissolution'], selection['pk_property'], selection['pk_time_prim'])\n",
    "\n",
    "# Certainty comment\n",
    "selection_certainty_comment = selection[(selection['certainty'] == 2) | (selection['certainty'] == 3)].copy()\n",
    "selection_certainty_comment['comment'] = ['Date reconstituée' if c == 2 else 'Date postulée' for c in selection_certainty_comment['certainty']]\n",
    "selection_certainty_comment['pk_certainty_comment'] = db.resources.create(pks.classes.comment, len(selection_certainty_comment))\n",
    "selection_certainty_comment['pk_appe'] = db.appellations.create(selection_certainty_comment['comment'])\n",
    "db.statements.create(selection_certainty_comment['pk_certainty_comment'], pks.properties.text_hasValueVersion_string, selection_certainty_comment['pk_appe'])\n",
    "db.statements.create(selection_certainty_comment['pk_certainty_comment'], pks.properties.comment_hasCommentType_CommentType, 7953586)\n",
    "db.statements.create(selection_certainty_comment['pk_dissolution'], pks.properties.entity_hasComment_text, selection_certainty_comment['pk_certainty_comment'])\n",
    "\n",
    "# URI\n",
    "selection_uri = selection[pd.notna(selection['uri'])]\n",
    "db.shortcuts.add_uris(selection_uri['pk_dissolution'], selection_uri['uri'])\n",
    "\n",
    "# Compléments sur la date\n",
    "selection_cplmt_date = selection[pd.notna(selection['complement'])].copy()\n",
    "selection_cplmt_date['pk_comment'] = db.resources.create(pks.classes.comment, len(selection_cplmt_date))\n",
    "db.statements.create(selection_cplmt_date['pk_comment'], pks.properties.comment_hasCommentType_CommentType, 8065621) # Complément sur la date\n",
    "selection_cplmt_date['pk_appe'] = db.appellations.create(selection_cplmt_date['complement'])\n",
    "db.statements.create(selection_cplmt_date['pk_comment'], pks.properties.text_hasValueVersion_string, selection_cplmt_date['pk_appe'])\n",
    "db.statements.create(selection_cplmt_date['pk_dissolution'], pks.properties.entity_hasComment_text, selection_cplmt_date['pk_comment'])\n",
    "\n",
    "# Notes sur la date\n",
    "selection_notes_date = selection[pd.notna(selection['notes'])].copy()\n",
    "selection_notes_date['pk_comment'] = db.resources.create(pks.classes.comment, len(selection_notes_date))\n",
    "db.statements.create(selection_notes_date['pk_comment'], pks.properties.comment_hasCommentType_CommentType, 8065632) # Notes sur la date\n",
    "selection_notes_date['pk_appe'] = db.appellations.create(selection_notes_date['notes'])\n",
    "db.statements.create(selection_notes_date['pk_comment'], pks.properties.text_hasValueVersion_string, selection_notes_date['pk_appe'])\n",
    "db.statements.create(selection_notes_date['pk_dissolution'], pks.properties.entity_hasComment_text, selection_notes_date['pk_comment'])\n",
    "\n",
    "# 55s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
